Directory Structure:
==================================================
└── tools/
    ├── build_index_payload.py
    ├── index_packs.py
    ├── lint_packs.py
    ├── offline_eval.py
    ├── README.md
    └── wire_check.py

File: build_index_payload.py
[build_index_payload.py: start]
#!/usr/bin/env python3
"""
build_index_payload.py
- Walks app/vault/*/pack.yml and templates/*.md
- Emits a flattened JSON array for indexing/search with fields:
  id, pack, version, status, section, path, body, labels, updated_at

Usage:
  python tools/build_index_payload.py --status candidate --out artifacts/index_docs.json [--packs "psg@1.0.0,edg@1.0.1"]
"""
from __future__ import annotations
import argparse, json, os, sys
from pathlib import Path
from datetime import datetime, timezone

def read_yaml(path: Path) -> dict:
    import yaml
    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def discover_packs(vault_root: Path):
    for p in vault_root.glob("*.*"):
        y = p / "pack.yml"
        if y.exists():
            yield p, y

def normalize_pack_id(name: str) -> str:
    return name.split(".")[0].upper()

def should_include(pack_id: str, version: str, status: str, cli_status: str, packs_filter: set[str] | None):
    if cli_status and status != cli_status:
        return False
    if packs_filter is None:
        return True
    key = f"{pack_id.lower()}@{version}"
    return key in packs_filter

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--vault", default="app/vault")
    ap.add_argument("--status", choices=["candidate", "approved"], required=True)
    ap.add_argument("--packs", help='Comma-separated like "psg@1.0.0,edg@1.0.1"', default=None)
    ap.add_argument("--out", default="artifacts/index_docs.json")
    args = ap.parse_args()

    vault = Path(args.vault)
    vault.mkdir(parents=True, exist_ok=True)
    out_path = Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    packs_filter = None
    if args.packs:
        packs_filter = {x.strip().lower() for x in args.packs.split(",") if x.strip()}

    docs = []
    now = datetime.now(timezone.utc).isoformat()

    for pack_dir, pack_yml in discover_packs(vault):
        pack = read_yaml(pack_yml)
        pack_id = (pack.get("id") or normalize_pack_id(pack_dir.name)).upper()
        version = str(pack.get("version"))
        status = str(pack.get("status")).lower()
        labels = pack.get("labels", {})
        sections = pack.get("sections", [])

        if not should_include(pack_id, version, status, args.status, packs_filter):
            continue

        tmpl_dir = pack_dir / "templates"
        for section in sections:
            md_path = tmpl_dir / f"{section}.md"
            if not md_path.exists():
                # tolerate missing section but flag it in output for visibility
                body = f"[[MISSING TEMPLATE: {md_path}]]"
            else:
                body = md_path.read_text(encoding="utf-8")

            doc_id = f"{pack_id}:{version}:{section}:{args.status}"
            docs.append({
                "id": doc_id,
                "pack": pack_id,
                "version": version,
                "status": args.status,
                "section": section,
                "path": str(md_path),
                "labels": labels,
                "body": body,
                "updated_at": now,
            })

    with out_path.open("w", encoding="utf-8") as f:
        json.dump(docs, f, ensure_ascii=False, indent=2)

    print(f"Wrote {len(docs)} docs → {out_path}")
    return 0

if __name__ == "__main__":
    sys.exit(main())
[build_index_payload.py: end]

File: index_packs.py
[index_packs.py: start]
#!/usr/bin/env python3
"""
index_packs.py
- Upserts docs into Azure AI Search using REST API.
- Reads a JSON array (from build_index_payload.py).

Env:
  AZURE_SEARCH_ENDPOINT (e.g., https://<name>.search.windows.net)
  AZURE_SEARCH_INDEX    (e.g., smartai-prompts-v2)
  AZURE_SEARCH_ADMIN_KEY

Usage:
  python tools/index_packs.py --in artifacts/index_docs.json [--batch 1000]
"""
from __future__ import annotations
import argparse, json, os, sys, time, urllib.request

def post_json(url: str, payload: dict, headers: dict):
    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(url, data=data, headers=headers, method="POST")
    with urllib.request.urlopen(req) as resp:
        return resp.read().decode("utf-8")

def chunked(iterable, n):
    buf = []
    for x in iterable:
        buf.append(x)
        if len(buf) >= n:
            yield buf
            buf = []
    if buf: yield buf

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in", dest="infile", required=True)
    ap.add_argument("--batch", type=int, default=500)
    args = ap.parse_args()

    endpoint = os.environ.get("AZURE_SEARCH_ENDPOINT", "").rstrip("/")
    index = os.environ.get("AZURE_SEARCH_INDEX", "")
    key = os.environ.get("AZURE_SEARCH_ADMIN_KEY", "")

    if not (endpoint and index and key):
        print("ERR: set AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX, AZURE_SEARCH_ADMIN_KEY", file=sys.stderr)
        return 2

    docs = json.loads(open(args.infile, "r", encoding="utf-8").read())
    url = f"{endpoint}/indexes/{index}/docs/index?api-version=2023-11-01"
    headers = {
        "Content-Type": "application/json",
        "api-key": key
    }

    total = 0
    for batch in chunked(docs, args.batch):
        payload = {"value": [{"@search.action": "mergeOrUpload", **d} for d in batch]}
        _ = post_json(url, payload, headers)
        total += len(batch)
        time.sleep(0.05)  # gentle

    print(f"Indexed {total} docs to {index}")
    return 0

if __name__ == "__main__":
    sys.exit(main())
[index_packs.py: end]

File: lint_packs.py
[lint_packs.py: start]
#!/usr/bin/env python3
"""
lint_packs.py
- Validates pack manifests and basic template structure.
- Enforces PAS/SCQA structure tokens.
- Fails (exit != 0) on any error.

Usage:
  python tools/lint_packs.py [--vault app/vault]
"""
from __future__ import annotations
import argparse, sys, re, json
from pathlib import Path

PAS_TOKENS = ["Problem", "Agitate", "Solve"]
SCQA_TOKENS = ["Situation", "Complication", "Question", "Answer"]

REQUIRED_PACK_FIELDS = ["id", "version", "status", "sections"]

def read_yaml(path: Path) -> dict:
    import yaml
    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def find_packs(vault_dir: Path):
    for p in vault_dir.glob("*.*"):
        pack_yml = p / "pack.yml"
        if pack_yml.exists():
            yield p, pack_yml

def check_tokens(md_text: str, required_tokens: list[str], file: Path, errs: list[str]):
    for tok in required_tokens:
        # token must appear as a header or strong marker at least once
        pattern = rf"(^|\n)\s*(#+\s*{re.escape(tok)}\b|(\*\*|__){re.escape(tok)}(\*\*|__))"
        if not re.search(pattern, md_text, flags=re.IGNORECASE):
            errs.append(f"[TOKENS] {file}: missing token '{tok}'")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--vault", default="app/vault", help="Root of packs")
    args = ap.parse_args()

    vault = Path(args.vault)
    if not vault.exists():
        print(f"ERR: vault path not found: {vault}", file=sys.stderr)
        return 2

    errors: list[str] = []
    warnings: list[str] = []

    for pack_dir, pack_yml_path in find_packs(vault):
        pack = read_yaml(pack_yml_path)

        # Basic pack.yml shape
        for field in REQUIRED_PACK_FIELDS:
            if field not in pack:
                errors.append(f"[PACK] {pack_yml_path}: missing '{field}'")

        pack_id = pack.get("id", pack_dir.name.split(".")[0].upper())
        version = str(pack.get("version", ""))
        status = pack.get("status", "").lower() if isinstance(pack.get("status", ""), str) else pack.get("status")
        if status not in {"candidate", "approved"}:
            errors.append(f"[PACK] {pack_yml_path}: status must be 'candidate' or 'approved'")

        sections = pack.get("sections", [])
        if not isinstance(sections, list) or not sections:
            errors.append(f"[PACK] {pack_yml_path}: 'sections' must be a non-empty list")

        # Check templates exist & tokens present
        tmpl_dir = pack_dir / "templates"
        if not tmpl_dir.exists():
            errors.append(f"[PACK] {pack_yml_path}: templates/ folder missing")
            continue

        # Decide structure expectation by filename heuristic
        for md in sorted(tmpl_dir.glob("*.md")):
            text = md.read_text(encoding="utf-8")
            fname = md.stem.lower()
            if any(key in fname for key in ["business_case", "impact", "solution", "proposal", "vendor"]):
                check_tokens(text, PAS_TOKENS, md, errors)
            else:
                # default to SCQA for "about_*", "scope", "milestones", etc.
                check_tokens(text, SCQA_TOKENS, md, errors)

        # Optional schema check against repo schema (non-fatal if not present)
        schema_path = Path("smartai-prompts-v2.schema.json")
        if schema_path.exists():
            try:
                import jsonschema
                schema = json.loads(schema_path.read_text(encoding="utf-8"))
                doc = {
                    "id": f"{pack_id}@{version}",
                    "pack": pack_id,
                    "version": version,
                    "status": status,
                    "sections": sections,
                }
                jsonschema.validate(doc, schema)
            except Exception as e:
                warnings.append(f"[SCHEMA] {pack_yml_path}: {e}")

    for w in warnings: print(f"WARNING: {w}")
    if errors:
        for e in errors: print(f"ERROR: {e}", file=sys.stderr)
        return 1
    print("lint_packs: OK")
    return 0

if __name__ == "__main__":
    sys.exit(main())
[lint_packs.py: end]

File: offline_eval.py
[offline_eval.py: start]
#!/usr/bin/env python3
"""
offline_eval.py
- Lightweight CI-time evaluation (no model calls).
- Computes proxy metrics over pack templates:
    * groundedness_proxy: share of non-empty lines that contain a citation marker "[source:"
    * avg_chars_per_template and per-section caps (optional)
- Fails PR if thresholds are not met.

Usage:
  python tools/offline_eval.py \
    --vault app/vault \
    --out artifacts/eval_report.json \
    --min_grounded 0.80 \
    --max_chars 12000 \
    [--goldens "app/vault/**/golden/*.jsonl"] \
    [--dry-worker]
"""
from __future__ import annotations
import argparse, json, sys, glob
from pathlib import Path

def read_yaml(path: Path) -> dict:
    import yaml
    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def discover_templates(vault_root: Path):
    for pack_dir in sorted(vault_root.glob("*.*")):
        pack_yml = pack_dir / "pack.yml"
        if not pack_yml.exists():
            continue
        pack = read_yaml(pack_yml)
        pack_id = str(pack.get("id") or pack_dir.name.split(".")[0].upper()).upper()
        version = str(pack.get("version"))
        sections = pack.get("sections", [])
        tmpl_dir = pack_dir / "templates"
        for sec in sections:
            p = tmpl_dir / f"{sec}.md"
            yield pack_id, version, sec, p

def groundedness_proxy(md_text: str) -> float:
    lines = [ln.strip() for ln in md_text.splitlines()]
    nonempty = [ln for ln in lines if ln]
    if not nonempty:
        return 0.0
    cited = [ln for ln in nonempty if "[source:" in ln.lower()]
    return len(cited) / max(1, len(nonempty))

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--vault", default="app/vault")
    ap.add_argument("--out", default="artifacts/eval_report.json")
    ap.add_argument("--goldens", default="app/vault/**/golden/*.jsonl")
    ap.add_argument("--min_grounded", type=float, default=0.80)
    ap.add_argument("--max_chars", type=int, default=12000)
    ap.add_argument("--dry-worker", action="store_true", help="placeholder flag for CI parity")
    args = ap.parse_args()

    vault = Path(args.vault)
    out = Path(args.out)
    out.parent.mkdir(parents=True, exist_ok=True)

    results = []
    failures = []

    # Optional: ingest golden hints (caps/overrides)
    # Format (jsonl): {"section":"business_case","max_chars":15000,"min_grounded":0.8}
    golden_overrides = {}
    for path in glob.glob(args.goldens, recursive=True):
        p = Path(path)
        for ln in p.read_text(encoding="utf-8").splitlines():
            if not ln.strip(): continue
            try:
                rec = json.loads(ln)
                sec = rec.get("section")
                if sec:
                    golden_overrides.setdefault(sec, {}).update(rec)
            except Exception:
                # ignore malformed lines
                pass

    for pack_id, version, section, md_path in discover_templates(vault):
        body = md_path.read_text(encoding="utf-8") if md_path.exists() else ""
        g = groundedness_proxy(body)
        char_len = len(body)

        sec_caps = golden_overrides.get(section, {})
        min_g = float(sec_caps.get("min_grounded", args.min_grounded))
        max_c = int(sec_caps.get("max_chars", args.max_chars))

        ok = (g >= min_g) and (char_len <= max_c)
        if not ok:
            reasons = []
            if g < min_g: reasons.append(f"groundedness {g:.2f} < {min_g:.2f}")
            if char_len > max_c: reasons.append(f"chars {char_len} > {max_c}")
            failures.append({
                "pack": pack_id, "version": version, "section": section, "reasons": reasons
            })

        results.append({
            "pack": pack_id,
            "version": version,
            "section": section,
            "groundedness_proxy": round(g, 4),
            "chars": char_len,
            "min_grounded": min_g,
            "max_chars": max_c,
            "path": str(md_path)
        })

    report = {"results": results, "failures": failures}
    out.write_text(json.dumps(report, indent=2), encoding="utf-8")
    print(f"offline_eval: wrote {out} with {len(failures)} failure(s).")
    return 1 if failures else 0

if __name__ == "__main__":
    sys.exit(main())
[offline_eval.py: end]

File: README.md
[README.md: start]
# Tools

This directory contains Python scripts for pack management and CI/CD operations.

## Scripts

- **`lint_packs.py`** - Validates pack manifests and enforces PAS/SCQA structure tokens
- **`build_index_payload.py`** - Builds JSON payloads for Azure Search indexing  
- **`offline_eval.py`** - Lightweight CI evaluation with groundedness metrics
- **`index_packs.py`** - Upserts docs to Azure AI Search via REST API
- **`wire_check.py`** - Verifies indexed docs are searchable

## Usage

See individual script help: `python tools/<script>.py --help`
[README.md: end]

File: wire_check.py
[wire_check.py: start]
#!/usr/bin/env python3
"""
wire_check.py
- Verifies indexed docs are searchable in Azure AI Search.
- Queries by pack@version and reports counts.

Env:
  AZURE_SEARCH_ENDPOINT (e.g., https://<name>.search.windows.net)
  AZURE_SEARCH_INDEX    (e.g., smartai-prompts-v2)
  AZURE_SEARCH_QUERY_KEY

Usage:
  python tools/wire_check.py --packs "psg@1.0.0,edg@1.0.1"
"""
from __future__ import annotations
import argparse, json, os, sys, urllib.request, urllib.parse

def query_search(endpoint: str, index: str, key: str, query: str) -> dict:
    url = f"{endpoint}/indexes/{index}/docs/search?api-version=2023-11-01"
    params = {"search": query, "count": "true"}
    url_with_params = f"{url}&{urllib.parse.urlencode(params)}"
    
    req = urllib.request.Request(url_with_params, headers={"api-key": key})
    with urllib.request.urlopen(req) as resp:
        return json.loads(resp.read().decode("utf-8"))

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--packs", required=True, help='Comma-separated like "psg@1.0.0,edg@1.0.1"')
    args = ap.parse_args()

    endpoint = os.environ.get("AZURE_SEARCH_ENDPOINT", "").rstrip("/")
    index = os.environ.get("AZURE_SEARCH_INDEX", "")
    key = os.environ.get("AZURE_SEARCH_QUERY_KEY", "")

    if not (endpoint and index and key):
        print("ERR: set AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX, AZURE_SEARCH_QUERY_KEY", file=sys.stderr)
        return 2

    packs = [p.strip() for p in args.packs.split(",") if p.strip()]
    all_good = True

    for pack_spec in packs:
        try:
            pack_id, version = pack_spec.split("@", 1)
            # Search for docs with this pack and version
            query = f"pack:{pack_id.upper()} AND version:{version}"
            result = query_search(endpoint, index, key, query)
            count = result.get("@odata.count", 0)
            print(f"{pack_spec}: {count} docs")
            if count == 0:
                all_good = False
        except Exception as e:
            print(f"ERR: {pack_spec}: {e}", file=sys.stderr)
            all_good = False

    return 0 if all_good else 1

if __name__ == "__main__":
    sys.exit(main())
[wire_check.py: end]


Directory Structure:
==================================================
â””â”€â”€ sgdev-smartai-poc/
    â”œâ”€â”€ .github/
    â”‚   â”œâ”€â”€ workflows/
    â”‚   â”‚   â”œâ”€â”€ api-rollback.yml
    â”‚   â”‚   â”œâ”€â”€ api-runfromzip.yml
    â”‚   â”‚   â”œâ”€â”€ ci-packs.yml
    â”‚   â”‚   â””â”€â”€ promote-pack.yml
    â”‚   â”œâ”€â”€ .DS_Store
    â”‚   â””â”€â”€ CODEOWNERS
    â”œâ”€â”€ .pytest_cache/
    â”œâ”€â”€ __pycache__/
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ __pycache__/
    â”‚   â”œâ”€â”€ manifests/
    â”‚   â”‚   â””â”€â”€ edg.yml
    â”‚   â”œâ”€â”€ scripts/
    â”‚   â”‚   â””â”€â”€ load_prompt_packs.py
    â”‚   â”œâ”€â”€ services/
    â”‚   â”‚   â”œâ”€â”€ __pycache__/
    â”‚   â”‚   â”œâ”€â”€ .DS_Store
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ aoai.py
    â”‚   â”‚   â”œâ”€â”€ appcfg.py
    â”‚   â”‚   â”œâ”€â”€ composer.py
    â”‚   â”‚   â”œâ”€â”€ evaluator.py
    â”‚   â”‚   â”œâ”€â”€ prompt_vault.py
    â”‚   â”‚   â”œâ”€â”€ secrets.py
    â”‚   â”‚   â”œâ”€â”€ storage.py
    â”‚   â”‚   â””â”€â”€ taxonomy.py
    â”‚   â”œâ”€â”€ vault/
    â”‚   â”‚   â”œâ”€â”€ EDG.v1/
    â”‚   â”‚   â”‚   â”œâ”€â”€ golden/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ about_company.jsonl
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ about_project.core.jsonl
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ about_project.i_and_p.automation.jsonl
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ business_case.jsonl
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ consultancy_scope.jsonl
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ expansion_plan.market_access.jsonl
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ project_milestones.jsonl
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ project_outcomes.jsonl
    â”‚   â”‚   â”‚   â”œâ”€â”€ templates/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ about_company.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ about_project.core.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ about_project.i_and_p.automation.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ about_project.i_and_p.product_development.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ business_case.manufacturing.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ business_case.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ consultancy_scope.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ expansion_plan.market_access.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ project_milestones.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ project_outcomes.md
    â”‚   â”‚   â”‚   â””â”€â”€ pack.yml
    â”‚   â”‚   â”œâ”€â”€ PSG.v1/
    â”‚   â”‚   â”‚   â”œâ”€â”€ golden/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ business_impact.jsonl
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ compliance_summary.jsonl
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ cost_breakdown.jsonl
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ solution_description.jsonl
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ vendor_quotation.jsonl
    â”‚   â”‚   â”‚   â”œâ”€â”€ templates/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ business_impact.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ compliance_summary.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ cost_breakdown.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ solution_description.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ vendor_quotation.md
    â”‚   â”‚   â”‚   â””â”€â”€ pack.yml
    â”‚   â”‚   â””â”€â”€ _labels.yml
    â”‚   â”œâ”€â”€ .DS_Store
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ main.py
    â”œâ”€â”€ artifacts/
    â”‚   â”œâ”€â”€ bundles/
    â”‚   â”‚   â””â”€â”€ .DS_Store
    â”‚   â”œâ”€â”€ .DS_Store
    â”‚   â”œâ”€â”€ index_docs.json
    â”‚   â””â”€â”€ local_smoke_index_docs.json
    â”œâ”€â”€ docs/
    â”‚   â””â”€â”€ release.md
    â”œâ”€â”€ tools/
    â”‚   â”œâ”€â”€ __pycache__/
    â”‚   â”œâ”€â”€ build_index_payload.py
    â”‚   â”œâ”€â”€ index_packs.py
    â”‚   â”œâ”€â”€ lint_packs.py
    â”‚   â”œâ”€â”€ local_pack_casing_smoke.py
    â”‚   â”œâ”€â”€ offline_eval.py
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ verify_psg_build.py
    â”‚   â””â”€â”€ wire_check.py
    â”œâ”€â”€ .DS_Store
    â”œâ”€â”€ .editorconfig
    â”œâ”€â”€ .gitattributes
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ collect_files.py
    â”œâ”€â”€ openapi_3.1.yaml
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ smartai-prompts-v2.schema.json
    â””â”€â”€ startup.sh

File: .DS_Store
[.DS_Store: start]
[Binary file - 6148 bytes]
[.DS_Store: end]

File: .editorconfig
[.editorconfig: start]
root = true

[*]
charset = utf-8
end_of_line = lf
insert_final_newline = true
trim_trailing_whitespace = true

[*.{py,yml,yaml}]
indent_style = space
indent_size = 2

[*.md]
trim_trailing_whitespace = false

[.editorconfig: end]

File: .gitattributes
[.gitattributes: start]
*.jsonl text eol=lf
*.md    text eol=lf

[.gitattributes: end]

File: .github/.DS_Store
[.DS_Store: start]
[Binary file - 6148 bytes]
[.DS_Store: end]

File: .github/CODEOWNERS
[CODEOWNERS: start]
# Packs â†’ content owners
app/vault/**   @your-handle-or-team

# API â†’ engineers
app/**         @your-eng-team

[CODEOWNERS: end]

File: .github/workflows/api-rollback.yml
[api-rollback.yml: start]
name: API Rollback

on:
  workflow_dispatch:
    inputs:
      bundle_url:
        description: "Public URL or GitHub artifact URL of api_bundle.zip"
        required: true
        type: string

permissions:
  contents: read

jobs:
  rollback:
    runs-on: ubuntu-latest
    environment: dev
    steps:
      - name: Download bundle
        run: |
          curl -L "${{ github.event.inputs.bundle_url }}" -o api_bundle.zip
          echo "Downloaded bundle:"
          ls -lh api_bundle.zip

      - name: Deploy old bundle to Azure (Rollback)
        uses: azure/webapps-deploy@v3
        with:
          app-name: sgdev-smartai-api-01
          publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE }}
          package: api_bundle.zip

      - name: Smoke test
        run: |
          APP_URL="https://sgdev-smartai-api-01.azurewebsites.net"
          curl -fsSL "${APP_URL}/health"
          curl -fsSL "${APP_URL}/v1/debug/whereami"
[api-rollback.yml: end]

File: .github/workflows/api-runfromzip.yml
[api-runfromzip.yml: start]
name: API Run-from-Zip

on:
  push:
    branches: [ main ]
    paths:
      - "app/**"
      - "!app/vault/**"            # do NOT redeploy when only packs change
      - "requirements.txt"
      - ".github/workflows/api-runfromzip.yml"
  workflow_dispatch:

permissions:
  contents: read

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    environment: dev
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Vendor deps into package/ (manylinux2014)
        run: |
          rm -rf package && mkdir -p package
          # Use pypa manylinux2014 image's Python 3.10 pip to install wheels into /work/package
          docker run --rm -v "$PWD":/work quay.io/pypa/manylinux2014_x86_64 \
            /opt/python/cp310-cp310/bin/pip install \
              --upgrade pip \
              --only-binary=:all: \
              -r /work/requirements.txt \
              -t /work/package
          rsync -a app/ package/app/
          cp startup.sh package/
          chmod +x package/startup.sh
          python -V
          ls -la package/ | sed -n '1,120p'

      - name: Zip bundle
        run: |
          cd package
          zip -r ../api_bundle.zip .
          cd -
          echo "Bundle size:"
          ls -lh api_bundle.zip

      - name: Upload bundle artifact (for rollback)
        uses: actions/upload-artifact@v4
        with:
          name: api_bundle.zip
          path: api_bundle.zip
          retention-days: 90

      - name: Deploy to Azure Web App (Run-from-Zip)
        uses: azure/webapps-deploy@v3
        with:
          app-name: sgdev-smartai-api-01   # <-- your App Service name
          publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE }}
          package: api_bundle.zip

      - name: Smoke test /health
        run: |
          set -e
          APP_URL="https://sgdev-smartai-api-01.azurewebsites.net"
          curl -fsSL "${APP_URL}/health"
          curl -fsSL "${APP_URL}/v1/debug/whereami"
[api-runfromzip.yml: end]

File: .github/workflows/ci-packs.yml
[ci-packs.yml: start]
name: CI Packs

on:
  pull_request:
    paths:
      - "app/vault/**"
      - "tools/**"
      - "smartai-prompts-v2.schema.json"

permissions:
  contents: read

jobs:
  ci:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          # ensure yamls & (optional) jsonschema for lint
          pip install pyyaml jsonschema

      - name: Lint packs (schema + structure tokens)
        run: python tools/lint_packs.py --vault app/vault

      - name: Build candidate payload
        run: python tools/build_index_payload.py --status candidate --out artifacts/index_docs.json

      - name: Offline eval (goldens proxy)
        run: |
          python tools/offline_eval.py \
            --vault app/vault \
            --goldens "app/vault/**/golden/*.jsonl" \
            --min_grounded 0.80 \
            --max_chars 12000 \
            --out artifacts/eval_report.json \
            --dry-worker

      - name: Upload CI artifacts
        uses: actions/upload-artifact@v4
        with:
          name: packs-ci-artifacts
          path: artifacts/
[ci-packs.yml: end]

File: .github/workflows/promote-pack.yml
[promote-pack.yml: start]
# (manual; next step)
#
# Sanity checks (manual verification):
# 1. Run ci-packs.yml on a PR that touches a candidate pack and check:
#    - packs-ci-artifacts/index_docs.json is non-empty
#    - packs-ci-artifacts/eval_report.json is generated by offline_eval.py
# 2. Run Promote Pack with packs = "psg@1.0.X" (or psg@1.0.X,edg@1.0.Y)
#    - Confirm promote-artifacts/index_docs.json has docs only for those packs
#    - Confirm promote-artifacts/eval_report.json no longer contains old TEST-pack Mac path,
#      but new entries generated during this workflow
#    - Confirm wire_check.py output logs <PACK@VERSION>: <non-zero> docs for each promoted pack

name: Promote Pack
on:
  workflow_dispatch:
    inputs:
      packs:
        description: "Comma-separated packs, e.g. psg@1.0.0,edg@1.0.1"
        required: true
        type: string

permissions:
  contents: read

jobs:
  promote:
    runs-on: ubuntu-latest
    environment: dev   # <â€” protect this env with required reviewers
    env:
      AZURE_SEARCH_ENDPOINT: https://sgdev-search-01.search.windows.net
      AZURE_SEARCH_INDEX: smartai-prompts-v2
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: "3.10" }   # match App Service
      - run: pip install -r requirements.txt
      - name: Validate packs input
        run: |
          if [ -z "${{ inputs.packs }}" ]; then
            echo "packs input is required"; exit 1
          fi
      - name: Normalise pack input to uppercase
        run: |
          echo "PACKS=$(echo '${{ inputs.packs }}' | tr '[:lower:]' '[:upper:]')" >> $GITHUB_ENV
      - name: Build approved payload
        run: python tools/build_index_payload.py --status approved --packs "$PACKS" --out artifacts/index_docs.json
      - name: Index to Azure Search
        env:
          AZURE_SEARCH_ADMIN_KEY: ${{ secrets.AZURE_SEARCH_ADMIN_KEY }}
        run: python tools/index_packs.py --in artifacts/index_docs.json
      - name: Wire-check
        env:
          AZURE_SEARCH_QUERY_KEY: ${{ secrets.AZURE_SEARCH_QUERY_KEY }}
        run: python tools/wire_check.py --packs "$PACKS"
      - name: Offline eval (goldens proxy)
        run: |
          python tools/offline_eval.py \
            --vault app/vault \
            --goldens "app/vault/**/golden/*.jsonl" \
            --min_grounded 0.80 \
            --max_chars 12000 \
            --out artifacts/eval_report.json \
            --dry-worker
      - name: Upload promote artifacts
        uses: actions/upload-artifact@v4
        with:
          name: promote-artifacts
          path: artifacts/

[promote-pack.yml: end]

File: .gitignore
[.gitignore: start]
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/

# Non-vital script output
output.txt

#zipped
*.zip

# GitHub workflow log files
.github/workflows/sgdev-smartai-api-01-Log stream
.github/workflows/workflow-API-Run-from-Zip-logs

# CI/CD build logs (contain sensitive data)
lint_check.txt
[.gitignore: end]

File: app/.DS_Store
[.DS_Store: start]
[Binary file - 6148 bytes]
[.DS_Store: end]

File: app/__init__.py
[__init__.py: start]
[__init__.py: end]

File: app/main.py
[main.py: start]
from typing import Any
from fastapi import FastAPI, UploadFile, Form, Query, HTTPException, Response
from pydantic import BaseModel, Field
from app.services import storage, taxonomy, composer, evaluator
from app.services.aoai import chat_completion
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from app.services.appcfg import get_bool, get as cfg_get
from app.services.prompt_vault import _resolve_pack as _pv_resolve
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
import os
import json

#test

app = FastAPI(title="SmartAI Proposal Builder (Dev)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=[ "https://wonderful-pebble-0bc6fc600.1.azurestaticapps.net" ],
    allow_methods=["*"], allow_headers=["*"]
)

class NoCache(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        resp = await call_next(request)
        resp.headers["Cache-Control"] = "no-store"
        return resp
app.add_middleware(NoCache)

# health + root so the platform has a quick 200
@app.get("/")
def root():
    return {"ok": True, "service": "smartai-api"}

@app.get("/health")
def health():
    return {"ok": True}

@app.get("/v1/config/features")
def features():
    return {
        "feature_psg_enabled": get_bool("FEATURE_PSG_ENABLED", False),
        "model_worker": cfg_get("MODEL.WORKER", "gpt-4.1-mini-worker"),
        "prompt_pack_active": cfg_get("PROMPT_PACK_ACTIVE", "edg@latest-approved"),
    }

class SessionCreate(BaseModel):
    grant: str = "EDG"
    company_name: str | None = None

# ------------------------------------------------------------
# Generic Fact Schema (base for all session metadata)
# ------------------------------------------------------------
class SessionFactsReq(BaseModel):
    """
    Generic session fact capture for eligibility, profiling, diagnostics.
    Works across grants, lead-gen, vendor profiling, and other use cases.
    """
    # Common across SME-type use cases
    local_equity_pct: float | None = Field(None, ge=0, le=100, description="Local equity percentage")
    turnover: float | None = Field(None, ge=0, description="Annual turnover/revenue")
    headcount: int | None = Field(None, ge=0, description="Number of employees")

    # Grant-specific attestations (optional)
    used_in_singapore: bool | None = Field(None, description="Will the grant outcome be used in Singapore?")
    no_payment_before_application: bool | None = Field(None, description="No payment made before application?")

    # Open extension for other verticals (lead-gen, diagnostics, etc.)
    extra: dict[str, Any] | None = Field(
        None,
        description="Free-form key-value facts, e.g. {'industry':'F&B','budget_range':'<50k'}"
    )

@app.post("/v1/session")
async def create_session(body: SessionCreate):
    table = storage.sessions()
    from uuid import uuid4; sid = f"s_{uuid4().hex[:8]}"
    entity = {"PartitionKey":"session","RowKey":sid,"grant":body.grant,"status":"new"}
    table.upsert_entity(entity)
    return {"session_id": sid}

# ------------------------------------------------------------
# Session Getter (debug / general retrieval)
# ------------------------------------------------------------
@app.get("/v1/session/{sid}")
async def get_session(sid: str):
    """
    Retrieve session metadata including all facts.
    """
    try:
        sess = storage.sessions().get_entity(partition_key="session", row_key=sid)
    except Exception:
        raise HTTPException(status_code=404, detail="Session not found")
    return {"session_id": sid, "session": dict(sess)}

# ------------------------------------------------------------
# Unified Fact Upsert Endpoint
# ------------------------------------------------------------
@app.post("/v1/session/{sid}/facts")
@app.post("/v1/session/{sid}/eligibility")  # backward-compatible alias
async def upsert_session_facts(sid: str, body: SessionFactsReq):
    """
    Upsert structured facts for a session (eligibility, profiling, diagnostics).
    
    This endpoint works as both:
    - /facts: Generic key-value fact capture for any use case
    - /eligibility: Backward-compatible alias for grant eligibility data
    
    Supports:
    - EDG/PSG grant eligibility (local_equity_pct, turnover, headcount)
    - Grant attestations (used_in_singapore, no_payment_before_application)
    - Free-form facts via 'extra' dict for lead-gen, diagnostics, vendor profiling
    """
    try:
        sess = storage.sessions().get_entity(partition_key="session", row_key=sid)
    except Exception:
        raise HTTPException(status_code=404, detail="Session not found")

    # Convert model to dict, excluding unset fields
    payload = body.model_dump(exclude_unset=True)

    # Flatten extra dict if present
    extras = payload.pop("extra", {}) or {}
    
    # Merge structured fields into session
    for k, v in payload.items():
        sess[k] = v
    
    # Merge dynamic facts at same level
    for k, v in extras.items():
        sess[k] = v

    storage.sessions().upsert_entity(sess)
    
    # Return combined facts for verification
    all_facts = payload.copy()
    all_facts.update(extras)
    
    return {"session_id": sid, "facts": all_facts}

# ------------------------------------------------------------
# Validation Stub (non-blocking)
# ------------------------------------------------------------
@app.post("/v1/session/{sid}/validate")
async def validate_session(sid: str):
    """
    Grant-agnostic validation stub.
    Returns validation checks for the session (eligibility, completeness, etc.)
    Currently a non-blocking stub - can be expanded with specific rules later.
    """
    try:
        sess = storage.sessions().get_entity(partition_key="session", row_key=sid)
    except Exception:
        raise HTTPException(status_code=404, detail="Session not found")
    
    # Start empty; you can add simple rules later
    # Example future checks:
    # - Grant-specific eligibility rules
    # - Required evidence completeness
    # - Data quality checks
    return {"session_id": sid, "checks": []}

@app.get("/v1/session/{sid}/checklist")
async def checklist(sid: str):
    # Read the session to know which grant this session is for
    try:
        sess = storage.sessions().get_entity(partition_key="session", row_key=sid)
        grant = (sess.get("grant") or "EDG").upper()
    except Exception:
        # If session not found or table hiccups, fall back safely
        grant = "EDG"

    if grant == "PSG":
        # PSG: uploads + drafts (no variant needed)
        tasks = [
            {"id": "vendor_quotation", "type": "upload"},
            {"id": "cost_breakdown", "type": "upload"},
            {"id": "business_impact", "type": "draft", "section_variant": None},
            {"id": "solution_description", "type": "draft", "section_variant": None},
            # (optional) compliance summary draft for your reviewers/UI
            {"id": "compliance_summary", "type": "draft", "section_variant": None},
        ]
    else:
        # EDG: uploads + drafts (WITH a variant example)
        tasks = [
            {"id": "acra_bizfile", "type": "upload"},
            {"id": "audited_financials", "type": "upload"},
            {"id": "consultancy_scope", "type": "draft", "section_variant": None},
            # Example: drive the "About the Project â€“ I&P (Automation)" variant
            {"id": "about_project", "type": "draft",
             "section_variant": "about_project.i_and_p.automation"},
            # (optional) include a Market Access draft variant
            {"id": "expansion_plan", "type": "draft",
             "section_variant": "expansion_plan.market_access"},
        ]

    return {"session_id": sid, "grant": grant, "tasks": tasks}

class DraftReq(BaseModel):
    session_id: str
    section_id: str
    section_variant: str | None = None
    inputs: dict = {}


# ------------------------------------------------------------
# Shared Draft Helper (grant-agnostic)
# ------------------------------------------------------------
async def _do_draft(req: DraftReq, response: Response, *, pack_hint: str):
    """
    Unified draft logic for any grant type.
    Uses pack_hint to select the appropriate prompt pack (edg, psg, etc.)
    """
    fw = taxonomy.pick_framework(req.section_id)

    # --- Evidence selection rules (EDG + PSG comprehensive defaults) ---
    # 1) If caller provides inputs.evidence_labels (list), use that order.
    # 2) Else if caller provides legacy inputs.evidence_label (single), use it.
    # 3) Else use sensible defaults per section.
    DEFAULT_EVIDENCE_BY_SECTION = {
        # EDG sections
        "business_case": ["acra_bizfile", "audited_financials"],
        "consultancy_scope": ["acra_bizfile"],
        "about_company": ["acra_bizfile", "audited_financials"],
        "about_project": ["acra_bizfile", "audited_financials"],
        "expansion_plan": ["acra_bizfile", "audited_financials"],
        "project_outcomes": ["audited_financials"],
        "project_milestones": ["acra_bizfile"],
        # PSG sections
        "solution_description": ["vendor_quotation", "product_brochure"],
        "vendor_quotation": ["vendor_quotation"],
        "cost_breakdown": ["cost_breakdown"],
        "business_impact": ["vendor_quotation", "cost_breakdown"],
        "compliance_summary": ["vendor_quotation", "cost_breakdown", "deployment_location_proof"],
    }

    labels = None
    try:
        labels = req.inputs.get("evidence_labels")
        if isinstance(labels, str):
            labels = [labels]
    except Exception:
        labels = None
    if not labels:
        single = req.inputs.get("evidence_label")
        if single:
            labels = [single]
    if not labels:
        labels = DEFAULT_EVIDENCE_BY_SECTION.get(req.section_id, [req.section_id])

    # --- Load snippets in order; cap total length ---
    MAX_CHARS = int(req.inputs.get("evidence_char_cap", 6000))
    parts = []
    evidence_used = []
    for label in labels:
        blob_name = f"{req.session_id}_{label}.txt"
        try:
            txt = storage.get_text("evidence", blob_name)
            if not txt:
                continue
            header = f"\n\n--- [evidence:{label}] ---\n"
            parts.append(header + txt)
            evidence_used.append(label)
            if sum(len(p) for p in parts) >= MAX_CHARS:
                break
        except Exception:
            # Missing evidence file is OK; skip
            continue

    snippet = ""
    if parts:
        joined = "".join(parts)
        snippet = joined[:MAX_CHARS]

    # Surface the labels into inputs so the prompt can mention them
    if evidence_used:
        req.inputs["evidence_labels"] = evidence_used
        req.inputs["evidence_label"] = ",".join(evidence_used)  # back-compat for any single-label template

    # --- Pack selection via pack_hint (EDG/PSG/etc.) ---
    try:
        msgs, packver, evidence_order_used = composer.compose_instruction(
            req.section_id, 
            fw, 
            req.inputs or {}, 
            snippet,
            section_variant=req.section_variant,
            pack_hint=pack_hint  # IMPORTANT: drives pack selection
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prompt Vault error: {type(e).__name__}: {e}")

    response.headers["x-prompt-pack"] = packver

    # --- Call AOAI ---
    try:
        out = await chat_completion(msgs, use="worker")
    except ValueError as e:
        raise HTTPException(status_code=400, detail=f"Model deployment error: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AI service error: {str(e)}")

    # --- Soft evaluator ---
    ev = evaluator.score(out, require_tokens=["source:"] if any(c.isdigit() for c in out) else None)

    # --- Lightweight warnings (grant-specific checks) ---
    warnings = []
    try:
        sess = storage.sessions().get_entity(partition_key="session", row_key=req.session_id)
        grant = (sess.get("grant") or "").upper()
        if grant == "PSG":
            equity = float(sess.get("local_equity_pct") or 0)
            if equity < 30:
                warnings.append({
                    "code": "PSG.ELIG.LOCAL_EQUITY_MIN_30",
                    "level": "warning",
                    "message": "Local equity below 30% (PSG minimum).",
                })
    except Exception:
        pass

    return {
        "section_id": req.section_id,
        "framework": fw,
        "evidence_used": evidence_order_used,  # Use the ordered labels from composer
        "output": out,
        "evaluation": ev,
        "warnings": warnings,
    }


# ------------------------------------------------------------
# Unified Draft Endpoint (grant-agnostic)
# ------------------------------------------------------------
@app.post("/v1/draft")
async def draft_any(req: DraftReq, response: Response):
    """
    Grant-agnostic draft endpoint.
    Determines grant type from session and selects appropriate prompt pack.
    """
    # Determine grant/pack from the session
    try:
        sess = storage.sessions().get_entity(partition_key="session", row_key=req.session_id)
    except Exception:
        raise HTTPException(status_code=404, detail="Session not found")
    
    grant = (sess.get("grant") or "EDG").lower()
    return await _do_draft(req, response, pack_hint=grant)


# ------------------------------------------------------------
# Backward-Compatible Grant-Specific Wrappers
# ------------------------------------------------------------
@app.post("/v1/grants/edg/draft")
async def draft_edg(req: DraftReq, response: Response):
    """
    EDG-specific draft endpoint (backward-compatible wrapper).
    Forwards to unified draft logic with pack_hint='edg'.
    """
    return await _do_draft(req, response, pack_hint="edg")


@app.post("/v1/grants/psg/draft")
async def draft_psg(req: DraftReq, response: Response):
    """
    PSG-specific draft endpoint (backward-compatible wrapper).
    Forwards to unified draft logic with pack_hint='psg'.
    """
    return await _do_draft(req, response, pack_hint="psg")

def _strip_label(sid: str, name: str) -> str:
    # safe strip without relying on removeprefix/removesuffix
    pref = f"{sid}_"
    if name.startswith(pref):
        name = name[len(pref):]
    if name.endswith(".txt"):
        name = name[:-4]
    return name

@app.get("/v1/debug/evidence/{sid}")
def debug_list_evidence(sid: str, preview: int = Query(0, ge=0, le=4000)):
    try:
        # 1) list blobs
        blobs = storage.list_blobs("evidence", prefix=f"{sid}_", suffix=".txt")

        # 2) optionally read previews
        items = []
        for name in blobs:
            label = _strip_label(sid, name)
            txt = storage.get_text("evidence", name) if preview else ""
            items.append({
                "name": name,
                "label": label,
                "chars": (len(txt) if txt else None),
                "preview": (txt[:preview] if txt else "")
            })

        return {"session_id": sid, "items": items}

    except Exception as e:
        # Return a clear 500 body so you can see the exact cause in the browser
        raise HTTPException(status_code=500, detail=f"debug_list_evidence failed: {type(e).__name__}: {e}")


# dev-only
@app.get("/v1/debug/packs")
def debug_packs(pack: str = Query("psg"), ver: str = Query("latest-approved")):
    endpoint = os.environ["AZURE_SEARCH_ENDPOINT"].rstrip("/")
    query_key = os.environ["AZURE_SEARCH_QUERY_KEY"]
    index_name = os.environ.get("AZURE_SEARCH_INDEX", "smartai-prompts")

    client = SearchClient(endpoint, index_name, AzureKeyCredential(query_key))

    # Resolve latest-approved â†’ concrete version using the same helper as the vault
    # Also canonicalize pack IDs to uppercase for explicit versions
    if ver == "latest-approved":
        resolved_pack, resolved_ver = _pv_resolve(pack)
    else:
        resolved_pack = pack.strip().upper()
        resolved_ver = ver.strip()

    flt = f"pack_id eq '{resolved_pack}' and status eq 'approved'"
    if resolved_ver != "latest-approved":
        flt += f" and version eq '{resolved_ver}'"

    # IMPORTANT: only select retrievable fields; metadata_json contains section_id/version/template_key
    rs = client.search(
        search_text="*",
        filter=flt,
        top=200,
        select=["metadata_json"],   # <- keep it to this one
    )

    items, sections = [], set()
    for d in rs:
        meta_raw = d.get("metadata_json") or "{}"
        try:
            meta = json.loads(meta_raw)
        except Exception:
            meta = {}
        sid = meta.get("section_id")
        tkey = meta.get("template_key")
        ver  = meta.get("version")
        if sid:
            sections.add(sid)
        items.append({"section_id": sid, "template_key": tkey, "version": ver})

    return {
        "pack": resolved_pack,
        "version": resolved_ver,
        "sections": sorted(s for s in sections if s),
        "items": items
    }


@app.get("/v1/debug/whereami")
def whereami():
    import os
    return {
        "endpoint": os.environ.get("AZURE_SEARCH_ENDPOINT"),
        "index": os.environ.get("AZURE_SEARCH_INDEX", "smartai-prompts-v2"),
        # Don't print keys. Do a minimal query to prove visibility:
        "probe": "ok"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
[main.py: end]

File: app/manifests/edg.yml
[edg.yml: start]
# EDG (Enterprise Data Governance) Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: edg-config
  namespace: default
data:
  # Configuration settings for EDG
  environment: "development"
  log_level: "info"
  max_retries: 3
  timeout: 30
[edg.yml: end]

File: app/scripts/load_prompt_packs.py
[load_prompt_packs.py: start]
import os, json, sys, pathlib, hashlib, yaml, re
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient

SEARCH_ENDPOINT = os.environ["AZURE_SEARCH_ENDPOINT"].rstrip("/")
SEARCH_KEY      = os.environ["AZURE_SEARCH_ADMIN_KEY"]
INDEX_NAME      = os.environ.get("AZURE_SEARCH_INDEX","smartai-prompts")

root = pathlib.Path(__file__).resolve().parents[1]  # repo root

def read_text(p: pathlib.Path) -> str:
    return p.read_text(encoding="utf-8")


_ALLOWED = re.compile(r"[^A-Za-z0-9_\-=]")  # Azure Search key charset

def _safe(s: str) -> str:
    # map anything not allowed to "_"
    return _ALLOWED.sub("_", s)

def doc_id(pack_id: str, version: str, section_id: str, tmpl_key: str) -> str:
    """
    Stable, collision-proof document id:
      - unique on (pack_id, version, section_id, tmpl_key)
      - independent of filename so you can reuse/rename files without creating new docs
    """
    version_s  = version.replace(".", "_")          # e.g. "1.0.1" -> "1_0_1"
    pack_s     = _safe(pack_id)
    section_s  = _safe(section_id)
    tmpl_s     = _safe(tmpl_key)

    # Basis excludes filename on purpose; tmpl_key is the disambiguator for variants
    basis = f"{pack_id}|{version}|{section_id}|{tmpl_key}"
    h = hashlib.sha1(basis.encode("utf-8")).hexdigest()[:12]

    return f"{pack_s}-{version_s}-{section_s}-{tmpl_s}-{h}"


def pack_to_docs(pack_dir: pathlib.Path):
    yml = yaml.safe_load(read_text(pack_dir / "pack.yml"))
    pack_id = yml["pack_id"]
    version = yml["version"]
    pack_status = yml.get("status","draft")
    labels  = yml.get("labels",{})
    templates = yml.get("templates",{})

    for tmpl_key, t in templates.items():
        file_rel = t["file"]
        text     = read_text(pack_dir / file_rel)
        tags     = t.get("retrieval_tags", [])
        rubric   = t.get("rubric", {})

        # Allow explicit override; otherwise use the YAML key as section_id
        section_id = t.get("section_id", tmpl_key)
        
        # Honor per-template status (so each doc gets its own status)
        tmpl_status = t.get("status", pack_status)  # <-- use template-level status if present

        meta = {
            "pack_id": pack_id,
            "version": version,
            "labels": labels,
            "section_id": section_id,
            "rubric": rubric,
            "template_key": tmpl_key,   # helpful for debugging
            "file": file_rel,           # optional: keep for traceability
        }

        yield {
            "id": doc_id(pack_id, version, section_id, tmpl_key),  # ðŸ‘ˆ uses tmpl_key
            "pack_id": pack_id,
            "version": version,
            "status": tmpl_status,  # <-- not the pack-level status
            "section_id": section_id,
            "retrieval_tags": tags,
            "template_text": text,
            "metadata_json": json.dumps(meta, ensure_ascii=False),
        }

def main():
    client = SearchClient(SEARCH_ENDPOINT, INDEX_NAME, AzureKeyCredential(SEARCH_KEY))
    docs = []
    for pack_dir in (root / "vault").glob("*.*"):
        if not (pack_dir / "pack.yml").exists():
            continue
        docs.extend(list(pack_to_docs(pack_dir)))

    # Upload all docs (both approved & draft) so demotions take effect
    r = client.upload_documents(docs)
    failed = [x for x in r if not x.succeeded]
    if failed:
        print("Some docs failed:", failed, file=sys.stderr)
        sys.exit(2)

    # Optional: visibility
    n_total = len(docs)
    n_approved = sum(1 for d in docs if d["status"] == "approved")
    print(f"Uploaded {n_total} docs ({n_approved} approved) to {INDEX_NAME}")

if __name__ == "__main__":
    main()
[load_prompt_packs.py: end]

File: app/services/.DS_Store
[.DS_Store: start]
[Binary file - 6148 bytes]
[.DS_Store: end]

File: app/services/__init__.py
[__init__.py: start]
[__init__.py: end]

File: app/services/aoai.py
[aoai.py: start]
# app/services/aoai.py
import os, httpx
from .secrets import get_secret
from .appcfg import get

def _get_endpoint() -> str:
    ep = os.getenv("AZURE_OPENAI_ENDPOINT")
    if not ep:
        raise RuntimeError("AOAI not configured: set AZURE_OPENAI_ENDPOINT")
    return ep.rstrip("/")

# Pull key from Key Vault at runtime (cached)
def _headers():
    return {"api-key": get_secret("aoai-key-dev"), "Content-Type": "application/json"}

def _deployment(use: str) -> str:
    if use == "manager":
        dep = get("MODEL.MANAGER", default="gpt-4.1-manager")
    else:
        dep = get("MODEL.WORKER", default="gpt-4.1-mini-worker")
    return dep.strip()

async def chat_completion(messages, *, use="worker", max_tokens=800, temperature=0.2, timeout=60):
    endpoint = _get_endpoint()
    dep = _deployment(use)
    #url = f"{endpoint}/openai/deployments/{dep}/chat/completions?api-version=2024-10-01-preview"
    url = f"{endpoint}/openai/deployments/{dep}/chat/completions?api-version=2024-02-15-preview"  # <= use a known-stable version

    payload = {"messages": messages, "max_tokens": max_tokens, "temperature": temperature}
    async with httpx.AsyncClient(timeout=timeout) as client:
        r = await client.post(url, headers=_headers(), json=payload)
    if r.status_code == 404:
        raise ValueError(f"MODEL.WORKER/manager points to unknown deployment: '{dep}'")
    try:
        r.raise_for_status()
    except httpx.HTTPStatusError as e:
        # Surface the AOAI body so you see the real reason in your 500
        raise RuntimeError(f"AOAI {r.status_code}: {r.text[:500]}") from e

    try:
        data = r.json()
    except Exception:
        raise RuntimeError(f"AOAI returned non-JSON ({r.status_code}): {r.text[:500]}")
    return data["choices"][0]["message"]["content"]
[aoai.py: end]

File: app/services/appcfg.py
[appcfg.py: start]
# app/services/appcfg.py
import os, time
from typing import Optional
from azure.identity import DefaultAzureCredential
from azure.appconfiguration import AzureAppConfigurationClient

_ENDPOINT = os.environ["APPCONFIG_ENDPOINT"]
_LABEL   = os.environ.get("APPCONFIG_LABEL", None)
_cred    = DefaultAzureCredential()
_client  = AzureAppConfigurationClient(_ENDPOINT, credential=_cred)

_cache: dict[tuple[str, Optional[str]], tuple[str, float]] = {}  # (key,label) -> (val, expires)

def get(key: str, default: Optional[str] = None, *, ttl_seconds: int = 30) -> str:
    now = time.time()
    k = (key, _LABEL)
    if k in _cache and _cache[k][1] > now: return _cache[k][0]
    try:
        cfg = _client.get_configuration_setting(key=key, label=_LABEL)
        val = cfg.value
    except Exception:
        val = default
    _cache[k] = (val, now + ttl_seconds)
    return val

def get_bool(key: str, default: bool = False) -> bool:
    v = get(key, None)
    if v is None: return default
    return str(v).lower() in ("1","true","yes","on")
[appcfg.py: end]

File: app/services/composer.py
[composer.py: start]
# composer.py
from .prompt_vault import retrieve_template
from .appcfg import get as cfg_get

import re
from typing import Dict, List, Tuple, Any, Optional

# --- tiny mustache-ish helpers (no external deps) ----------------------------

_BLOCK_OPEN = re.compile(r"{{#\s*labels\.([a-zA-Z0-9_]+)\s*}}")
_BLOCK_CLOSE = re.compile(r"{{/\s*labels\.([a-zA-Z0-9_]+)\s*}}")

def _render_label_blocks(text: str, labels: Dict[str, str]) -> str:
    """
    Supports:
      {{#labels.key}} ... [source:{{labels.key}}] ... {{/labels.key}}
    If labels[key] exists, keep inner and replace {{labels.key}} with the value.
    Else, drop the whole block.
    """
    # Find blocks and resolve from inside out
    out = []
    stack = []
    i = 0
    while i < len(text):
        m_open = _BLOCK_OPEN.search(text, i)
        m_close = _BLOCK_CLOSE.search(text, i)

        if m_open and (not m_close or m_open.start() < m_close.start()):
            # Push current segment
            out.append(text[i:m_open.start()])
            stack.append((m_open.group(1), len(out)))  # key, insertion index
            out.append("")  # placeholder for block content
            i = m_open.end()
        elif m_close and stack:
            key = m_close.group(1)
            blk_key, idx = stack.pop()
            # current segment inside the block is out[idx]
            block_inner = "".join(out[idx:]) + text[i:m_close.start()]
            # truncate to idx (remove accumulated inner)
            out = out[:idx]
            if blk_key == key and key in labels:
                # substitute {{labels.key}} -> value
                block_inner = re.sub(r"{{\s*labels\."+re.escape(key)+r"\s*}}", labels[key], block_inner)
                out.append(block_inner)
            # else: drop the whole block (append nothing)
            i = m_close.end()
        else:
            # no more blocks
            out.append(text[i:])
            break

    rendered = "".join(out)
    # Replace any remaining simple {{labels.key}} occurrences
    for k, v in labels.items():
        rendered = re.sub(r"{{\s*labels\."+re.escape(k)+r"\s*}}", v, rendered)
    # Remove any unresolved label refs safely
    rendered = re.sub(r"{{\s*labels\.[a-zA-Z0-9_]+\s*}}", "", rendered)
    # Clean up any double spaces that might have been left behind
    rendered = re.sub(r"\s+", " ", rendered).strip()
    return rendered

# --- evidence label helpers ---------------------------------------------------

_LABEL_HEAD = re.compile(r'---\s*\[evidence:([^\]]+)\]\s*---')

def _extract_labels_from_snippet(snippet: str) -> List[str]:
    return _LABEL_HEAD.findall(snippet or "")

def _ordered_labels(
    available: List[str],
    hints: Dict[str, Any],
    explicit: List[str]
) -> List[str]:
    seen = set()
    order: List[str] = []

    # priority -> optional -> explicit (but keep de-dup and must exist)
    for src in (hints.get("priority_labels") or []):
        if src in available and src not in seen:
            seen.add(src); order.append(src)
    for src in (hints.get("optional_labels") or []):
        if src in available and src not in seen:
            seen.add(src); order.append(src)
    for src in (explicit or []):
        if src in available and src not in seen:
            seen.add(src); order.append(src)

    # append any remaining available labels
    for src in available:
        if src not in seen:
            seen.add(src); order.append(src)
    return order

def _labels_map_from_available(avail: List[str]) -> Dict[str, str]:
    m: Dict[str, str] = {}
    # common
    if "acra_bizfile" in avail:           m["registry"] = "acra_bizfile"
    if "audited_financials" in avail:     m["financials"] = "audited_financials"
    # PSG
    if "vendor_quotation" in avail:       m["vendor_quote"] = "vendor_quotation"
    if "cost_breakdown" in avail:         m["costs"] = "cost_breakdown"
    if "deployment_location_proof" in avail: m["deployment_proof"] = "deployment_location_proof"
    if "annex3_package" in avail:         m["annex3_package"] = "annex3_package"
    # market/evidence hints
    if "market_analysis" in avail:        m["market_analysis"] = "market_analysis"
    if "consultant_proposal" in avail:    m["consultant_proposal"] = "consultant_proposal"
    return m

# --- main entrypoint ----------------------------------------------------------

def compose_instruction(
    section_id: str,
    framework: str,
    inputs: dict,
    evidence_snippet: str = "",
    *,
    section_variant: Optional[str] = None,
    pack_hint: Optional[str] = None,
) -> Tuple[List[Dict[str, str]], str, List[str]]:
    """
    Returns (messages, pack_header, evidence_order_used)
    - messages: for chat completion
    - pack_header: 'pack@version' string (for x-prompt-pack)
    - evidence_order_used: the labels we prioritized
    """

    style = inputs.get("style", "Formal, consultant voice")
    length = int(inputs.get("length_limit", 350))
    grant = (inputs.get("grant") or inputs.get("grant_id") or "edg").lower()
    user_prompt = (inputs.get("prompt") or "").strip()

    # tags help retrieval choose variant-specific prompts too
    tags = [section_id, framework.lower(), grant] + list(inputs.get("tags", []))
    if section_variant:
        # add variant tokens to help retrieval ranking
        tags += section_variant.replace(".", " ").replace("__", " ").split()

    # Retrieve template (+metadata) with awareness of variant & pack if provided
    tpl_obj = retrieve_template(
        section_id,
        tags=tags,
        section_variant=section_variant,   # <-- supports Day-7 delta
        pack_hint=pack_hint
    ) or {}

    tpl = tpl_obj.get("template") or ""
    metadata = tpl_obj.get("metadata", {})
    pack_header = f"{tpl_obj.get('pack_id','unknown')}@{tpl_obj.get('version','0.0.0')}"

    # Evidence selection
    hints = metadata.get("evidence_hints", {}) or {}
    available = _extract_labels_from_snippet(evidence_snippet)

    # if caller passed explicit labels but snippet is empty, still respect explicit
    if not available and inputs.get("evidence_labels"):
        # allow composer to order explicit labels by hints anyway
        available = list(dict.fromkeys(inputs["evidence_labels"]))  # preserve order, de-dup

    chosen_order = _ordered_labels(
        available=available,
        hints=hints,
        explicit=inputs.get("evidence_labels", [])
    )

    # Build labels map for optional blocks
    labels_map = _labels_map_from_available(chosen_order)

    # Evidence window
    cap_cfg = cfg_get("EVIDENCE_CHAR_CAP")
    cap = int(cap_cfg) if str(cap_cfg).isdigit() else 6000
    evidence_window = (evidence_snippet or "")[: cap]

    # Fill core vars first (leave labels blocks untouched here)
    # We use a lightweight replace for {{framework}}, {{style}}, {{length_limit}}, {{evidence_window}}
    def _kv_replace(s: str, kv: Dict[str, str]) -> str:
        for k, v in kv.items():
            s = s.replace("{{" + k + "}}", str(v))
        return s

    prompt_text = _kv_replace(tpl, {
        "framework": framework,
        "style": style,
        "length_limit": str(length),
        "evidence_window": evidence_window,
        "user_prompt": user_prompt
    })

    # Render optional label blocks + substitute {{labels.*}}
    prompt_text = _render_label_blocks(prompt_text, labels_map)

    # Prepend the operator's free-text prompt so the model MUST address it
    if user_prompt:
        prompt_text = (
            "Operator prompt (must be addressed explicitly): "
            + user_prompt
            + "\n\n"
            + prompt_text
        )

    # Final messages; keep system brief and generic to avoid over-constraining the template
    messages = [
        {
            "role": "system",
            "content": "You are a grant consultant. Use only the provided evidence; cite factual claims with [source:<label>]."
        },
        {
            "role": "user",
            "content": prompt_text
        }
    ]

    return messages, pack_header, chosen_order
[composer.py: end]

File: app/services/evaluator.py
[evaluator.py: start]
def score(text:str, *, require_tokens=None, max_words=400):
    ok = True; fails=[]
    if len(text.split()) > max_words: ok=False; fails.append("length_cap")
    if require_tokens:
        for tok in require_tokens:
            if tok.lower() not in text.lower():
                ok=False; fails.append(f"missing:{tok}")
    return {"score": 85 if ok else 55, "fails": fails}
[evaluator.py: end]

File: app/services/prompt_vault.py
[prompt_vault.py: start]
# app/services/prompt_vault.py
import os, time, json
from typing import Dict, List, Tuple, Optional
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from .appcfg import get as cfg_get

_SEARCH_ENDPOINT = os.environ["AZURE_SEARCH_ENDPOINT"].rstrip("/")
_SEARCH_KEY      = os.environ["AZURE_SEARCH_QUERY_KEY"]  # use *query* key in app svc
_INDEX           = os.environ.get("AZURE_SEARCH_INDEX","smartai-prompts")

_client = SearchClient(_SEARCH_ENDPOINT, _INDEX, AzureKeyCredential(_SEARCH_KEY))

_cache: Dict[Tuple[str,str,str,str], Tuple[dict,float]] = {}
# key=(pack,ver,section,variant) -> (doc, expires)

def _active_pack() -> Tuple[str,str]:
    v = cfg_get("PROMPT_PACK_ACTIVE", "edg@latest-approved")
    if "@" not in v: return v, "latest-approved"
    p, ver = v.split("@",1)
    return p, ver

def _resolve_pack(pack_hint: Optional[str]) -> Tuple[str, str]:
    """
    Resolve pack and version from hint or fall back to active pack.
    Accept forms: "psg", "edg", "psg@1.0.0".

    Returns canonical (PACK_ID_UPPER, version), because index rows store
    pack_id as uppercase (e.g., "PSG", "EDG").
    """
    if pack_hint:
        if "@" in pack_hint:
            p, ver = pack_hint.split("@", 1)
        else:
            p, ver = pack_hint, "latest-approved"
    else:
        p, ver = _active_pack()  # reads PROMPT_PACK_ACTIVE

    p = (p or "").strip()
    ver = (ver or "latest-approved").strip()

    # Canonicalise pack IDs to uppercase -> matches index docs with pack_id="PSG"/"EDG"
    p_norm = p.upper()

    # Map "latest-approved" -> concrete version via per-pack config
    if ver == "latest-approved":
        key = f"PROMPT_PACK_LATEST.{p_norm}"  # e.g. PROMPT_PACK_LATEST.PSG
        pinned = (cfg_get(key) or "").strip()
        if pinned:
            ver = pinned

    return p_norm, ver

def _cache_get(pack, ver, section, variant) -> Optional[dict]:
    key = (pack, ver, section, variant or "")
    item = _cache.get(key)
    if item and item[1] > time.time():
        return item[0]
    return None

def _cache_set(pack, ver, section, variant, doc, ttl=30):
    _cache[(pack,ver,section,variant or "")] = (doc, time.time()+ttl)

def retrieve_template(
    section_id: str,
    tags: Optional[List[str]] = None,
    section_variant: Optional[str] = None,
    pack_hint: Optional[str] = None,
) -> dict:
    """
    Retrieve the template for a given section/pack.

    - pack_hint: "psg", "psg@1.0.3", etc. (resolved via _resolve_pack)
    - If a concrete version is requested (ver != "latest-approved") and not found,
      we hard-fail with LookupError instead of silently downgrading.
    """
    pack, ver = _resolve_pack(pack_hint)
    cached = _cache_get(pack, ver, section_id, section_variant)
    if cached:
        return cached

    flt = f"pack_id eq '{pack}' and status eq 'approved' and section_id eq '{section_id}'"
    if ver != "latest-approved":
        flt += f" and version eq '{ver}'"

    search_text = " ".join(tags or [section_id])
    SELECT_FIELDS = ["template_text", "metadata_json"]

    # Primary search: honour pack + version strictly
    results = _client.search(
        search_text=search_text,
        filter=flt,
        top=3,
        query_type="simple",
        select=SELECT_FIELDS,
    )

    hit: Optional[dict] = None
    for d in results:
        try:
            meta = json.loads(d.get("metadata_json") or "{}")
        except Exception:
            meta = {}
        hit = {
            "template": d.get("template_text", ""),
            "pack_id": meta.get("pack_id"),
            "version": meta.get("version"),
            "metadata": meta,
        }
        break

    # If explicit version requested and nothing found â†’ hard fail
    if not hit and ver != "latest-approved":
        raise LookupError(f"No template found for {pack}@{ver}:{section_id}")

    # Optional, looser fallback only for "latest-approved"
    if not hit and ver == "latest-approved":
        results = _client.search(
            search_text=section_id,
            filter=f"pack_id eq '{pack}' and status eq 'approved' and section_id eq '{section_id}'",
            top=1,
            select=SELECT_FIELDS,
        )
        for d in results:
            try:
                meta = json.loads(d.get("metadata_json") or "{}")
            except Exception:
                meta = {}
            hit = {
                "template": d.get("template_text", ""),
                "pack_id": meta.get("pack_id"),
                "version": meta.get("version"),
                "metadata": meta,
            }
            break

    if not hit:
        raise LookupError(f"No template found for {pack}@{ver}:{section_id}")

    _cache_set(pack, ver, section_id, section_variant, hit)
    return hit
[prompt_vault.py: end]

File: app/services/secrets.py
[secrets.py: start]
# app/services/secrets.py
import os, time
from typing import Optional
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient

_KV_URI = os.environ["KEYVAULT_URI"]
_cred = DefaultAzureCredential()
_client = SecretClient(vault_url=_KV_URI, credential=_cred)

_cache: dict[str, tuple[str, float]] = {}  # name -> (value, expires_at)

def get_secret(name: str, ttl_seconds: int = 900) -> str:
    now = time.time()
    if name in _cache and _cache[name][1] > now:
        return _cache[name][0]
    val = _client.get_secret(name).value
    _cache[name] = (val, now + ttl_seconds)
    return val
[secrets.py: end]

File: app/services/storage.py
[storage.py: start]
import os
from azure.identity import DefaultAzureCredential
from azure.storage.blob import BlobServiceClient
from azure.data.tables import TableServiceClient

ACCOUNT = os.environ["STORAGE_ACCOUNT_NAME"]
CONTAINER_UPLOADS  = os.environ["STORAGE_CONTAINER_UPLOADS"]
CONTAINER_EVIDENCE = os.environ["STORAGE_CONTAINER_EVIDENCE"]
CONTAINER_OUTPUTS  = os.environ["STORAGE_CONTAINER_OUTPUTS"]
CONTAINER_TRACES   = os.environ["STORAGE_CONTAINER_TRACES"]
TABLE_SESSIONS     = os.environ["STORAGE_TABLE_SESSIONS"]

_cred = DefaultAzureCredential()
_blob = BlobServiceClient(f"https://{ACCOUNT}.blob.core.windows.net", credential=_cred)    
_table = TableServiceClient(endpoint=f"https://{ACCOUNT}.table.core.windows.net", credential=_cred)

def list_blobs(container: str, prefix: str = "", suffix: str = "") -> list[str]:
    cc = _blob.get_container_client(container)
    names = []
    for b in cc.list_blobs(name_starts_with=prefix):
        n = b.name
        if not suffix or n.endswith(suffix):
            names.append(n)
    return names

def put_text(container:str, name:str, text:str):
    _blob.get_container_client(container).upload_blob(name, text, overwrite=True)
    return f"https://{ACCOUNT}.blob.core.windows.net/{container}/{name}"

def get_text(container:str, name:str)->str:
    b = _blob.get_container_client(container).download_blob(name)
    return b.content_as_text()

def sessions():
    return _table.get_table_client(table_name=TABLE_SESSIONS)
[storage.py: end]

File: app/services/taxonomy.py
[taxonomy.py: start]
def pick_framework(section_id:str)->str:
    if section_id == "business_case": return "PAS"
    if section_id == "consultancy_scope": return "SCQA"
    return "SCQA"
[taxonomy.py: end]

File: app/vault/_labels.yml
[_labels.yml: start]
# Canonical Evidence Labels Reference
# ===================================
# This file documents the standardized evidence labels available for use
# in pack.yml files. These labels serve as the single source of truth
# for evidence identification across all grant types.

# Usage in pack.yml:
# templates:
#   section_name:
#     evidence_hints:
#       priority_labels: ["label1", "label2"]
#       optional_labels: ["label3", "label4"]

# Common/EDG Evidence Labels (shared between Common and EDG grants)
# =================================================================
common_edg:
  acra_bizfile:
    description: "ACRA BizFile extract containing company registration details"
    type: "upload"
    usage: "Company information, registration status, business activities"
    priority: "high"
    example_sections: ["about_company", "business_case"]
    
  audited_financials:
    description: "Audited financial statements and company financials"
    type: "upload"
    usage: "Financial performance, revenue, costs, funding requirements"
    priority: "high"
    example_sections: ["business_case", "about_company", "project_milestones", "expansion_plan"]
    
  parent_consolidated_fs:
    description: "Parent company consolidated financial statements"
    type: "upload"
    usage: "Parent company financial backing, group financial position"
    priority: "medium"
    example_sections: ["about_company", "business_case"]

# PSG (Productivity Solutions Grant) Evidence Labels
# ==================================================
psg:
  vendor_quotation:
    description: "Vendor quotation matching Annex 3 pre-approved packages"
    type: "upload"
    usage: "Solution pricing, vendor details, package compliance"
    priority: "high"
    example_sections: ["vendor_quotation", "compliance_summary", "business_impact"]
    
  cost_breakdown:
    description: "Detailed cost breakdown of the proposed solution"
    type: "upload"
    usage: "Cost analysis, line items, total pricing, value proposition"
    priority: "high"
    example_sections: ["cost_breakdown", "compliance_summary", "business_impact"]
    
  deployment_location_proof:
    description: "Proof of deployment location (e.g., Singapore address)"
    type: "upload"
    usage: "Location verification, deployment site confirmation"
    priority: "high"
    example_sections: ["compliance_summary"]
    
  product_brochure:
    description: "Product brochure or technical specification document"
    type: "upload"
    usage: "Product features, technical details, capabilities"
    priority: "medium"
    example_sections: ["solution_description", "compliance_summary"]
    
  annex3_package:
    description: "Annex 3 pre-approved package documentation"
    type: "upload"
    usage: "Package compliance, pre-approval verification"
    priority: "medium"
    example_sections: ["compliance_summary"]

# Cross-Grant Labels (if any)
# ============================
# Note: Common/EDG labels are shared between Common and EDG grants.
# PSG labels are specific to PSG grants only.

# Label Naming Conventions
# =========================
# - Use snake_case for all label names
# - Be descriptive and specific
# - Avoid abbreviations unless widely understood (e.g., "fs" for financial statements)
# - Group related labels with common prefixes when applicable

# Priority Guidelines
# ===================
# high: Essential evidence that should always be included when available
# medium: Important evidence that adds value but not strictly required
# low: Supplementary evidence that provides additional context

# Evidence Hints Configuration
# =============================
# Use in pack.yml templates:
#
# evidence_hints:
#   priority_labels: ["label1", "label2"]  # Always try to use these first
#   optional_labels: ["label3", "label4"]  # Use if available and helpful
#
# The system will attempt to load evidence in priority order and stop
# when the character cap (default 6000) is reached.

# Adding New Labels
# ==================
# When adding new evidence labels:
# 1. Add to this file with full documentation
# 2. Update pack.yml files to use the new labels
# 3. Update OpenAPI specification if needed
# 4. Add to composer.py label mapping if referenced in templates
# 5. Create golden test data using the new labels
[_labels.yml: end]

File: app/vault/EDG.v1/golden/about_company.jsonl
[about_company.jsonl: start]
{"id":"edg-ac-001","pack":"EDG","section":"about_company","inputs":{"style":"Formal, outcome-oriented","length_limit":220},"evidence_labels":["acra_bizfile","audited_financials"],"min_grounded":0,"max_chars":2880,"min_citation_coverage":0.6,"structure_tokens":["Year","Key"],"notes":""}
[about_company.jsonl: end]

File: app/vault/EDG.v1/golden/about_project.core.jsonl
[about_project.core.jsonl: start]
{"id":"edg-apc-001","pack":"EDG","section":"about_project","inputs":{"style":"Formal, outcome-oriented","length_limit":260},"evidence_labels":["audited_financials"],"min_grounded":0,"max_chars":3360,"min_citation_coverage":0.5,"structure_tokens":["Current","Challenges","Proposed"],"notes":""}
[about_project.core.jsonl: end]

File: app/vault/EDG.v1/golden/about_project.i_and_p.automation.jsonl
[about_project.i_and_p.automation.jsonl: start]
{"id":"edg-apia-001","pack":"EDG","section":"about_project","inputs":{"style":"Formal, outcome-oriented","length_limit":260},"evidence_labels":["audited_financials"],"min_grounded":0,"max_chars":3360,"min_citation_coverage":0.5,"structure_tokens":["Current","Challenges","Proposed"],"notes":""}
[about_project.i_and_p.automation.jsonl: end]

File: app/vault/EDG.v1/golden/business_case.jsonl
[business_case.jsonl: start]
{"id":"edg-gs-1","pack":"EDG","section":"business_case","inputs":{"style":"Formal, outcome-oriented","length_limit":300},"evidence_labels":["acra_bizfile","audited_financials"],"min_grounded":0,"max_chars":3840,"min_citation_coverage":0.6,"structure_tokens":["Problem","Agitate","Solve"],"notes":""}
[business_case.jsonl: end]

File: app/vault/EDG.v1/golden/consultancy_scope.jsonl
[consultancy_scope.jsonl: start]
{"id":"edg-cs-001","pack":"EDG","section":"consultancy_scope","inputs":{"style":"Formal, outcome-oriented","length_limit":220},"evidence_labels":["acra_bizfile","audited_financials"],"min_grounded":0,"max_chars":6000,"min_citation_coverage":0.6,"structure_tokens":["Situation","Complication","Question","Answer"],"notes":""}

[consultancy_scope.jsonl: end]

File: app/vault/EDG.v1/golden/expansion_plan.market_access.jsonl
[expansion_plan.market_access.jsonl: start]
{"id":"edg-epma-001","pack":"EDG","section":"expansion_plan","inputs":{"style":"Formal, outcome-oriented","length_limit":240},"evidence_labels":["audited_financials"],"min_grounded":0,"max_chars":3120,"min_citation_coverage":0.5,"structure_tokens":["Target Country/Market","Competitors","Competitive Advantage"],"notes":""}
[expansion_plan.market_access.jsonl: end]

File: app/vault/EDG.v1/golden/project_milestones.jsonl
[project_milestones.jsonl: start]
{"id":"edg-pm-001","pack":"EDG","section":"project_milestones","inputs":{"style":"Formal, outcome-oriented","length_limit":200},"evidence_labels":["audited_financials"],"min_grounded":0,"max_chars":2640,"min_citation_coverage":0.4,"structure_tokens":["Phase","Deliverables"],"notes":""}
[project_milestones.jsonl: end]

File: app/vault/EDG.v1/golden/project_outcomes.jsonl
[project_outcomes.jsonl: start]
{"id":"edg-po-001","pack":"EDG","section":"project_outcomes","inputs":{"style":"Formal, outcome-oriented","length_limit":220},"evidence_labels":["audited_financials"],"min_grounded":0,"max_chars":2880,"min_citation_coverage":0.5,"structure_tokens":["Capability Building","Quantitative Outcomes","Qualitative Outcomes"],"notes":""}
[project_outcomes.jsonl: end]

File: app/vault/EDG.v1/pack.yml
[pack.yml: start]
pack_id: EDG
version: 1.0.1
status: approved   # draft | candidate | approved
labels: { grant: EDG, locale: en-SG }

defaults:
  frameworks:
    business_case: PAS
    business_case__manufacturing: PAS
    consultancy_scope: SCQA
    about_company: SCQA
    about_project__core: SCQA
    about_project__i_and_p__automation: SCQA
    about_project__i_and_p__product_development: SCQA
    expansion_plan__market_access: SCQA
    project_outcomes: SCQA
    project_milestones: SCQA
  style: "Formal, outcome-oriented"
  evidence_char_cap: 6000

templates:
  business_case:
    retrieval_tags: ["business_case","edg","pas","generic"]
    section_id: business_case
    file: templates/business_case.md
    rubric:
      required_tokens: ["Problem","Agitate","Solve"]
  consultancy_scope:
    section_id: consultancy_scope
    retrieval_tags: ["scope","edg","scqa","generic"]
    file: templates/consultancy_scope.md
    rubric:
      required_tokens: ["Situation","Complication","Question","Answer"]
  business_case__manufacturing:
    status: draft        
    section_id: business_case
    retrieval_tags: ["business_case","edg","pas","manufacturing"]
    file: templates/business_case.manufacturing.md
    rubric:
      required_tokens: ["Problem","Agitate","Solve"]

  about_company:
    retrieval_tags: ["edg","about_company"]
    file: templates/about_company.md
    rubric:
      required_tokens: ["Year","Key"]
    evidence_hints:
      priority_labels: ["acra_bizfile","audited_financials"]
      optional_labels: ["parent_consolidated_fs"]

  about_project__core:
    section_id: about_project
    retrieval_tags: ["edg","about_project","core"]
    file: templates/about_project.core.md
    rubric:
      required_tokens: ["Current","Challenges","Proposed"]
    evidence_hints:
      priority_labels: ["audited_financials"]
      optional_labels: []

  about_project__i_and_p__automation:
    section_id: about_project
    retrieval_tags: ["edg","about_project","innovation_productivity","automation"]
    file: templates/about_project.i_and_p.automation.md
    rubric:
      required_tokens: ["Current State of Operations","Proposed Automation","Expected Productivity Improvements"]
    evidence_hints:
      priority_labels: ["audited_financials"]
      optional_labels: []

  about_project__i_and_p__product_development:
    section_id: about_project
    retrieval_tags: ["edg","about_project","innovation_productivity","product_development"]
    file: templates/about_project.i_and_p.product_development.md
    rubric:
      required_tokens: ["Product","Market","Barriers","Target"]
    evidence_hints:
      priority_labels: ["audited_financials"]
      optional_labels: []

  expansion_plan__market_access:
    section_id: expansion_plan
    retrieval_tags: ["edg","market_access","expansion_plan"]
    file: templates/expansion_plan.market_access.md
    rubric:
      required_tokens: ["Target Country/Market","Competitors","Competitive Advantage"]
    evidence_hints:
      priority_labels: ["audited_financials"]
      optional_labels: []

  project_outcomes:
    retrieval_tags: ["edg","project_outcomes"]
    file: templates/project_outcomes.md
    rubric:
      required_tokens: ["Capability Building","Quantitative Outcomes","Qualitative Outcomes"]
    evidence_hints:
      priority_labels: ["audited_financials"]
      optional_labels: []

  project_milestones:
    retrieval_tags: ["edg","project_milestones"]
    file: templates/project_milestones.md
    rubric:
      required_tokens: ["Phase","Deliverables"]
    evidence_hints:
      priority_labels: ["audited_financials"]
      optional_labels: []
[pack.yml: end]

File: app/vault/EDG.v1/templates/about_company.md
[about_company.md: start]
You are a grant consultant. Draft the **About the Company** section using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
Cite with [source:<label>]. Use only facts present in the evidence.

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

---
## Situation
Describe the current company context and business environment.

## Complication
Explain the challenges and opportunities the company faces.

## Question
What are the key questions about the company's capabilities and market position?

## Answer
Present the company's strengths, activities, and market position.

## Year of Incorporation
Summarise the company's incorporation year and age using the company registry document. {{#labels.registry}}Prefer [source:{{labels.registry}}].{{/labels.registry}}

## Company Progress & Milestones
Outline notable milestones supported by financial or board documents. {{#labels.financials}}Prefer [source:{{labels.financials}}].{{/labels.financials}}

## Key Business Activities & Products/Services
State main activities and offerings grounded in official records. {{#labels.registry}}Prefer [source:{{labels.registry}}].{{/labels.registry}}

## Key Customer Segments & Markets
Describe customers, segments, and overseas presence grounded in financial or sales evidence. {{#labels.financials}}Prefer [source:{{labels.financials}}].{{/labels.financials}}

## Growth & Internationalisation Plans
Highlight growth targets only if present (plans, projections, minutes).
[about_company.md: end]

File: app/vault/EDG.v1/templates/about_project.core.md
[about_project.core.md: start]
You are a grant consultant. Draft the **About the Project (Core Capabilities)** section using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
Cite with [source:<label>] only for facts present in evidence.

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

---
## Situation
Describe the current business context and project requirements.

## Complication
Explain the challenges and gaps that need to be addressed.

## Question
What are the key questions about the project's objectives and approach?

## Answer
Present the proposed project solution and approach.

## Current State
Summarise existing business operations or processes.

## Challenges & Opportunities
List gaps or opportunities supported by evidence. {{#labels.financials}}Prefer [source:{{labels.financials}}].{{/labels.financials}}

## Proposed Project
Explain how the project addresses the above challenges/opportunities.

## Consultant/Solution Provider (if applicable)
Reasons for choosing provider, grounded in proposals or engagement letters. {{#labels.consultant_proposal}}Prefer [source:{{labels.consultant_proposal}}].{{/labels.consultant_proposal}}
[about_project.core.md: end]

File: app/vault/EDG.v1/templates/about_project.i_and_p.automation.md
[about_project.i_and_p.automation.md: start]
(variant: about_project.i_and_p.automation)

You are a grant consultant. Draft **About the Project â€“ I&P (Automation)** using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
Cite with [source:<label>] only for facts present in evidence.

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

---
## Situation
Describe the current automation context and requirements.

## Complication
Explain the challenges and inefficiencies in current processes.

## Question
What are the key questions about automation implementation and benefits?

## Answer
Present the proposed automation solution and expected improvements.

## Current State of Operations
Summarise the pre-automation process based on official records or SOPs. {{#labels.registry}}Prefer [source:{{labels.registry}}].{{/labels.registry}}

## Challenges
List documented inefficiencies or pain points.

## Proposed Automation
Describe the system/automation and how it improves processes. {{#labels.vendor_quote}}Prefer [source:{{labels.vendor_quote}}].{{/labels.vendor_quote}}

## Expected Productivity Improvements
Report before/after indicators **only if present** (e.g., time saved, error rate). {{#labels.costs}}Prefer [source:{{labels.costs}}].{{/labels.costs}}
[about_project.i_and_p.automation.md: end]

File: app/vault/EDG.v1/templates/about_project.i_and_p.product_development.md
[about_project.i_and_p.product_development.md: start]
(variant: about_project.i_and_p.product_development)

You are a grant consultant. Draft **About the Project â€“ I&P (Product Development)** using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
Cite with [source:<label>] only for facts present in evidence.

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

---
## Situation
Describe the current product development context and market environment.

## Complication
Explain the challenges and opportunities in product development.

## Question
What are the key questions about the product's market potential and development approach?

## Answer
Present the product development strategy and market approach.

## Product/Technology
Describe the product or technology under development.

## Market Comparison
Summarise existing solutions and differentiation. {{#labels.market_analysis}}Prefer [source:{{labels.market_analysis}}].{{/labels.market_analysis}}

## Barriers to Entry
List barriers or unique advantages grounded in evidence.

## Target Market
Define target market (size, geography, niche). {{#labels.market_analysis}}Prefer [source:{{labels.market_analysis}}].{{/labels.market_analysis}}

## Commercialisation Strategy
Summarise pricing, promotion, sales, and distribution if present.
[about_project.i_and_p.product_development.md: end]

File: app/vault/EDG.v1/templates/business_case.manufacturing.md
[business_case.manufacturing.md: start]
You are a grant consultant. Draft the **Business Case** using the {{framework}} pattern. Always suggest solutions which are geared towards Manufacturing optimisation. 

Tone: {{style}}. Max words: {{length_limit}}.
Cite numbers with [source:<label>].

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

## Problem
Identify the manufacturing challenges and optimization requirements.

## Agitate
Highlight the urgency and importance of addressing these manufacturing challenges.

## Solve
Present the proposed manufacturing optimization solution and business case.
[business_case.manufacturing.md: end]

File: app/vault/EDG.v1/templates/business_case.md
[business_case.md: start]
You are a grant consultant. Draft the **Business Case** using the {{framework}} pattern. 

Tone: {{style}}. Max words: {{length_limit}}.
Cite numbers with [source:<label>].

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

## Problem
Identify the business challenges and requirements.

## Agitate
Highlight the urgency and importance of addressing these challenges.

## Solve
Present the proposed solution and business case.
[business_case.md: end]

File: app/vault/EDG.v1/templates/consultancy_scope.md
[consultancy_scope.md: start]
You are a grant consultant. Draft the **Consultancy Scope** using the {{framework}} pattern (SCQA).

Tone: {{style}}. Max words: {{length_limit}}.
Cite any figure, rate, man-day, or date with [source:<label>].
Only use facts found in the evidence window. Do not invent items that are not in evidence.

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

---
## Situation
Briefly describe the current business context and objectives for this EDG project (1â€“2 sentences). Reference factual anchors such as revenue scale, team size, ops footprint, or systems if available. Include citations for any numbers. 

## Complication
Summarise the core pain points that justify external consultancy (process gaps, capability gaps, compliance/productivity issues, data fragmentation, change management, etc.). Keep concise and grounded in the clientâ€™s materials.

## Question
State the key question this engagement must answer (e.g., â€œHow do we redesign process X and implement solution Y to achieve Z outcomes within N months?â€). Keep this as one clear question.

## Answer (Scope of Work)
Outline **what the consultants will do** and **what the client will get**.

### 1) Approach & Methodology
- Brief description of the approach (diagnose â†’ design â†’ pilot â†’ implement â†’ stabilise), adapted to the projectâ€™s context.
- Name any frameworks, toolkits, or benchmarks only if they appear in evidence.
- Call out data collection methods (interviews, shadowing, system logs) if present in evidence.

### 2) Phased Workplan & Deliverables
Break down the project into phases. For each phase, include **objective, activities, deliverables, man-days**. Use only activities/deliverables present in the consultantâ€™s proposal or client brief.

**Phase 1 â€” Discovery / Diagnosis**
- Objective: â€¦
- Activities: â€¦
- Deliverables: â€¦
- Estimated man-days: â€¦ [source:vendor_proposal or consultant_proposal]

**Phase 2 â€” Design**
- Objective: â€¦
- Activities: â€¦
- Deliverables: â€¦
- Estimated man-days: â€¦ [source:vendor_proposal]

**Phase 3 â€” Implementation / Pilot**
- Objective: â€¦
- Activities: â€¦
- Deliverables: â€¦
- Estimated man-days: â€¦ [source:vendor_proposal]

**Phase 4 â€” Stabilisation / Handover**
- Objective: â€¦
- Activities: â€¦
- Deliverables: SOPs, training, handover notes, KPIs baseline, etc. (only if stated in evidence)
- Estimated man-days: â€¦ [source:vendor_proposal]

> If the proposal defines different or fewer phases, mirror those exactly and keep the same structure.

### 3) Team & Credentials
- List named consultants and roles only if present in evidence.  
  Include **man-day rate breakdown** (e.g., Partner, Principal, Consultant) if stated.  
  - Example: Partner S$X/day; Consultant S$Y/day. [source:fee_breakdown]
- For projects where **management consultants** are engaged, include:
  - Summary of **scope of work** drawn from the consultantsâ€™ proposal. [source:consultant_proposal]
  - **Man-day rate breakdown** for each grade. [source:fee_breakdown]
  - **CV highlights** (relevant projects, years experience) for key individuals, if provided. [source:consultant_cvs]
  - **TR 43 or SS 680 certification** details for each consultant; mention certificate IDs or scans if present. [source:consultant_certifications]

### 4) Fee Breakdown (by phase and role)
Provide a transparent breakdown strictly from the proposal:
- **Per phase**: activities covered, man-days, rate(s), **subtotal**.  
- **By role/grade**: rate and allocated man-days.  
- **Total professional fees** and any **out-of-pocket expenses** if stated.  
All figures must appear in evidence; do **not** estimate.  
Use a short bullet/table-like structure in text, e.g.:

- Phase 1: Discovery â€” X man-days Ã— S$R/day = **S$â€¦**. [source:fee_breakdown]  
- Phase 2: Design â€” â€¦  
- Total Professional Fees: **S$â€¦**. [source:fee_breakdown]

### 5) Client Responsibilities & Assumptions
List any assumptions or prerequisites explicitly mentioned in the proposal (e.g., timely access to data/stakeholders, test environment availability, decision cadence, travel policy). [source:consultant_proposal]

### 6) Timeline & Milestones
Summarise duration and key milestones per phase (e.g., â€œWeek 1â€“3 Discovery; Week 4 Design workshops; Week 5â€“8 Pilotâ€). Only include dates/durations present in evidence. [source:project_timeline]

### 7) Governance & Reporting
- Steering or working committee structure, meeting cadence, and artefacts (status reports, RAID logs) **if** stated. [source:consultant_proposal]
- Escalation path and acceptance checkpoints per phase if available.

### 8) Risks & Mitigations
List the **top 3â€“5 engagement risks** that are mentioned or clearly implied in evidence (e.g., data availability, stakeholder bandwidth, integration complexity), each with a mitigation drawn from the proposal/plan. Keep crisp and factual.

### 9) Expected Outcomes & KPIs
Summarise the outcomes the consultancy will enable (process cycle-time reduction, error-rate reduction, adoption targets) **only if** such outcomes/KPIs are present in evidence. If quantitative targets exist, cite them. [source:benefits_case or vendor_proposal]

---
**Output rules (strict)**
- Use concise headings and bullet lists; avoid marketing fluff.
- Do **not** fabricate deliverables, rates, man-days, or certifications.
- Every number must have a citation like [source:fee_breakdown] or [source:consultant_proposal].
- Prefer client/consultant wording where available; otherwise summarise neutrally.
[consultancy_scope.md: end]

File: app/vault/EDG.v1/templates/expansion_plan.market_access.md
[expansion_plan.market_access.md: start]
(variant: expansion_plan.market_access)

You are a grant consultant. Draft the **Expansion Plan (Market Access)** section using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
Cite with [source:<label>]. Use only facts present in evidence.

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

---
## Situation
Describe the current market expansion context and internationalization goals.

## Complication
Explain the challenges and opportunities in market access.

## Question
What are the key questions about market expansion strategy and competitive positioning?

## Answer
Present the market access strategy and competitive advantages.

## Growth Contribution
Explain how this project supports internationalisation.

## Target Country/Market
State the target market and reasons. {{#labels.market_analysis}}Prefer [source:{{labels.market_analysis}}].{{/labels.market_analysis}}

## Competitors
Summarise competitors in target market. {{#labels.market_analysis}}Prefer [source:{{labels.market_analysis}}].{{/labels.market_analysis}}

## Competitive Advantage
Describe USP grounded in evidence.

## Track Record
List past successes in the market if any. {{#labels.financials}}Prefer [source:{{labels.financials}}].{{/labels.financials}}

## Partners/Network
Summarise existing partners or opportunities. {{#labels.consultant_proposal}}Prefer [source:{{labels.consultant_proposal}}].{{/labels.consultant_proposal}}

## Other Benefits
Highlight other benefits only if present in evidence.
[expansion_plan.market_access.md: end]

File: app/vault/EDG.v1/templates/project_milestones.md
[project_milestones.md: start]
You are a grant consultant. Draft the **Project Milestones** section using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
Cite with [source:<label>]. Use only facts present in evidence.

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

---
## Situation
Describe the current project timeline and milestone requirements.

## Complication
Explain the challenges in project planning and execution.

## Question
What are the key questions about project milestones and deliverables?

## Answer
Present the project milestone plan and timeline.

Provide the milestone table as narrative bullets using only dates/durations from evidence:

**Phase** â€” Key Activity
**Start/End (mm/yyyy)** â€” Timeline from evidence
**Deliverables** â€” Outputs as stated

Example (replace only if present):
- Phase 1: Discovery â€” Jan 2025 to Feb 2025. Deliverable: Consultant Report. [source:project_plan]
- Phase 2: Development â€” Mar 2025 to Jun 2025. Deliverable: Prototype. [source:project_plan]
[project_milestones.md: end]

File: app/vault/EDG.v1/templates/project_outcomes.md
[project_outcomes.md: start]
You are a grant consultant. Draft the **Project Outcomes** section using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
Cite with [source:<label>]. Use only facts present in evidence.

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

---
## Situation
Describe the current project outcomes context and expected results.

## Complication
Explain the challenges in achieving project outcomes and benefits.

## Question
What are the key questions about project outcomes and success metrics?

## Answer
Present the expected project outcomes and benefits.

## Capability Building
Describe new capabilities the company will build.

## Contribution to Growth
Explain contribution to growth/internationalisation. {{#labels.financials}}Prefer [source:{{labels.financials}}].{{/labels.financials}}

## Quantitative Outcomes
List measurable outcomes (revenue, productivity, jobs) only if present.

## Qualitative Outcomes
Summarise qualitative benefits (e.g., CX, skills) from evidence.
[project_outcomes.md: end]

File: app/vault/PSG.v1/golden/business_impact.jsonl
[business_impact.jsonl: start]
{"id":"psg-bi-001","pack":"PSG","section":"business_impact","inputs":{"style":"Formal, outcome-oriented","length_limit":280},"evidence_labels":["vendor_quotation","cost_breakdown","business_impact_report"],"min_grounded":0,"max_chars":3600,"min_citation_coverage":0.7,"structure_tokens":["Situation","Complication","Question","Answer"],"notes":"Generic IT solution; expect clear link from features->productivity and citations on every number."}
{"id":"psg-bi-002","pack":"PSG","section":"business_impact","inputs":{"style":"Formal, outcome-oriented","length_limit":260},"evidence_labels":["vendor_quotation","cost_breakdown"],"min_grounded":0,"max_chars":3360,"min_citation_coverage":0.6,"structure_tokens":["Situation","Complication","Question","Answer"],"notes":"Quotation + cost only; model must avoid inventing ROI. OK to state qualitative impact if numbers not present; still cite costs."}
{"id":"psg-bi-003","pack":"PSG","section":"business_impact","inputs":{"style":"Formal, outcome-oriented","length_limit":300},"evidence_labels":["vendor_quotation","cost_breakdown","business_impact_report"],"min_grounded":0,"max_chars":3840,"min_citation_coverage":0.8,"structure_tokens":["Situation","Complication","Question","Answer"],"notes":"Stricter case; business_impact_report contains % time-saved. Output should echo % with citation and remain within word cap."}
[business_impact.jsonl: end]

File: app/vault/PSG.v1/golden/compliance_summary.jsonl
[compliance_summary.jsonl: start]
{"id":"psg-cs-001","pack":"PSG","section":"compliance_summary","inputs":{"style":"Formal, outcome-oriented","length_limit":280},"evidence_labels":["vendor_quotation","cost_breakdown","deployment_location_proof"],"min_grounded":0,"max_chars":3360,"min_citation_coverage":0.70,"structure_tokens":["Annex","Deployment","Payment","Cost","Situation","Complication","Question","Answer"],"notes":"Compliance summary must cover Annex, Deployment, Payment, and Cost sections with citations. Keep neutral; do not infer beyond provided documents."}

[compliance_summary.jsonl: end]

File: app/vault/PSG.v1/golden/cost_breakdown.jsonl
[cost_breakdown.jsonl: start]
{"id":"psg-cb-001","pack":"PSG","section":"cost_breakdown","inputs":{"style":"Formal, outcome-oriented","length_limit":220},"evidence_labels":["cost_breakdown"],"min_grounded":0,"max_chars":1200,"min_citation_coverage":0.7,"structure_tokens":["Problem","Agitate","Solve"],"notes":""}
[cost_breakdown.jsonl: end]

File: app/vault/PSG.v1/golden/solution_description.jsonl
[solution_description.jsonl: start]
{"id":"psg-sd-001","pack":"PSG","section":"solution_description","inputs":{"style":"Formal, outcome-oriented","length_limit":220},"evidence_labels":["vendor_quotation"],"min_grounded":0,"max_chars":2880,"min_citation_coverage":0.5,"structure_tokens":["Situation","Complication","Question","Answer"],"notes":""}
[solution_description.jsonl: end]

File: app/vault/PSG.v1/golden/vendor_quotation.jsonl
[vendor_quotation.jsonl: start]
{"id":"psg-vq-001","pack":"PSG","section":"vendor_quotation","inputs":{"style":"Formal, outcome-oriented","length_limit":220},"evidence_labels":["vendor_quotation"],"min_grounded":0,"max_chars":2880,"min_citation_coverage":0.7,"structure_tokens":["Problem","Agitate","Solve"],"notes":""}
[vendor_quotation.jsonl: end]

File: app/vault/PSG.v1/pack.yml
[pack.yml: start]
pack_id: PSG
version: 1.0.1
status: approved     # draft | candidate | approved
labels:
  grant: PSG
  locale: en-SG

defaults:
  frameworks:
    solution_description: SCQA
    vendor_quotation: PAS
    cost_breakdown: PAS
    business_impact: SCQA
    compliance_summary: SCQA
  style: "Formal, outcome-oriented"
  evidence_char_cap: 6000

templates:
  solution_description:
    retrieval_tags: ["psg","solution","description","scqa"]
    file: templates/solution_description.md
    status: approved
    rubric:
      required_tokens: ["Situation","Complication","Question","Answer"]
  
  vendor_quotation:
    section_id: vendor_quotation
    retrieval_tags: ["psg","vendor","quotation","pas"]
    file: templates/vendor_quotation.md
    status: approved
    rubric:
      required_tokens: ["Problem","Agitate","Solve"]
  
  cost_breakdown:
    section_id: cost_breakdown
    retrieval_tags: ["psg","cost","breakdown","pas"]
    file: templates/cost_breakdown.md
    status: approved
    rubric:
      required_tokens: ["Problem","Agitate","Solve"]

  business_impact:
    section_id: business_impact
    retrieval_tags: ["psg","impact","productivity","scqa"]
    file: templates/business_impact.md
    status: approved
    rubric:
      required_tokens: ["Situation","Complication","Question","Answer"]

  compliance_summary:
    section_id: compliance_summary
    retrieval_tags: ["psg","compliance","summary"]
    file: templates/compliance_summary.md
    status: approved
    rubric:
      required_tokens: ["Annex","Deployment","Payment","Cost", "Situation","Complication","Question","Answer"]
    evidence_hints:
      priority_labels: ["vendor_quotation","cost_breakdown","deployment_location_proof"]
      optional_labels: ["annex3_package","product_brochure"]
[pack.yml: end]

File: app/vault/PSG.v1/templates/business_impact.md
[business_impact.md: start]
You are a grant consultant. Draft the **Business Impact** document for a PSG application using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
Cite facts with [source:<label>] â€” e.g., vendor_quotation, cost_breakdown, business_impact_report.

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

Instructions:
- Explain how the solution will improve productivity, efficiency, or reduce costs.
- Mention quantitative benefits (e.g., % time saved, revenue uplift) if evidence provides figures.
- Highlight alignment with PSG objectives: automation, productivity gains, digital transformation.
- Include qualitative benefits (e.g., improved service delivery, reduced manual errors).
- Note that benefits must be realistic and traceable to uploaded evidence.

## Situation
Describe the current productivity context and business operations.

## Complication
Explain the challenges, inefficiencies, and lost opportunities affecting productivity.

## Question
What are the key questions about productivity improvements and expected business impact?

## Answer
Present the solution and its expected business impact, including quantitative and qualitative benefits.
[business_impact.md: end]

File: app/vault/PSG.v1/templates/compliance_summary.md
[compliance_summary.md: start]
You are a grant consultant. Draft the **Compliance Summary** for a PSG application using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
Cite with [source:<label>] where applicable.

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

---
Summarise the compliance signals visible in the uploaded materials:

# Required Sections
## Annex
State whether line items and unit prices appear to match the pre-approved package. If mismatches appear, note them factually. {{#labels.vendor_quote}}Prefer [source:{{labels.vendor_quote}}].{{/labels.vendor_quote}}{{#labels.annex3_package}} and [source:{{labels.annex3_package}}]{{/labels.annex3_package}}.

## Deployment
Describe deployment location evidence and whether the materials indicate usage in Singapore. {{#labels.deployment_proof}}Prefer [source:{{labels.deployment_proof}}].{{/labels.deployment_proof}}

## Payment
If evidence indicates pre-payment before application, state that risk; otherwise remain silent.

## Cost
Briefly cross-reference quotation and cost schedule for totals and any non-eligible items. {{#labels.costs}}Prefer [source:{{labels.costs}}].{{/labels.costs}}

# Brief 
Create a brief summary in Annex, Deployment, Payment and Cost sections above.

**Situation** - Summarise the current compliance requirements and context in Annex, Deployment, Payment and Cost sections above.

**Complication** - Summarise the challenges in meeting compliance requirements in in Annex, Deployment, Payment and Cost sections above.

**Question** - What are the key compliance considerations and requirements in Annex, Deployment, Payment and Cost sections above?

**Answer** - Present the compliance status and evidence in Annex, Deployment, Payment and Cost sections above.

Keep neutral; do not infer beyond the provided documents.


[compliance_summary.md: end]

File: app/vault/PSG.v1/templates/cost_breakdown.md
[cost_breakdown.md: start]
You are a grant consultant. Draft the **Cost Breakdown** section using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
All costs must come from the uploaded evidence.
Cite with [source:cost_breakdown].

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

Instructions:
- Present costs grouped by category (e.g., software license, equipment, consultancy fees).
- Clearly separate eligible vs non-eligible costs if visible in evidence.
- Provide totals and sub-totals with [source:cost_breakdown].
- Emphasize compliance with PSG rules (e.g., quotation matches Annex 3 of pre-approved package).
- Mention that retrospective payments are not allowed if relevant.

## Problem
Identify the cost challenges and financial requirements for the solution.

## Agitate
Highlight the urgency of cost control, compliance requirements, and budget constraints.

## Solve
Present the detailed cost breakdown, compliance with PSG rules, and financial solution.
[cost_breakdown.md: end]

File: app/vault/PSG.v1/templates/solution_description.md
[solution_description.md: start]
You are a grant consultant. Draft the **Solution Description** section for a PSG proposal using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
Cite facts directly from evidence with [source:<label>].

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

Instructions:
- Always start your output with " *regression-test* "
- Summarize the IT solution, equipment, or consultancy service being applied for.
- Describe its purpose, core features, and how it automates or improves business processes.
- Mention deployment details (e.g., location, equipment specs, vendor name) if available in evidence.
- Emphasize alignment with productivity improvement and automation goals of PSG.
- Use clear, outcome-oriented language that would be acceptable to EnterpriseSG evaluators.

## Situation
Describe the current business context and operational requirements.

## Complication
Explain the challenges, inefficiencies, and gaps in current processes.

## Question
What are the key questions about the solution, its capabilities, and expected outcomes?

## Answer
Present the proposed solution and its capabilities, including deployment details and alignment with PSG goals.
[solution_description.md: end]

File: app/vault/PSG.v1/templates/vendor_quotation.md
[vendor_quotation.md: start]
You are a grant consultant. Draft the **Vendor Quotation Summary** section using the {{framework}} pattern.

Tone: {{style}}. Max words: {{length_limit}}.
All numbers must be grounded in the quotation evidence.
Cite numbers with [source:vendor_quotation].

Context (evidence): {{evidence_window}}
User context: {{user_prompt}}

Instructions:
- Summarize the vendor's official quotation exactly as provided.
- Highlight vendor name, quotation reference, and date if available.
- List key items/services with line-item descriptions.
- Report costs and man-day rates as per the quotation. Do not invent or round numbers.
- Flag if the quotation does not appear to match a pre-approved vendor solution.

## Problem
Identify the procurement challenges and requirements.

## Agitate
Highlight the importance of proper vendor selection and cost validation.

## Solve
Present the vendor quotation details and solution.
[vendor_quotation.md: end]

File: artifacts/.DS_Store
[.DS_Store: start]
[Binary file - 6148 bytes]
[.DS_Store: end]

File: artifacts/bundles/.DS_Store
[.DS_Store: start]
[Binary file - 6148 bytes]
[.DS_Store: end]

File: artifacts/index_docs.json
[index_docs.json: start]
[
  {
    "id": "PSG=1_0_3=solution_description=approved",
    "pack_id": "PSG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "solution_description",
    "retrieval_tags": [
      "psg",
      "solution",
      "description",
      "scqa"
    ],
    "template_text": "You are a grant consultant. Draft the **Solution Description** section for a PSG proposal using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite facts directly from evidence with [source:<label>].\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\nInstructions:\n- Summarize the IT solution, equipment, or consultancy service being applied for.\n- Describe its purpose, core features, and how it automates or improves business processes.\n- Mention deployment details (e.g., location, equipment specs, vendor name) if available in evidence.\n- Emphasize alignment with productivity improvement and automation goals of PSG.\n- Use clear, outcome-oriented language that would be acceptable to EnterpriseSG evaluators.\n- Always start with \"Version 1.0.3\"\n\n## Situation\nDescribe the current business context and operational requirements.\n\n## Complication\nExplain the challenges, inefficiencies, and gaps in current processes.\n\n## Question\nWhat are the key questions about the solution, its capabilities, and expected outcomes?\n\n## Answer\nPresent the proposed solution and its capabilities, including deployment details and alignment with PSG goals.",
    "metadata_json": "{\"path\": \"app/vault/PSG.v1/templates/solution_description.md\", \"labels\": {\"grant\": \"PSG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"PSG\", \"version\": \"1.0.3\", \"section_id\": \"solution_description\", \"template_key\": \"solution_description\"}"
  },
  {
    "id": "PSG=1_0_3=vendor_quotation=approved",
    "pack_id": "PSG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "vendor_quotation",
    "retrieval_tags": [
      "psg",
      "vendor",
      "quotation",
      "pas"
    ],
    "template_text": "You are a grant consultant. Draft the **Vendor Quotation Summary** section using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nAll numbers must be grounded in the quotation evidence.\nCite numbers with [source:vendor_quotation].\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\nInstructions:\n- Summarize the vendor's official quotation exactly as provided.\n- Highlight vendor name, quotation reference, and date if available.\n- List key items/services with line-item descriptions.\n- Report costs and man-day rates as per the quotation. Do not invent or round numbers.\n- Flag if the quotation does not appear to match a pre-approved vendor solution.\n\n## Problem\nIdentify the procurement challenges and requirements.\n\n## Agitate\nHighlight the importance of proper vendor selection and cost validation.\n\n## Solve\nPresent the vendor quotation details and solution.",
    "metadata_json": "{\"path\": \"app/vault/PSG.v1/templates/vendor_quotation.md\", \"labels\": {\"grant\": \"PSG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"PSG\", \"version\": \"1.0.3\", \"section_id\": \"vendor_quotation\", \"template_key\": \"vendor_quotation\"}"
  },
  {
    "id": "PSG=1_0_3=cost_breakdown=approved",
    "pack_id": "PSG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "cost_breakdown",
    "retrieval_tags": [
      "psg",
      "cost",
      "breakdown",
      "pas"
    ],
    "template_text": "You are a grant consultant. Draft the **Cost Breakdown** section using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nAll costs must come from the uploaded evidence.\nCite with [source:cost_breakdown].\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\nInstructions:\n- Present costs grouped by category (e.g., software license, equipment, consultancy fees).\n- Clearly separate eligible vs non-eligible costs if visible in evidence.\n- Provide totals and sub-totals with [source:cost_breakdown].\n- Emphasize compliance with PSG rules (e.g., quotation matches Annex 3 of pre-approved package).\n- Mention that retrospective payments are not allowed if relevant.\n\n## Problem\nIdentify the cost challenges and financial requirements for the solution.\n\n## Agitate\nHighlight the urgency of cost control, compliance requirements, and budget constraints.\n\n## Solve\nPresent the detailed cost breakdown, compliance with PSG rules, and financial solution.",
    "metadata_json": "{\"path\": \"app/vault/PSG.v1/templates/cost_breakdown.md\", \"labels\": {\"grant\": \"PSG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"PSG\", \"version\": \"1.0.3\", \"section_id\": \"cost_breakdown\", \"template_key\": \"cost_breakdown\"}"
  },
  {
    "id": "PSG=1_0_3=business_impact=approved",
    "pack_id": "PSG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "business_impact",
    "retrieval_tags": [
      "psg",
      "impact",
      "productivity",
      "scqa"
    ],
    "template_text": "You are a grant consultant. Draft the **Business Impact** document for a PSG application using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite facts with [source:<label>] â€” e.g., vendor_quotation, cost_breakdown, business_impact_report.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\nInstructions:\n- Explain how the solution will improve productivity, efficiency, or reduce costs.\n- Mention quantitative benefits (e.g., % time saved, revenue uplift) if evidence provides figures.\n- Highlight alignment with PSG objectives: automation, productivity gains, digital transformation.\n- Include qualitative benefits (e.g., improved service delivery, reduced manual errors).\n- Note that benefits must be realistic and traceable to uploaded evidence.\n\n## Situation\nDescribe the current productivity context and business operations.\n\n## Complication\nExplain the challenges, inefficiencies, and lost opportunities affecting productivity.\n\n## Question\nWhat are the key questions about productivity improvements and expected business impact?\n\n## Answer\nPresent the solution and its expected business impact, including quantitative and qualitative benefits.",
    "metadata_json": "{\"path\": \"app/vault/PSG.v1/templates/business_impact.md\", \"labels\": {\"grant\": \"PSG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"PSG\", \"version\": \"1.0.3\", \"section_id\": \"business_impact\", \"template_key\": \"business_impact\"}"
  },
  {
    "id": "PSG=1_0_3=compliance_summary=approved",
    "pack_id": "PSG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "compliance_summary",
    "retrieval_tags": [
      "psg",
      "compliance",
      "summary"
    ],
    "template_text": "You are a grant consultant. Draft the **Compliance Summary** for a PSG application using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite with [source:<label>] where applicable.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n---\nSummarise the compliance signals visible in the uploaded materials:\n\n# Required Sections\n## Annex\nState whether line items and unit prices appear to match the pre-approved package. If mismatches appear, note them factually. {{#labels.vendor_quote}}Prefer [source:{{labels.vendor_quote}}].{{/labels.vendor_quote}}{{#labels.annex3_package}} and [source:{{labels.annex3_package}}]{{/labels.annex3_package}}.\n\n## Deployment\nDescribe deployment location evidence and whether the materials indicate usage in Singapore. {{#labels.deployment_proof}}Prefer [source:{{labels.deployment_proof}}].{{/labels.deployment_proof}}\n\n## Payment\nIf evidence indicates pre-payment before application, state that risk; otherwise remain silent.\n\n## Cost\nBriefly cross-reference quotation and cost schedule for totals and any non-eligible items. {{#labels.costs}}Prefer [source:{{labels.costs}}].{{/labels.costs}}\n\n# Brief \nCreate a brief summary in Annex, Deployment, Payment and Cost sections above.\n\n**Situation** - Summarise the current compliance requirements and context in Annex, Deployment, Payment and Cost sections above.\n\n**Complication** - Summarise the challenges in meeting compliance requirements in in Annex, Deployment, Payment and Cost sections above.\n\n**Question** - What are the key compliance considerations and requirements in Annex, Deployment, Payment and Cost sections above?\n\n**Answer** - Present the compliance status and evidence in Annex, Deployment, Payment and Cost sections above.\n\nKeep neutral; do not infer beyond the provided documents.\n\n\n",
    "metadata_json": "{\"path\": \"app/vault/PSG.v1/templates/compliance_summary.md\", \"labels\": {\"grant\": \"PSG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"PSG\", \"version\": \"1.0.3\", \"section_id\": \"compliance_summary\", \"template_key\": \"compliance_summary\"}"
  },
  {
    "id": "EDG=1_0_3=business_case=approved",
    "pack_id": "EDG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "business_case",
    "retrieval_tags": [
      "business_case",
      "edg",
      "pas",
      "generic"
    ],
    "template_text": "You are a grant consultant. Draft the **Business Case** using the {{framework}} pattern. \n\nTone: {{style}}. Max words: {{length_limit}}.\nCite numbers with [source:<label>].\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n## Problem\nIdentify the business challenges and requirements.\n\n## Agitate\nHighlight the urgency and importance of addressing these challenges.\n\n## Solve\nPresent the proposed solution and business case.",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/business_case.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.3\", \"section_id\": \"business_case\", \"template_key\": \"business_case\"}"
  },
  {
    "id": "EDG=1_0_3=consultancy_scope=approved",
    "pack_id": "EDG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "consultancy_scope",
    "retrieval_tags": [
      "scope",
      "edg",
      "scqa",
      "generic"
    ],
    "template_text": "You are a grant consultant. Draft the **Consultancy Scope** using the {{framework}} pattern (SCQA).\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite any figure, rate, man-day, or date with [source:<label>].\nOnly use facts found in the evidence window. Do not invent items that are not in evidence.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n---\n## Situation\nBriefly describe the current business context and objectives for this EDG project (1â€“2 sentences). Reference factual anchors such as revenue scale, team size, ops footprint, or systems if available. Include citations for any numbers. \n\n## Complication\nSummarise the core pain points that justify external consultancy (process gaps, capability gaps, compliance/productivity issues, data fragmentation, change management, etc.). Keep concise and grounded in the clientâ€™s materials.\n\n## Question\nState the key question this engagement must answer (e.g., â€œHow do we redesign process X and implement solution Y to achieve Z outcomes within N months?â€). Keep this as one clear question.\n\n## Answer (Scope of Work)\nOutline **what the consultants will do** and **what the client will get**.\n\n### 1) Approach & Methodology\n- Brief description of the approach (diagnose â†’ design â†’ pilot â†’ implement â†’ stabilise), adapted to the projectâ€™s context.\n- Name any frameworks, toolkits, or benchmarks only if they appear in evidence.\n- Call out data collection methods (interviews, shadowing, system logs) if present in evidence.\n\n### 2) Phased Workplan & Deliverables\nBreak down the project into phases. For each phase, include **objective, activities, deliverables, man-days**. Use only activities/deliverables present in the consultantâ€™s proposal or client brief.\n\n**Phase 1 â€” Discovery / Diagnosis**\n- Objective: â€¦\n- Activities: â€¦\n- Deliverables: â€¦\n- Estimated man-days: â€¦ [source:vendor_proposal or consultant_proposal]\n\n**Phase 2 â€” Design**\n- Objective: â€¦\n- Activities: â€¦\n- Deliverables: â€¦\n- Estimated man-days: â€¦ [source:vendor_proposal]\n\n**Phase 3 â€” Implementation / Pilot**\n- Objective: â€¦\n- Activities: â€¦\n- Deliverables: â€¦\n- Estimated man-days: â€¦ [source:vendor_proposal]\n\n**Phase 4 â€” Stabilisation / Handover**\n- Objective: â€¦\n- Activities: â€¦\n- Deliverables: SOPs, training, handover notes, KPIs baseline, etc. (only if stated in evidence)\n- Estimated man-days: â€¦ [source:vendor_proposal]\n\n> If the proposal defines different or fewer phases, mirror those exactly and keep the same structure.\n\n### 3) Team & Credentials\n- List named consultants and roles only if present in evidence.  \n  Include **man-day rate breakdown** (e.g., Partner, Principal, Consultant) if stated.  \n  - Example: Partner S$X/day; Consultant S$Y/day. [source:fee_breakdown]\n- For projects where **management consultants** are engaged, include:\n  - Summary of **scope of work** drawn from the consultantsâ€™ proposal. [source:consultant_proposal]\n  - **Man-day rate breakdown** for each grade. [source:fee_breakdown]\n  - **CV highlights** (relevant projects, years experience) for key individuals, if provided. [source:consultant_cvs]\n  - **TR 43 or SS 680 certification** details for each consultant; mention certificate IDs or scans if present. [source:consultant_certifications]\n\n### 4) Fee Breakdown (by phase and role)\nProvide a transparent breakdown strictly from the proposal:\n- **Per phase**: activities covered, man-days, rate(s), **subtotal**.  \n- **By role/grade**: rate and allocated man-days.  \n- **Total professional fees** and any **out-of-pocket expenses** if stated.  \nAll figures must appear in evidence; do **not** estimate.  \nUse a short bullet/table-like structure in text, e.g.:\n\n- Phase 1: Discovery â€” X man-days Ã— S$R/day = **S$â€¦**. [source:fee_breakdown]  \n- Phase 2: Design â€” â€¦  \n- Total Professional Fees: **S$â€¦**. [source:fee_breakdown]\n\n### 5) Client Responsibilities & Assumptions\nList any assumptions or prerequisites explicitly mentioned in the proposal (e.g., timely access to data/stakeholders, test environment availability, decision cadence, travel policy). [source:consultant_proposal]\n\n### 6) Timeline & Milestones\nSummarise duration and key milestones per phase (e.g., â€œWeek 1â€“3 Discovery; Week 4 Design workshops; Week 5â€“8 Pilotâ€). Only include dates/durations present in evidence. [source:project_timeline]\n\n### 7) Governance & Reporting\n- Steering or working committee structure, meeting cadence, and artefacts (status reports, RAID logs) **if** stated. [source:consultant_proposal]\n- Escalation path and acceptance checkpoints per phase if available.\n\n### 8) Risks & Mitigations\nList the **top 3â€“5 engagement risks** that are mentioned or clearly implied in evidence (e.g., data availability, stakeholder bandwidth, integration complexity), each with a mitigation drawn from the proposal/plan. Keep crisp and factual.\n\n### 9) Expected Outcomes & KPIs\nSummarise the outcomes the consultancy will enable (process cycle-time reduction, error-rate reduction, adoption targets) **only if** such outcomes/KPIs are present in evidence. If quantitative targets exist, cite them. [source:benefits_case or vendor_proposal]\n\n---\n**Output rules (strict)**\n- Use concise headings and bullet lists; avoid marketing fluff.\n- Do **not** fabricate deliverables, rates, man-days, or certifications.\n- Every number must have a citation like [source:fee_breakdown] or [source:consultant_proposal].\n- Prefer client/consultant wording where available; otherwise summarise neutrally.",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/consultancy_scope.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.3\", \"section_id\": \"consultancy_scope\", \"template_key\": \"consultancy_scope\"}"
  },
  {
    "id": "EDG=1_0_3=business_case__manufacturing=approved",
    "pack_id": "EDG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "business_case__manufacturing",
    "retrieval_tags": [
      "business_case",
      "edg",
      "pas",
      "manufacturing"
    ],
    "template_text": "[[MISSING TEMPLATE: app/vault/EDG.v1/templates/business_case__manufacturing.md]]",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/business_case__manufacturing.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.3\", \"section_id\": \"business_case__manufacturing\", \"template_key\": \"business_case__manufacturing\"}"
  },
  {
    "id": "EDG=1_0_3=about_company=approved",
    "pack_id": "EDG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "about_company",
    "retrieval_tags": [
      "edg",
      "about_company"
    ],
    "template_text": "You are a grant consultant. Draft the **About the Company** section using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite with [source:<label>]. Use only facts present in the evidence.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n---\n## Situation\nDescribe the current company context and business environment.\n\n## Complication\nExplain the challenges and opportunities the company faces.\n\n## Question\nWhat are the key questions about the company's capabilities and market position?\n\n## Answer\nPresent the company's strengths, activities, and market position.\n\n## Year of Incorporation\nSummarise the company's incorporation year and age using the company registry document. {{#labels.registry}}Prefer [source:{{labels.registry}}].{{/labels.registry}}\n\n## Company Progress & Milestones\nOutline notable milestones supported by financial or board documents. {{#labels.financials}}Prefer [source:{{labels.financials}}].{{/labels.financials}}\n\n## Key Business Activities & Products/Services\nState main activities and offerings grounded in official records. {{#labels.registry}}Prefer [source:{{labels.registry}}].{{/labels.registry}}\n\n## Key Customer Segments & Markets\nDescribe customers, segments, and overseas presence grounded in financial or sales evidence. {{#labels.financials}}Prefer [source:{{labels.financials}}].{{/labels.financials}}\n\n## Growth & Internationalisation Plans\nHighlight growth targets only if present (plans, projections, minutes).",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/about_company.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.3\", \"section_id\": \"about_company\", \"template_key\": \"about_company\"}"
  },
  {
    "id": "EDG=1_0_3=about_project__core=approved",
    "pack_id": "EDG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "about_project__core",
    "retrieval_tags": [
      "edg",
      "about_project",
      "core"
    ],
    "template_text": "[[MISSING TEMPLATE: app/vault/EDG.v1/templates/about_project__core.md]]",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/about_project__core.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.3\", \"section_id\": \"about_project__core\", \"template_key\": \"about_project__core\"}"
  },
  {
    "id": "EDG=1_0_3=about_project__i_and_p__automation=approved",
    "pack_id": "EDG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "about_project__i_and_p__automation",
    "retrieval_tags": [
      "edg",
      "about_project",
      "innovation_productivity",
      "automation"
    ],
    "template_text": "[[MISSING TEMPLATE: app/vault/EDG.v1/templates/about_project__i_and_p__automation.md]]",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/about_project__i_and_p__automation.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.3\", \"section_id\": \"about_project__i_and_p__automation\", \"template_key\": \"about_project__i_and_p__automation\"}"
  },
  {
    "id": "EDG=1_0_3=about_project__i_and_p__product_development=approved",
    "pack_id": "EDG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "about_project__i_and_p__product_development",
    "retrieval_tags": [
      "edg",
      "about_project",
      "innovation_productivity",
      "product_development"
    ],
    "template_text": "[[MISSING TEMPLATE: app/vault/EDG.v1/templates/about_project__i_and_p__product_development.md]]",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/about_project__i_and_p__product_development.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.3\", \"section_id\": \"about_project__i_and_p__product_development\", \"template_key\": \"about_project__i_and_p__product_development\"}"
  },
  {
    "id": "EDG=1_0_3=expansion_plan__market_access=approved",
    "pack_id": "EDG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "expansion_plan__market_access",
    "retrieval_tags": [
      "edg",
      "market_access",
      "expansion_plan"
    ],
    "template_text": "[[MISSING TEMPLATE: app/vault/EDG.v1/templates/expansion_plan__market_access.md]]",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/expansion_plan__market_access.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.3\", \"section_id\": \"expansion_plan__market_access\", \"template_key\": \"expansion_plan__market_access\"}"
  },
  {
    "id": "EDG=1_0_3=project_outcomes=approved",
    "pack_id": "EDG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "project_outcomes",
    "retrieval_tags": [
      "edg",
      "project_outcomes"
    ],
    "template_text": "You are a grant consultant. Draft the **Project Outcomes** section using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite with [source:<label>]. Use only facts present in evidence.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n---\n## Situation\nDescribe the current project outcomes context and expected results.\n\n## Complication\nExplain the challenges in achieving project outcomes and benefits.\n\n## Question\nWhat are the key questions about project outcomes and success metrics?\n\n## Answer\nPresent the expected project outcomes and benefits.\n\n## Capability Building\nDescribe new capabilities the company will build.\n\n## Contribution to Growth\nExplain contribution to growth/internationalisation. {{#labels.financials}}Prefer [source:{{labels.financials}}].{{/labels.financials}}\n\n## Quantitative Outcomes\nList measurable outcomes (revenue, productivity, jobs) only if present.\n\n## Qualitative Outcomes\nSummarise qualitative benefits (e.g., CX, skills) from evidence.\n",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/project_outcomes.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.3\", \"section_id\": \"project_outcomes\", \"template_key\": \"project_outcomes\"}"
  },
  {
    "id": "EDG=1_0_3=project_milestones=approved",
    "pack_id": "EDG",
    "version": "1.0.3",
    "status": "approved",
    "section_id": "project_milestones",
    "retrieval_tags": [
      "edg",
      "project_milestones"
    ],
    "template_text": "You are a grant consultant. Draft the **Project Milestones** section using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite with [source:<label>]. Use only facts present in evidence.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n---\n## Situation\nDescribe the current project timeline and milestone requirements.\n\n## Complication\nExplain the challenges in project planning and execution.\n\n## Question\nWhat are the key questions about project milestones and deliverables?\n\n## Answer\nPresent the project milestone plan and timeline.\n\nProvide the milestone table as narrative bullets using only dates/durations from evidence:\n\n**Phase** â€” Key Activity\n**Start/End (mm/yyyy)** â€” Timeline from evidence\n**Deliverables** â€” Outputs as stated\n\nExample (replace only if present):\n- Phase 1: Discovery â€” Jan 2025 to Feb 2025. Deliverable: Consultant Report. [source:project_plan]\n- Phase 2: Development â€” Mar 2025 to Jun 2025. Deliverable: Prototype. [source:project_plan]\n",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/project_milestones.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-17T17:49:22.173157+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.3\", \"section_id\": \"project_milestones\", \"template_key\": \"project_milestones\"}"
  }
]
[index_docs.json: end]

File: artifacts/local_smoke_index_docs.json
[local_smoke_index_docs.json: start]
[
  {
    "id": "PSG=1_0_1=solution_description=approved",
    "pack_id": "PSG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "solution_description",
    "retrieval_tags": [
      "psg",
      "solution",
      "description",
      "scqa"
    ],
    "template_text": "You are a grant consultant. Draft the **Solution Description** section for a PSG proposal using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite facts directly from evidence with [source:<label>].\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\nInstructions:\n- Always start your output with \" *regression-test* \"\n- Summarize the IT solution, equipment, or consultancy service being applied for.\n- Describe its purpose, core features, and how it automates or improves business processes.\n- Mention deployment details (e.g., location, equipment specs, vendor name) if available in evidence.\n- Emphasize alignment with productivity improvement and automation goals of PSG.\n- Use clear, outcome-oriented language that would be acceptable to EnterpriseSG evaluators.\n\n## Situation\nDescribe the current business context and operational requirements.\n\n## Complication\nExplain the challenges, inefficiencies, and gaps in current processes.\n\n## Question\nWhat are the key questions about the solution, its capabilities, and expected outcomes?\n\n## Answer\nPresent the proposed solution and its capabilities, including deployment details and alignment with PSG goals.",
    "metadata_json": "{\"path\": \"app/vault/PSG.v1/templates/solution_description.md\", \"labels\": {\"grant\": \"PSG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"PSG\", \"version\": \"1.0.1\", \"section_id\": \"solution_description\", \"template_key\": \"solution_description\"}"
  },
  {
    "id": "PSG=1_0_1=vendor_quotation=approved",
    "pack_id": "PSG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "vendor_quotation",
    "retrieval_tags": [
      "psg",
      "vendor",
      "quotation",
      "pas"
    ],
    "template_text": "You are a grant consultant. Draft the **Vendor Quotation Summary** section using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nAll numbers must be grounded in the quotation evidence.\nCite numbers with [source:vendor_quotation].\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\nInstructions:\n- Summarize the vendor's official quotation exactly as provided.\n- Highlight vendor name, quotation reference, and date if available.\n- List key items/services with line-item descriptions.\n- Report costs and man-day rates as per the quotation. Do not invent or round numbers.\n- Flag if the quotation does not appear to match a pre-approved vendor solution.\n\n## Problem\nIdentify the procurement challenges and requirements.\n\n## Agitate\nHighlight the importance of proper vendor selection and cost validation.\n\n## Solve\nPresent the vendor quotation details and solution.",
    "metadata_json": "{\"path\": \"app/vault/PSG.v1/templates/vendor_quotation.md\", \"labels\": {\"grant\": \"PSG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"PSG\", \"version\": \"1.0.1\", \"section_id\": \"vendor_quotation\", \"template_key\": \"vendor_quotation\"}"
  },
  {
    "id": "PSG=1_0_1=cost_breakdown=approved",
    "pack_id": "PSG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "cost_breakdown",
    "retrieval_tags": [
      "psg",
      "cost",
      "breakdown",
      "pas"
    ],
    "template_text": "You are a grant consultant. Draft the **Cost Breakdown** section using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nAll costs must come from the uploaded evidence.\nCite with [source:cost_breakdown].\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\nInstructions:\n- Present costs grouped by category (e.g., software license, equipment, consultancy fees).\n- Clearly separate eligible vs non-eligible costs if visible in evidence.\n- Provide totals and sub-totals with [source:cost_breakdown].\n- Emphasize compliance with PSG rules (e.g., quotation matches Annex 3 of pre-approved package).\n- Mention that retrospective payments are not allowed if relevant.\n\n## Problem\nIdentify the cost challenges and financial requirements for the solution.\n\n## Agitate\nHighlight the urgency of cost control, compliance requirements, and budget constraints.\n\n## Solve\nPresent the detailed cost breakdown, compliance with PSG rules, and financial solution.",
    "metadata_json": "{\"path\": \"app/vault/PSG.v1/templates/cost_breakdown.md\", \"labels\": {\"grant\": \"PSG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"PSG\", \"version\": \"1.0.1\", \"section_id\": \"cost_breakdown\", \"template_key\": \"cost_breakdown\"}"
  },
  {
    "id": "PSG=1_0_1=business_impact=approved",
    "pack_id": "PSG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "business_impact",
    "retrieval_tags": [
      "psg",
      "impact",
      "productivity",
      "scqa"
    ],
    "template_text": "You are a grant consultant. Draft the **Business Impact** document for a PSG application using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite facts with [source:<label>] â€” e.g., vendor_quotation, cost_breakdown, business_impact_report.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\nInstructions:\n- Explain how the solution will improve productivity, efficiency, or reduce costs.\n- Mention quantitative benefits (e.g., % time saved, revenue uplift) if evidence provides figures.\n- Highlight alignment with PSG objectives: automation, productivity gains, digital transformation.\n- Include qualitative benefits (e.g., improved service delivery, reduced manual errors).\n- Note that benefits must be realistic and traceable to uploaded evidence.\n\n## Situation\nDescribe the current productivity context and business operations.\n\n## Complication\nExplain the challenges, inefficiencies, and lost opportunities affecting productivity.\n\n## Question\nWhat are the key questions about productivity improvements and expected business impact?\n\n## Answer\nPresent the solution and its expected business impact, including quantitative and qualitative benefits.",
    "metadata_json": "{\"path\": \"app/vault/PSG.v1/templates/business_impact.md\", \"labels\": {\"grant\": \"PSG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"PSG\", \"version\": \"1.0.1\", \"section_id\": \"business_impact\", \"template_key\": \"business_impact\"}"
  },
  {
    "id": "PSG=1_0_1=compliance_summary=approved",
    "pack_id": "PSG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "compliance_summary",
    "retrieval_tags": [
      "psg",
      "compliance",
      "summary"
    ],
    "template_text": "You are a grant consultant. Draft the **Compliance Summary** for a PSG application using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite with [source:<label>] where applicable.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n---\nSummarise the compliance signals visible in the uploaded materials:\n\n# Required Sections\n## Annex\nState whether line items and unit prices appear to match the pre-approved package. If mismatches appear, note them factually. {{#labels.vendor_quote}}Prefer [source:{{labels.vendor_quote}}].{{/labels.vendor_quote}}{{#labels.annex3_package}} and [source:{{labels.annex3_package}}]{{/labels.annex3_package}}.\n\n## Deployment\nDescribe deployment location evidence and whether the materials indicate usage in Singapore. {{#labels.deployment_proof}}Prefer [source:{{labels.deployment_proof}}].{{/labels.deployment_proof}}\n\n## Payment\nIf evidence indicates pre-payment before application, state that risk; otherwise remain silent.\n\n## Cost\nBriefly cross-reference quotation and cost schedule for totals and any non-eligible items. {{#labels.costs}}Prefer [source:{{labels.costs}}].{{/labels.costs}}\n\n# Brief \nCreate a brief summary in Annex, Deployment, Payment and Cost sections above.\n\n**Situation** - Summarise the current compliance requirements and context in Annex, Deployment, Payment and Cost sections above.\n\n**Complication** - Summarise the challenges in meeting compliance requirements in in Annex, Deployment, Payment and Cost sections above.\n\n**Question** - What are the key compliance considerations and requirements in Annex, Deployment, Payment and Cost sections above?\n\n**Answer** - Present the compliance status and evidence in Annex, Deployment, Payment and Cost sections above.\n\nKeep neutral; do not infer beyond the provided documents.\n\n\n",
    "metadata_json": "{\"path\": \"app/vault/PSG.v1/templates/compliance_summary.md\", \"labels\": {\"grant\": \"PSG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"PSG\", \"version\": \"1.0.1\", \"section_id\": \"compliance_summary\", \"template_key\": \"compliance_summary\"}"
  },
  {
    "id": "EDG=1_0_1=business_case=approved",
    "pack_id": "EDG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "business_case",
    "retrieval_tags": [
      "business_case",
      "edg",
      "pas",
      "generic"
    ],
    "template_text": "You are a grant consultant. Draft the **Business Case** using the {{framework}} pattern. \n\nTone: {{style}}. Max words: {{length_limit}}.\nCite numbers with [source:<label>].\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n## Problem\nIdentify the business challenges and requirements.\n\n## Agitate\nHighlight the urgency and importance of addressing these challenges.\n\n## Solve\nPresent the proposed solution and business case.",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/business_case.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.1\", \"section_id\": \"business_case\", \"template_key\": \"business_case\"}"
  },
  {
    "id": "EDG=1_0_1=consultancy_scope=approved",
    "pack_id": "EDG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "consultancy_scope",
    "retrieval_tags": [
      "scope",
      "edg",
      "scqa",
      "generic"
    ],
    "template_text": "You are a grant consultant. Draft the **Consultancy Scope** using the {{framework}} pattern (SCQA).\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite any figure, rate, man-day, or date with [source:<label>].\nOnly use facts found in the evidence window. Do not invent items that are not in evidence.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n---\n## Situation\nBriefly describe the current business context and objectives for this EDG project (1â€“2 sentences). Reference factual anchors such as revenue scale, team size, ops footprint, or systems if available. Include citations for any numbers. \n\n## Complication\nSummarise the core pain points that justify external consultancy (process gaps, capability gaps, compliance/productivity issues, data fragmentation, change management, etc.). Keep concise and grounded in the clientâ€™s materials.\n\n## Question\nState the key question this engagement must answer (e.g., â€œHow do we redesign process X and implement solution Y to achieve Z outcomes within N months?â€). Keep this as one clear question.\n\n## Answer (Scope of Work)\nOutline **what the consultants will do** and **what the client will get**.\n\n### 1) Approach & Methodology\n- Brief description of the approach (diagnose â†’ design â†’ pilot â†’ implement â†’ stabilise), adapted to the projectâ€™s context.\n- Name any frameworks, toolkits, or benchmarks only if they appear in evidence.\n- Call out data collection methods (interviews, shadowing, system logs) if present in evidence.\n\n### 2) Phased Workplan & Deliverables\nBreak down the project into phases. For each phase, include **objective, activities, deliverables, man-days**. Use only activities/deliverables present in the consultantâ€™s proposal or client brief.\n\n**Phase 1 â€” Discovery / Diagnosis**\n- Objective: â€¦\n- Activities: â€¦\n- Deliverables: â€¦\n- Estimated man-days: â€¦ [source:vendor_proposal or consultant_proposal]\n\n**Phase 2 â€” Design**\n- Objective: â€¦\n- Activities: â€¦\n- Deliverables: â€¦\n- Estimated man-days: â€¦ [source:vendor_proposal]\n\n**Phase 3 â€” Implementation / Pilot**\n- Objective: â€¦\n- Activities: â€¦\n- Deliverables: â€¦\n- Estimated man-days: â€¦ [source:vendor_proposal]\n\n**Phase 4 â€” Stabilisation / Handover**\n- Objective: â€¦\n- Activities: â€¦\n- Deliverables: SOPs, training, handover notes, KPIs baseline, etc. (only if stated in evidence)\n- Estimated man-days: â€¦ [source:vendor_proposal]\n\n> If the proposal defines different or fewer phases, mirror those exactly and keep the same structure.\n\n### 3) Team & Credentials\n- List named consultants and roles only if present in evidence.  \n  Include **man-day rate breakdown** (e.g., Partner, Principal, Consultant) if stated.  \n  - Example: Partner S$X/day; Consultant S$Y/day. [source:fee_breakdown]\n- For projects where **management consultants** are engaged, include:\n  - Summary of **scope of work** drawn from the consultantsâ€™ proposal. [source:consultant_proposal]\n  - **Man-day rate breakdown** for each grade. [source:fee_breakdown]\n  - **CV highlights** (relevant projects, years experience) for key individuals, if provided. [source:consultant_cvs]\n  - **TR 43 or SS 680 certification** details for each consultant; mention certificate IDs or scans if present. [source:consultant_certifications]\n\n### 4) Fee Breakdown (by phase and role)\nProvide a transparent breakdown strictly from the proposal:\n- **Per phase**: activities covered, man-days, rate(s), **subtotal**.  \n- **By role/grade**: rate and allocated man-days.  \n- **Total professional fees** and any **out-of-pocket expenses** if stated.  \nAll figures must appear in evidence; do **not** estimate.  \nUse a short bullet/table-like structure in text, e.g.:\n\n- Phase 1: Discovery â€” X man-days Ã— S$R/day = **S$â€¦**. [source:fee_breakdown]  \n- Phase 2: Design â€” â€¦  \n- Total Professional Fees: **S$â€¦**. [source:fee_breakdown]\n\n### 5) Client Responsibilities & Assumptions\nList any assumptions or prerequisites explicitly mentioned in the proposal (e.g., timely access to data/stakeholders, test environment availability, decision cadence, travel policy). [source:consultant_proposal]\n\n### 6) Timeline & Milestones\nSummarise duration and key milestones per phase (e.g., â€œWeek 1â€“3 Discovery; Week 4 Design workshops; Week 5â€“8 Pilotâ€). Only include dates/durations present in evidence. [source:project_timeline]\n\n### 7) Governance & Reporting\n- Steering or working committee structure, meeting cadence, and artefacts (status reports, RAID logs) **if** stated. [source:consultant_proposal]\n- Escalation path and acceptance checkpoints per phase if available.\n\n### 8) Risks & Mitigations\nList the **top 3â€“5 engagement risks** that are mentioned or clearly implied in evidence (e.g., data availability, stakeholder bandwidth, integration complexity), each with a mitigation drawn from the proposal/plan. Keep crisp and factual.\n\n### 9) Expected Outcomes & KPIs\nSummarise the outcomes the consultancy will enable (process cycle-time reduction, error-rate reduction, adoption targets) **only if** such outcomes/KPIs are present in evidence. If quantitative targets exist, cite them. [source:benefits_case or vendor_proposal]\n\n---\n**Output rules (strict)**\n- Use concise headings and bullet lists; avoid marketing fluff.\n- Do **not** fabricate deliverables, rates, man-days, or certifications.\n- Every number must have a citation like [source:fee_breakdown] or [source:consultant_proposal].\n- Prefer client/consultant wording where available; otherwise summarise neutrally.",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/consultancy_scope.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.1\", \"section_id\": \"consultancy_scope\", \"template_key\": \"consultancy_scope\"}"
  },
  {
    "id": "EDG=1_0_1=business_case__manufacturing=approved",
    "pack_id": "EDG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "business_case__manufacturing",
    "retrieval_tags": [
      "business_case",
      "edg",
      "pas",
      "manufacturing"
    ],
    "template_text": "[[MISSING TEMPLATE: app/vault/EDG.v1/templates/business_case__manufacturing.md]]",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/business_case__manufacturing.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.1\", \"section_id\": \"business_case__manufacturing\", \"template_key\": \"business_case__manufacturing\"}"
  },
  {
    "id": "EDG=1_0_1=about_company=approved",
    "pack_id": "EDG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "about_company",
    "retrieval_tags": [
      "edg",
      "about_company"
    ],
    "template_text": "You are a grant consultant. Draft the **About the Company** section using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite with [source:<label>]. Use only facts present in the evidence.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n---\n## Situation\nDescribe the current company context and business environment.\n\n## Complication\nExplain the challenges and opportunities the company faces.\n\n## Question\nWhat are the key questions about the company's capabilities and market position?\n\n## Answer\nPresent the company's strengths, activities, and market position.\n\n## Year of Incorporation\nSummarise the company's incorporation year and age using the company registry document. {{#labels.registry}}Prefer [source:{{labels.registry}}].{{/labels.registry}}\n\n## Company Progress & Milestones\nOutline notable milestones supported by financial or board documents. {{#labels.financials}}Prefer [source:{{labels.financials}}].{{/labels.financials}}\n\n## Key Business Activities & Products/Services\nState main activities and offerings grounded in official records. {{#labels.registry}}Prefer [source:{{labels.registry}}].{{/labels.registry}}\n\n## Key Customer Segments & Markets\nDescribe customers, segments, and overseas presence grounded in financial or sales evidence. {{#labels.financials}}Prefer [source:{{labels.financials}}].{{/labels.financials}}\n\n## Growth & Internationalisation Plans\nHighlight growth targets only if present (plans, projections, minutes).",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/about_company.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.1\", \"section_id\": \"about_company\", \"template_key\": \"about_company\"}"
  },
  {
    "id": "EDG=1_0_1=about_project__core=approved",
    "pack_id": "EDG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "about_project__core",
    "retrieval_tags": [
      "edg",
      "about_project",
      "core"
    ],
    "template_text": "[[MISSING TEMPLATE: app/vault/EDG.v1/templates/about_project__core.md]]",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/about_project__core.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.1\", \"section_id\": \"about_project__core\", \"template_key\": \"about_project__core\"}"
  },
  {
    "id": "EDG=1_0_1=about_project__i_and_p__automation=approved",
    "pack_id": "EDG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "about_project__i_and_p__automation",
    "retrieval_tags": [
      "edg",
      "about_project",
      "innovation_productivity",
      "automation"
    ],
    "template_text": "[[MISSING TEMPLATE: app/vault/EDG.v1/templates/about_project__i_and_p__automation.md]]",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/about_project__i_and_p__automation.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.1\", \"section_id\": \"about_project__i_and_p__automation\", \"template_key\": \"about_project__i_and_p__automation\"}"
  },
  {
    "id": "EDG=1_0_1=about_project__i_and_p__product_development=approved",
    "pack_id": "EDG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "about_project__i_and_p__product_development",
    "retrieval_tags": [
      "edg",
      "about_project",
      "innovation_productivity",
      "product_development"
    ],
    "template_text": "[[MISSING TEMPLATE: app/vault/EDG.v1/templates/about_project__i_and_p__product_development.md]]",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/about_project__i_and_p__product_development.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.1\", \"section_id\": \"about_project__i_and_p__product_development\", \"template_key\": \"about_project__i_and_p__product_development\"}"
  },
  {
    "id": "EDG=1_0_1=expansion_plan__market_access=approved",
    "pack_id": "EDG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "expansion_plan__market_access",
    "retrieval_tags": [
      "edg",
      "market_access",
      "expansion_plan"
    ],
    "template_text": "[[MISSING TEMPLATE: app/vault/EDG.v1/templates/expansion_plan__market_access.md]]",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/expansion_plan__market_access.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.1\", \"section_id\": \"expansion_plan__market_access\", \"template_key\": \"expansion_plan__market_access\"}"
  },
  {
    "id": "EDG=1_0_1=project_outcomes=approved",
    "pack_id": "EDG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "project_outcomes",
    "retrieval_tags": [
      "edg",
      "project_outcomes"
    ],
    "template_text": "You are a grant consultant. Draft the **Project Outcomes** section using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite with [source:<label>]. Use only facts present in evidence.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n---\n## Situation\nDescribe the current project outcomes context and expected results.\n\n## Complication\nExplain the challenges in achieving project outcomes and benefits.\n\n## Question\nWhat are the key questions about project outcomes and success metrics?\n\n## Answer\nPresent the expected project outcomes and benefits.\n\n## Capability Building\nDescribe new capabilities the company will build.\n\n## Contribution to Growth\nExplain contribution to growth/internationalisation. {{#labels.financials}}Prefer [source:{{labels.financials}}].{{/labels.financials}}\n\n## Quantitative Outcomes\nList measurable outcomes (revenue, productivity, jobs) only if present.\n\n## Qualitative Outcomes\nSummarise qualitative benefits (e.g., CX, skills) from evidence.\n",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/project_outcomes.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.1\", \"section_id\": \"project_outcomes\", \"template_key\": \"project_outcomes\"}"
  },
  {
    "id": "EDG=1_0_1=project_milestones=approved",
    "pack_id": "EDG",
    "version": "1.0.1",
    "status": "approved",
    "section_id": "project_milestones",
    "retrieval_tags": [
      "edg",
      "project_milestones"
    ],
    "template_text": "You are a grant consultant. Draft the **Project Milestones** section using the {{framework}} pattern.\n\nTone: {{style}}. Max words: {{length_limit}}.\nCite with [source:<label>]. Use only facts present in evidence.\n\nContext (evidence): {{evidence_window}}\nUser context: {{user_prompt}}\n\n---\n## Situation\nDescribe the current project timeline and milestone requirements.\n\n## Complication\nExplain the challenges in project planning and execution.\n\n## Question\nWhat are the key questions about project milestones and deliverables?\n\n## Answer\nPresent the project milestone plan and timeline.\n\nProvide the milestone table as narrative bullets using only dates/durations from evidence:\n\n**Phase** â€” Key Activity\n**Start/End (mm/yyyy)** â€” Timeline from evidence\n**Deliverables** â€” Outputs as stated\n\nExample (replace only if present):\n- Phase 1: Discovery â€” Jan 2025 to Feb 2025. Deliverable: Consultant Report. [source:project_plan]\n- Phase 2: Development â€” Mar 2025 to Jun 2025. Deliverable: Prototype. [source:project_plan]\n",
    "metadata_json": "{\"path\": \"app/vault/EDG.v1/templates/project_milestones.md\", \"labels\": {\"grant\": \"EDG\", \"locale\": \"en-SG\"}, \"updated_at\": \"2025-11-19T08:26:44.877076+00:00\", \"pack_id\": \"EDG\", \"version\": \"1.0.1\", \"section_id\": \"project_milestones\", \"template_key\": \"project_milestones\"}"
  }
]
[local_smoke_index_docs.json: end]

File: collect_files.py
[collect_files.py: start]
#!/usr/bin/env python3
"""
Script to recursively collect all files from a directory and output them
in a structured text format with tree hierarchy and file contents.
Respects .gitignore patterns.
"""

import os
import argparse
import fnmatch
from pathlib import Path

# Try to import pathspec for better gitignore support, fallback to fnmatch if not available
try:
    import pathspec
    HAS_PATHSPEC = True
except ImportError:
    HAS_PATHSPEC = False


def load_gitignore_patterns(root_dir):
    """Load and parse .gitignore patterns from root directory."""
    gitignore_path = Path(root_dir) / ".gitignore"
    patterns = []
    
    if gitignore_path.exists():
        with open(gitignore_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # Skip empty lines and comments
                if not line or line.startswith('#'):
                    continue
                patterns.append(line)
    
    if not patterns:
        return None
    
    if HAS_PATHSPEC:
        # Use pathspec for proper gitignore pattern matching
        spec = pathspec.PathSpec.from_lines('gitwildmatch', patterns)
        def check_path(rel_path_str):
            # pathspec expects forward slashes
            normalized = rel_path_str.replace('\\', '/')
            return spec.match_file(normalized)
        return check_path
    else:
        # Fallback to fnmatch-based matching
        def should_ignore(rel_path_str):
            # Normalize path separators
            rel_path = rel_path_str.replace('\\', '/')
            # Check each pattern
            for pattern in patterns:
                # Handle directory patterns (ending with /)
                if pattern.endswith('/'):
                    pattern = pattern[:-1]
                    if fnmatch.fnmatch(rel_path, pattern) or rel_path.startswith(pattern + '/'):
                        return True
                # Handle patterns starting with / (root-relative)
                elif pattern.startswith('/'):
                    pattern = pattern[1:]
                    if fnmatch.fnmatch(rel_path, pattern) or rel_path.startswith(pattern + '/'):
                        return True
                # Regular pattern matching - check if pattern matches any part of the path
                else:
                    # Check if pattern matches the path or any parent directory
                    parts = rel_path.split('/')
                    for i in range(len(parts)):
                        subpath = '/'.join(parts[i:])
                        if fnmatch.fnmatch(subpath, pattern) or subpath.startswith(pattern + '/'):
                            return True
            return False
        return should_ignore


def generate_tree(directory, gitignore_check=None):
    """Generate a tree-like structure of the directory, respecting gitignore."""
    tree_lines = []
    root_dir = Path(directory).resolve()
    
    def build_tree(path, prefix="", is_last=True):
        path = Path(path)
        rel_path = path.relative_to(root_dir)
        
        # Check if path should be ignored
        if gitignore_check and gitignore_check(str(rel_path)):
            return
        
        if path.is_file():
            tree_lines.append(f"{prefix}{'â””â”€â”€ ' if is_last else 'â”œâ”€â”€ '}{path.name}")
        elif path.is_dir():
            tree_lines.append(f"{prefix}{'â””â”€â”€ ' if is_last else 'â”œâ”€â”€ '}{path.name}/")
            
            try:
                children = sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name.lower()))
                # Always exclude .git directories
                children = [c for c in children if c.name != '.git']
                # Filter out ignored children
                if gitignore_check:
                    children = [c for c in children if not gitignore_check(str(c.relative_to(root_dir)))]
                
                for i, child in enumerate(children):
                    is_last_child = i == len(children) - 1
                    child_prefix = prefix + ("    " if is_last else "â”‚   ")
                    build_tree(child, child_prefix, is_last_child)
            except PermissionError:
                tree_lines.append(f"{prefix}    [Permission Denied]")
    
    build_tree(root_dir)
    return tree_lines


def collect_files(directory, output_file):
    """
    Recursively collect all files from directory and write to output file.
    Respects .gitignore patterns.
    
    Args:
        directory (str): Source directory to collect files from
        output_file (str): Output file path
    """
    source_dir = Path(directory).resolve()
    output_path = Path(output_file).resolve()
    
    if not source_dir.exists():
        raise FileNotFoundError(f"Source directory '{directory}' does not exist")
    
    # Ensure output directory exists
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Load gitignore patterns
    gitignore_check = load_gitignore_patterns(source_dir)
    if gitignore_check:
        print("Loaded .gitignore patterns")
    else:
        print("No .gitignore found or no patterns loaded")
    
    # Get all files recursively, filtering by gitignore
    all_files = []
    for root, dirs, files in os.walk(source_dir):
        # Always exclude .git directories (not useful source code, can be very large)
        dirs[:] = [d for d in dirs if d != '.git']
        
        # Filter out ignored directories before descending
        if gitignore_check:
            dirs[:] = [d for d in dirs if not gitignore_check(str(Path(root).relative_to(source_dir) / d))]
        
        for file in files:
            file_path = Path(root) / file
            rel_path = file_path.relative_to(source_dir)
            
            # Skip if file matches gitignore pattern
            if gitignore_check and gitignore_check(str(rel_path)):
                continue
            
            all_files.append(file_path)
    
    # Sort files for consistent ordering
    all_files.sort(key=lambda x: str(x).lower())
    
    print(f"Found {len(all_files)} files in '{source_dir}' (after gitignore filtering)")
    print(f"Writing to '{output_path}'")
    
    with open(output_path, 'w', encoding='utf-8') as outfile:
        # Write tree hierarchy
        outfile.write("Directory Structure:\n")
        outfile.write("=" * 50 + "\n")
        tree_lines = generate_tree(source_dir, gitignore_check)
        for line in tree_lines:
            outfile.write(line + "\n")
        outfile.write("\n")
        
        # Write file contents
        for file_path in all_files:
            try:
                # Calculate relative path from source directory
                rel_path = file_path.relative_to(source_dir)
                
                # Write file header
                outfile.write(f"File: {rel_path}\n")
                outfile.write(f"[{file_path.name}: start]\n")
                
                # Read and write file content
                try:
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        content = infile.read()
                        outfile.write(content)
                        
                        # Ensure content ends with newline if it doesn't already
                        if content and not content.endswith('\n'):
                            outfile.write('\n')
                            
                except UnicodeDecodeError:
                    # Handle binary files
                    outfile.write(f"[Binary file - {file_path.stat().st_size} bytes]\n")
                except Exception as e:
                    outfile.write(f"[Error reading file: {str(e)}]\n")
                
                # Write file footer
                outfile.write(f"[{file_path.name}: end]\n")
                outfile.write("\n")
                
            except Exception as e:
                outfile.write(f"Error processing {file_path}: {str(e)}\n")
                outfile.write(f"[{file_path.name}: end]\n")
                outfile.write("\n")
    
    print(f"Successfully wrote {len(all_files)} files to '{output_path}'")


def main():
    parser = argparse.ArgumentParser(
        description="Recursively collect all files from a directory and output them in a structured format"
    )
    parser.add_argument(
        "source_dir", 
        help="Source directory to collect files from (directory A)"
    )
    parser.add_argument(
        "output_file", 
        help="Output file path"
    )
    
    args = parser.parse_args()
    
    try:
        collect_files(args.source_dir, args.output_file)
    except Exception as e:
        print(f"Error: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
[collect_files.py: end]

File: docs/release.md
[release.md: start]
# Release Guide (SmartAI â€“ Day4 flow)

## TL;DR
- **Content (packs) PRs** â†’ CI runs lint/build/offline-eval. **No deploy**.
- After merge, a human runs **Promote Pack** (manual workflow) to publish **approved** docs to Azure AI Search.
- **API code** merges to `main` â†’ **Run-from-Zip** auto-deploys App Service (Oryx disabled).
- **Rollback** = re-deploy the last successful `api_bundle.zip` artifact.

---

## 1) Content changes (packs)
**Who:** content owners / engineers  
**Paths:** `app/vault/**` (packs live at app/vault/), `tools/**`

1. Create PR with template edits.
2. CI **must pass**:
   - `CI Packs`: lint (schema + PAS/SCQA), build payload, offline eval (goldens).
3. Merge PR.
4. Publish to Search:
   - GitHub â†’ **Actions â†’ Promote Pack â†’ Run workflow**
   - Input `packs`, e.g. `psg@1.0.0,edg@1.0.1`
   - Wait for **Wire-check** to print expected counts (e.g., `psg 1.0.0 5`).
5. Verify with API (optional):
   - `GET /v1/debug/packs?pack=psg` shows sections.
   - Draft once; response header includes `x-prompt-pack: psg@1.0.0`.

> Note: Promotion is **manual by design** (governed content release).

---

## 2) API code changes
**Who:** engineers  
**Paths:** `app/**`, `requirements.txt`

1. Merge to `main`.
2. Workflow **API Run-from-Zip** auto-builds:
   - Vendors deps into `package/`, zips â†’ `api_bundle.zip`, deploys.
3. Post-deploy smoke runs:
   - `GET /health` â†’ 200
   - `GET /v1/debug/whereami` â†’ correct index/config

> Path filters prevent `/vault/**` edits from redeploying the API.

---

## 3) Rollback
1. GitHub â†’ Actions â†’ last **API Run-from-Zip** that was green.
2. Download its `api_bundle.zip` artifact (or â€œRe-run jobâ€ to redeploy the same).
3. Re-deploy that artifact.
4. Confirm `/health` and `/v1/debug/whereami`.

---

## 4) Environments & secrets
- **App Service settings:** `SCM_DO_BUILD_DURING_DEPLOYMENT=false`, `WEBSITE_RUN_FROM_PACKAGE=1`.
- **Secrets (env: dev):** `AZURE_WEBAPP_PUBLISH_PROFILE`, `AZURE_SEARCH_ADMIN_KEY`, `AZURE_SEARCH_QUERY_KEY`.
- Future: add `qa`, `stage`, `prod` environments with their own publish profiles.

---

## 5) Quality gates (what makes a PR fail)
- Missing structure token (e.g., PAS lacking **Solve**).
- Golden eval fails: groundedness proxy < 0.8, length > cap, or latency > cap.
- Lint errors: missing required fields/labels in templates.

---

## 6) Responsibilities
- **Content owners:** maintain `/vault/**`, update goldens, run **Promote Pack** after merge.
- **Engineers:** maintain `/app/**`, review CI gates, monitor deploys, own rollback.

---

## 7) References
- Workflows:
  - `.github/workflows/ci-packs.yml`
  - `.github/workflows/promote-pack.yml`
  - `.github/workflows/api-runfromzip.yml`
- Runbooks:
  - `/docs/runbooks/rollback.md` (redeploy last `api_bundle.zip`)
  - `/docs/runbooks/promote-pack.md` (optional, one-pager â€œhow to clickâ€)
[release.md: end]

File: openapi_3.1.yaml
[openapi_3.1.yaml: start]
openapi: 3.1.0
info:
  title: SmartAI API
  version: 1.1.0
  description: >
    Runtime-driven proposal automation & related AI services for grants (EDG, PSG, MRA),
    designed to extend to future use-cases (e.g., lead_generation) via packs, solutions, and section variants.
servers:
  - url: https://api.smartai.local/v1
security:
  - bearerAuth: []

tags:
  - name: Health
  - name: Sessions
  - name: Intake & Recommendations
  - name: Solutions
  - name: Checklist
  - name: Evidence
  - name: Eligibility
  - name: Drafts
  - name: Validation
  - name: Packs
  - name: Catalog & Vendor Docs

paths:
  /health:
    get:
      tags: [Health]
      summary: Health ping
      responses:
        '200': { description: OK }

  /sessions:
    post:
      tags: [Sessions]
      summary: Create a workflow session
      requestBody:
        required: true
        content:
          application/json:
            examples:
              edg:
                summary: EDG session
                value: { grant: EDG, use_case: proposal_builder, company: { uen: "201912345Z", name: "Innovate Pte. Ltd." } }
              psg:
                summary: PSG session
                value: { grant: PSG, use_case: proposal_builder, company: { uen: "202055555A", name: "Alpha SG Pte. Ltd." } }
            schema: { $ref: '#/components/schemas/SessionCreate' }
      responses:
        '201':
          description: Created
          content:
            application/json:
              schema: { $ref: '#/components/schemas/Session' }

  /sessions/{sessionId}:
    get:
      tags: [Sessions]
      summary: Get session
      parameters: [ { $ref: '#/components/parameters/SessionId' } ]
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema: { $ref: '#/components/schemas/Session' }

  /intake:
    post:
      tags: [Intake & Recommendations]
      summary: Start an intake and attach it to a session
      requestBody:
        required: true
        content:
          application/json:
            examples:
              ex:
                value:
                  session_id: s_123
                  problem_statement: "We want to expand into Malaysia and need a digital marketing push."
                  context: { sector: "F&B", target_market: "MY", budget_sgd: 30000 }
            schema:
              type: object
              required: [session_id, problem_statement]
              properties:
                session_id: { type: string }
                problem_statement: { type: string }
                context:
                  type: object
                  additionalProperties: true
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                type: object
                properties:
                  session_id: { type: string }
                  intake_id: { type: string }

  /recommendations:
    post:
      tags: [Intake & Recommendations]
      summary: Get solution recommendations for a problem statement
      requestBody:
        required: true
        content:
          application/json:
            examples:
              ex: { value: { session_id: s_123 } }
            schema:
              type: object
              required: [session_id]
              properties:
                session_id: { type: string }
      responses:
        '200':
          description: OK
          content:
            application/json:
              examples:
                ex:
                  value:
                    items:
                      - id: sol_edg_iap_automation
                        title: "Warehouse Process Automation"
                        grant: EDG
                        category: "Innovation & Productivity"
                        tags: ["Automation"]
                        rationale: "High fit vs. stated fulfilment bottlenecks; quantifiable productivity gains."
                      - id: sol_mra_bd_my
                        title: "In-market Business Development (Malaysia)"
                        grant: MRA
                        category: "Overseas Business Development"
                        tags: ["In-market Business Development"]
                        rationale: "Target market: MY; budget and timing align with MRA caps."
              schema:
                type: object
                properties:
                  items:
                    type: array
                    items: { $ref: '#/components/schemas/SolutionOption' }

  /sessions/{sessionId}/solution:
    post:
      tags: [Solutions]
      summary: Attach chosen solution to session (locks category/variant)
      parameters: [ { $ref: '#/components/parameters/SessionId' } ]
      requestBody:
        required: true
        content:
          application/json:
            examples:
              ex: { value: { solution_id: sol_edg_iap_automation } }
            schema:
              type: object
              required: [solution_id]
              properties:
                solution_id: { type: string }
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                type: object
                properties:
                  session_id: { type: string }
                  solution: { $ref: '#/components/schemas/SolutionOption' }

  /sessions/{sessionId}/checklist:
    get:
      tags: [Checklist]
      summary: Get tasks for this grant/use_case/solution
      parameters: [ { $ref: '#/components/parameters/SessionId' } ]
      responses:
        '200':
          description: OK
          content:
            application/json:
              examples:
                edg_iap:
                  value:
                    session_id: s_123
                    grant: EDG
                    tasks:
                      - { id: "acra_bizfile", type: "upload", title: "ACRA BizFile" }
                      - { id: "audited_financials", type: "upload", title: "Audited Financials" }
                      - { id: "about_company", type: "draft", title: "About the Company" }
                      - { id: "about_project", type: "draft", title: "About the Project", category: "Innovation & Productivity", section_variant: "about_project.i_and_p.automation" }
                      - { id: "project_outcomes", type: "draft", title: "Project Outcomes" }
                psg:
                  value:
                    session_id: s_777
                    grant: PSG
                    tasks:
                      - { id: "vendor_quotation", type: "upload", title: "Vendor Quotation (Annex 3 match)" }
                      - { id: "cost_breakdown", type: "upload", title: "Cost Breakdown" }
                      - { id: "solution_description", type: "draft", title: "Solution Description", category: "IT" }
                      - { id: "business_impact", type: "draft", title: "Business Impact", requires: ["vendor_quotation"] }
              schema:
                type: object
                properties:
                  session_id: { type: string }
                  grant: { $ref: '#/components/schemas/GrantId' }
                  tasks:
                    type: array
                    items: { $ref: '#/components/schemas/ChecklistItem' }

  /sessions/{sessionId}/eligibility:
    post:
      tags: [Eligibility]
      summary: Upsert eligibility facts for this session
      parameters: [ { $ref: '#/components/parameters/SessionId' } ]
      requestBody:
        required: true
        content:
          application/json:
            examples:
              edg: { value: { local_equity_pct: 45, turnover: 1200000, headcount: 18 } }
              psg: { value: { local_equity_pct: 60, group_turnover: 80000000, used_in_singapore: true, no_payment_before_application: true } }
              mra: { value: { local_equity_pct: 40, new_market_sales_3y_under_100k: true, group_headcount: 150 } }
            schema: { $ref: '#/components/schemas/Eligibility' }
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                type: object
                properties:
                  session_id: { type: string }
                  eligibility: { $ref: '#/components/schemas/Eligibility' }

  /evidence:
    post:
      tags: [Evidence]
      summary: Upload evidence file (bind to session + label)
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              type: object
              required: [session_id, label, file]
              properties:
                session_id: { type: string }
                label: { type: string, description: "Must match checklist upload id" }
                file: { type: string, format: binary }
      responses:
        '201':
          description: Stored
          content:
            application/json:
              schema: { $ref: '#/components/schemas/EvidenceReceipt' }

  /evidence/{sessionId}:
    get:
      tags: [Evidence]
      summary: List evidence for session
      parameters: [ { $ref: '#/components/parameters/SessionId' } ]
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                type: object
                properties:
                  session_id: { type: string }
                  items:
                    type: array
                    items: { $ref: '#/components/schemas/EvidenceItem' }

  /drafts:
    post:
      tags: [Drafts]
      summary: Generate a draft section (grant-agnostic)
      description: Selects pack by session grant/use_case/solution unless overridden.
      parameters:
        - in: header
          name: X-Debug-Evidence
          schema: { type: boolean, default: false }
      requestBody:
        required: true
        content:
          application/json:
            examples:
              edg_about_project:
                value:
                  session_id: s_123
                  section_id: "about_project"
                  section_variant: "about_project.i_and_p.automation"
                  inputs:
                    tone: "Formal consultant voice"
                    length_limit: 350
                    evidence_labels: ["acra_bizfile","audited_financials"]
              psg_impact:
                value:
                  session_id: s_777
                  section_id: "business_impact"
                  inputs:
                    evidence_labels: ["vendor_quotation","cost_breakdown"]
            schema: { $ref: '#/components/schemas/DraftRequest' }
      responses:
        '200':
          description: Draft generated
          headers:
            x-prompt-pack:
              schema: { type: string }
              description: pack@version used (e.g., edg@1.2.0)
          content:
            application/json:
              schema: { $ref: '#/components/schemas/DraftResponse' }

  /sessions/{sessionId}/validate:
    post:
      tags: [Validation]
      summary: Run grant-specific validations for current state
      parameters: [ { $ref: '#/components/parameters/SessionId' } ]
      requestBody:
        required: false
        content:
          application/json:
            examples:
              all: { value: { scope: "all" } }
              annex3: { value: { scope: "annex3", section_id: "business_impact" } }
            schema:
              type: object
              properties:
                scope: { type: string, enum: [eligibility, evidence, annex3, completeness, all], default: all }
                section_id: { type: string }
      responses:
        '200':
          description: OK
          content:
            application/json:
              examples:
                ex:
                  value:
                    checks:
                      - code: "PSG.ELIG.LOCAL_EQUITY_MIN_30"
                        level: "error"
                        message: "Local equity below 30%."
                        hint: "Update shareholders or switch to non-grant workflow."
                        related_items: []
                      - code: "PSG.ANNEX3.MISMATCH"
                        level: "warning"
                        message: "Quotation items do not match Annex 3."
                        hint: "Ensure SKUs and unit prices match the approved package."
                        related_items: ["vendor_quotation"]
              schema: { $ref: '#/components/schemas/ValidationResult' }

  /packs:
    get:
      tags: [Packs]
      summary: List available packs
      parameters:
        - in: query
          name: grant
          schema: { $ref: '#/components/schemas/GrantId' }
        - in: query
          name: use_case
          schema: { $ref: '#/components/schemas/UseCase' }
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                type: object
                properties:
                  items:
                    type: array
                    items: { $ref: '#/components/schemas/PackSummary' }

  /catalog/vendors:
    get:
      tags: [Catalog & Vendor Docs]
      summary: Search vendor solutions (mock or mirrored index)
      parameters:
        - in: query
          name: q
          schema: { type: string }
        - in: query
          name: category
          schema: { type: string, enum: [IT, Equipment, Consultancy] }
      responses:
        '200':
          description: OK
          content:
            application/json:
              examples:
                ex:
                  value:
                    items:
                      - id: "vend_it_pos_abc"
                        name: "ABC POS Cloud (Annex 3 Package A)"
                        category: "IT"
                        vendor: "ABC Systems Pte. Ltd."
                        annex3_ref: "PSG-IT-12345-A"
              schema:
                type: object
                properties:
                  items:
                    type: array
                    items: { $ref: '#/components/schemas/VendorSolution' }

  /vendor-docs:
    post:
      tags: [Catalog & Vendor Docs]
      summary: Generate a vendor quotation/proposal draft (optional path)
      requestBody:
        required: true
        content:
          application/json:
            examples:
              ex:
                value:
                  session_id: s_777
                  vendor_solution_id: "vend_it_pos_abc"
                  inputs: { seats: 10, locations: 2, term_months: 12 }
            schema:
              type: object
              required: [session_id, vendor_solution_id]
              properties:
                session_id: { type: string }
                vendor_solution_id: { type: string }
                inputs:
                  type: object
                  additionalProperties: true
      responses:
        '201':
          description: Draft vendor doc stored as evidence
          content:
            application/json:
              schema:
                type: object
                properties:
                  file_id: { type: string }
                  label: { type: string, example: "vendor_quotation" }

components:
  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT

  parameters:
    SessionId:
      in: path
      name: sessionId
      required: true
      schema: { type: string }

  schemas:
    GrantId:
      type: string
      enum: [EDG, PSG, MRA]
    UseCase:
      type: string
      enum: [proposal_builder, lead_generation]

    SessionCreate:
      type: object
      required: [grant, use_case]
      properties:
        grant: { $ref: '#/components/schemas/GrantId' }
        use_case: { $ref: '#/components/schemas/UseCase' }
        company:
          type: object
          properties:
            uen: { type: string }
            name: { type: string }
        metadata:
          type: object
          additionalProperties: true
    Session:
      type: object
      properties:
        session_id: { type: string }
        grant: { $ref: '#/components/schemas/GrantId' }
        use_case: { $ref: '#/components/schemas/UseCase' }
        created_at: { type: string, format: date-time }
        company:
          type: object
          properties:
            uen: { type: string }
            name: { type: string }
        solution: { $ref: '#/components/schemas/SolutionOption' }
        eligibility: { $ref: '#/components/schemas/Eligibility' }
        metadata:
          type: object
          additionalProperties: true

    SolutionOption:
      type: object
      required: [id, title, grant, category, tags, rationale]
      properties:
        id: { type: string }
        title: { type: string }
        grant: { $ref: '#/components/schemas/GrantId' }
        category: { type: string, description: "EDG: Core/I&P/Market Access; PSG: IT/Equipment/Consultancy; MRA: pillar" }
        tags:
          type: array
          items: { type: string }
        rationale: { type: string }

    ChecklistItem:
      type: object
      required: [id, type]
      properties:
        id: { type: string, description: "Section or evidence label, e.g., vendor_quotation, about_project" }
        type: { type: string, enum: [upload, draft] }
        title: { type: string }
        help_text: { type: string }
        category: { type: string }
        section_variant: { type: string, description: "e.g., about_project.i_and_p.automation" }
        requires:
          type: array
          items: { type: string }

    Eligibility:
      type: object
      properties:
        local_equity_pct: { type: number, minimum: 0, maximum: 100 }
        turnover: { type: integer, minimum: 0 }
        headcount: { type: integer, minimum: 0 }
        group_turnover: { type: integer, minimum: 0 }
        group_headcount: { type: integer, minimum: 0 }
        new_market_sales_3y_under_100k: { type: boolean } # MRA
        used_in_singapore: { type: boolean }               # PSG
        no_payment_before_application: { type: boolean }   # PSG

    EvidenceReceipt:
      type: object
      properties:
        session_id: { type: string }
        label: { type: string }
        file_id: { type: string }
        text_extracted: { type: boolean }

    EvidenceItem:
      type: object
      properties:
        label: { type: string }
        filename: { type: string }
        size_bytes: { type: integer }
        uploaded_at: { type: string, format: date-time }

    DraftRequest:
      type: object
      required: [session_id, section_id]
      properties:
        session_id: { type: string }
        section_id: { type: string }
        section_variant: { type: string, description: "If omitted, server infers from selected solution" }
        inputs:
          type: object
          additionalProperties: true
          properties:
            tone: { type: string, default: "Formal consultant voice" }
            length_limit: { type: integer, default: 350 }
            evidence_labels:
              type: array
              items: { type: string }
            pack_override:
              type: string
        debug:
          type: object
          properties:
            return_evidence_snippet: { type: boolean, default: false }

    DraftResponse:
      type: object
      properties:
        section_id: { type: string }
        framework: { type: string, description: "e.g., PAS, SCQA" }
        evidence_used:
          type: array
          items: { type: string }
        output: { type: string }
        evaluation:
          type: object
          properties:
            groundedness: { type: number, minimum: 0, maximum: 1 }
            toxicity: { type: number, minimum: 0, maximum: 1 }

    ValidationResult:
      type: object
      properties:
        checks:
          type: array
          items:
            type: object
            properties:
              code: { type: string }
              level: { type: string, enum: [error, warning, info] }
              message: { type: string }
              hint: { type: string }
              related_items:
                type: array
                items: { type: string }

    PackSummary:
      type: object
      properties:
        id: { type: string, description: "pack id, e.g., edg|psg|mra|leadgen" }
        version: { type: string }
        status: { type: string, enum: [approved, draft, archived] }
        sections:
          type: array
          items: { type: string }

    VendorSolution:
      type: object
      properties:
        id: { type: string }
        name: { type: string }
        category: { type: string, enum: [IT, Equipment, Consultancy] }
        vendor: { type: string }
        annex3_ref: { type: string }
[openapi_3.1.yaml: end]

File: requirements.txt
[requirements.txt: start]
fastapi==0.110.0
uvicorn[standard]==0.29.0
gunicorn==21.2.0
azure-identity==1.17.1
azure-keyvault-secrets==4.7.0
azure-appconfiguration==1.6.0
azure-storage-blob==12.19.1
azure-data-tables==12.6.0
pydantic==2.7.0
httpx==0.27.0
pyyaml==6.0.2
azure-search-documents==11.6.0b2
jsonschema==4.23.0
[requirements.txt: end]

File: smartai-prompts-v2.schema.json
[smartai-prompts-v2.schema.json: start]
{
  "name": "smartai-prompts-v2",
  "fields": [
    {"name":"id",           "type":"Edm.String",                 "key":true,  "retrievable":true},
    {"name":"pack_id",      "type":"Edm.String", "filterable":true,           "retrievable":true},
    {"name":"version",      "type":"Edm.String", "filterable":true, "sortable":true, "retrievable":true},
    {"name":"status",       "type":"Edm.String", "filterable":true,           "retrievable":true},
    {"name":"section_id",   "type":"Edm.String", "filterable":true,           "retrievable":true},
    {"name":"retrieval_tags","type":"Collection(Edm.String)","filterable":true,"facetable":true,"retrievable":true},
    {"name":"template_text","type":"Edm.String", "searchable":true,           "retrievable":true},
    {"name":"metadata_json","type":"Edm.String",                                 "retrievable":true}
  ],
  "similarity": { "@odata.type":"#Microsoft.Azure.Search.BM25Similarity" }
}
[smartai-prompts-v2.schema.json: end]

File: startup.sh
[startup.sh: start]
#!/usr/bin/env bash
set -euo pipefail

# Ensure vendored deps (root of the zip) are importable
export PYTHONPATH="/home/site/wwwroot:${PYTHONPATH:-}"

# (Optional) show what's there during troubleshooting
python -V || true
python -c "import sys, pkgutil; print('PYTHONPATH=', sys.path)" || true
python -c "import uvicorn, gunicorn; print('uvicorn', uvicorn.__version__)" || true
python -c "import sys, cryptography; print('cryptography=', cryptography.__version__, cryptography.__file__); print('sys.maxunicode=', sys.maxunicode)" || true

# Start the API
exec gunicorn -w 1 -k uvicorn.workers.UvicornWorker app.main:app --bind 0.0.0.0:${PORT}
[startup.sh: end]

File: tools/build_index_payload.py
[build_index_payload.py: start]
#!/usr/bin/env python3
"""
build_index_payload.py
Purpose
	â€¢	Walks your repo under app/vault/**/pack.yml.
	â€¢	Validates + normalizes each pack.
	â€¢	Produces a single JSON array of documents ready for Azure Search, saved to a file (e.g., artifacts/index_docs.json).

Why split it out
	â€¢	PR CI can run this safely (no admin keys).
	â€¢	You get a reviewable artifact (what would be indexed).
	â€¢	Itâ€™s deterministic input to the next step.

Usage:
  python tools/build_index_payload.py --status candidate --out artifacts/index_docs.json [--packs "psg@1.0.0,edg@1.0.1"]
"""
from __future__ import annotations
import argparse, json, os, sys
from pathlib import Path
from datetime import datetime, timezone

def read_yaml(path: Path) -> dict:
    import yaml
    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def discover_packs(vault_root: Path):
    for p in vault_root.glob("*.*"):
        y = p / "pack.yml"
        if y.exists():
            yield p, y

def normalize_pack_id(name: str) -> str:
    return name.split(".")[0].upper()

def make_safe_doc_id(pack_id: str, version: str, section: str, status: str) -> str:
    # Azure Search key rules: letters, digits, underscore (_), dash (-), equal sign (=)
    safe_version = str(version).replace(".", "_")
    return f"{pack_id}={safe_version}={section}={status}"

def should_include(pack_id: str, version: str, status: str, cli_status: str, packs_filter: set[str] | None):
    if cli_status and status != cli_status:
        return False
    if packs_filter is None:
        return True
    key = f"{pack_id}@{version}"
    return key in packs_filter

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--vault", default="app/vault")
    ap.add_argument("--status", choices=["candidate", "approved"], required=True)
    ap.add_argument("--packs", help='Comma-separated like "psg@1.0.0,edg@1.0.1"', default=None)
    ap.add_argument("--out", default="artifacts/index_docs.json")
    args = ap.parse_args()

    vault = Path(args.vault)
    if not vault.exists():
        print(f"ERR: vault path not found: {vault}", file=sys.stderr)
        return 2
    
    out_path = Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    packs_filter = None
    if args.packs:
        packs_filter = {x.strip().upper() for x in args.packs.split(",") if x.strip()}

    docs = []
    now = datetime.now(timezone.utc).isoformat()

    for pack_dir, pack_yml in discover_packs(vault):
        pack = read_yaml(pack_yml)
        pack_id = (pack.get("id") or normalize_pack_id(pack_dir.name)).upper()
        version = pack.get("version")
        if not version:
            print(f"ERR: {pack_yml}: version is required", file=sys.stderr)
            return 1
        version = str(version)
        status = str(pack.get("status")).lower()
        labels = pack.get("labels", {})
        
        # --- Back-compat: derive `sections` from `templates` if missing ---
        sections = pack.get("sections", [])
        if not sections:
            tmpl = pack.get("templates")
            if isinstance(tmpl, dict) and tmpl:
                sections = list(tmpl.keys())
                print(f"WARNING: [PACK] {pack_yml}: deriving 'sections' from 'templates' (deprecated)", file=sys.stderr)
        # -----------------------------------------------------------------

        if not should_include(pack_id, version, status, args.status, packs_filter):
            continue

        if not sections:
            print(f"WARN: [PACK] {pack_yml}: no sections found (neither 'sections' nor 'templates'); skipping", file=sys.stderr)
            continue

        tmpl_dir = pack_dir / "templates"
        templates_cfg = pack.get("templates", {})
        for section in sections:
            md_path = tmpl_dir / f"{section}.md"
            if not md_path.exists():
                # tolerate missing section but flag it in output for visibility
                body = f"[[MISSING TEMPLATE: {md_path}]]"
            else:
                body = md_path.read_text(encoding="utf-8")

            doc_id = make_safe_doc_id(pack_id, version, section, args.status)
            tmpl_cfg = templates_cfg.get(section, {}) if isinstance(templates_cfg, dict) else {}
            retrieval_tags = tmpl_cfg.get("retrieval_tags", []) or []

            metadata = {
                "path": str(md_path),
                "labels": labels,
                "updated_at": now,
                "pack_id": pack_id,
                "version": version,
                "section_id": section,
                "template_key": section,
            }

            docs.append({
                "id": doc_id,
                "pack_id": pack_id,
                "version": version,
                "status": args.status,
                "section_id": section,
                "retrieval_tags": retrieval_tags,
                "template_text": body,
                "metadata_json": json.dumps(metadata, ensure_ascii=False),
            })

    with out_path.open("w", encoding="utf-8") as f:
        json.dump(docs, f, ensure_ascii=False, indent=2)

    if not docs:
        print("WARN: no docs built (check --status and --packs filters)", file=sys.stderr)
    
    print(f"Wrote {len(docs)} docs â†’ {out_path}")
    return 0

if __name__ == "__main__":
    sys.exit(main())
[build_index_payload.py: end]

File: tools/index_packs.py
[index_packs.py: start]
#!/usr/bin/env python3
"""
index_packs.py
- Upserts docs into Azure AI Search using REST API.
- Reads a JSON array (from build_index_payload.py).

	â€¢	Read the JSON produced by build_index_payload.py.
	â€¢	Upsert those documents to Azure Cognitive Search.

Why separate?
	â€¢	Needs admin key; you donâ€™t want this in PR CI.
	â€¢	Keeps the â€œmutating the worldâ€ step inside a gated Promote workflow.

Env:
  AZURE_SEARCH_ENDPOINT (e.g., https://<name>.search.windows.net)
  AZURE_SEARCH_INDEX    (e.g., smartai-prompts-v2)
  AZURE_SEARCH_ADMIN_KEY

Usage:
  python tools/index_packs.py --in artifacts/index_docs.json [--batch 1000]
"""
from __future__ import annotations
import argparse, json, os, sys, time, urllib.request, urllib.error, urllib.parse

def post_json(url: str, payload: dict, headers: dict):
    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(url, data=data, headers=headers, method="POST")
    try:
        with urllib.request.urlopen(req) as resp:
            return json.loads(resp.read().decode("utf-8"))
    except urllib.error.HTTPError as e:
        body = e.read().decode("utf-8", errors="ignore")
        print(f"\n=== Azure Search HTTPError {e.code} ===", file=sys.stderr)
        print(f"URL: {req.full_url}", file=sys.stderr)
        print("Response headers:", e.headers, file=sys.stderr)
        print("Body:", body[:2000], file=sys.stderr)  # print the first ~2KB
        raise

def chunked(iterable, n):
    buf = []
    for x in iterable:
        buf.append(x)
        if len(buf) >= n:
            yield buf
            buf = []
    if buf: yield buf

def fetch_index_schema(endpoint: str, index: str, key: str) -> set[str]:
    url = f"{endpoint}/indexes/{urllib.parse.quote(index)}?api-version=2024-07-01"
    req = urllib.request.Request(url, headers={"api-key": key})
    try:
        with urllib.request.urlopen(req) as resp:
            meta = json.loads(resp.read().decode("utf-8"))
        fields = meta.get("fields", [])
        allowed = {f["name"] for f in fields if isinstance(f, dict) and "name" in f}
        return allowed
    except urllib.error.HTTPError as e:
        body = e.read().decode("utf-8", errors="ignore")
        print(f"ERR: Failed to fetch index schema: HTTP {e.code}", file=sys.stderr)
        print("Body:", body[:500], file=sys.stderr)
        raise

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in", dest="infile", help="JSON file of docs to index")
    ap.add_argument("--batch", type=int, default=500)
    ap.add_argument("--print-schema", action="store_true", help="Print Azure Search index schema and exit")
    args = ap.parse_args()

    endpoint = os.environ.get("AZURE_SEARCH_ENDPOINT", "").rstrip("/")
    index = os.environ.get("AZURE_SEARCH_INDEX", "")
    key = os.environ.get("AZURE_SEARCH_ADMIN_KEY", "")

    if args.print_schema:
        url = f"{endpoint}/indexes/{index}?api-version=2024-07-01"
        headers = {"api-key": key}
        req = urllib.request.Request(url, headers=headers, method="GET")
        try:
            with urllib.request.urlopen(req) as resp:
                schema = json.loads(resp.read().decode("utf-8"))
                print(json.dumps(schema.get("fields", []), indent=2))
                return 0
        except Exception as e:
            print("ERR: Failed to fetch schema:", e)
            return 2

    if not args.infile:
        print("ERR: --in is required unless using --print-schema", file=sys.stderr)
        return 2

    if not (endpoint and index and key):
        print("ERR: set AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX, AZURE_SEARCH_ADMIN_KEY", file=sys.stderr)
        return 2

    docs = json.loads(open(args.infile, "r", encoding="utf-8").read())
    
    # Schema preflight check
    allowed = fetch_index_schema(endpoint, index, key)
    allowed_with_action = set(allowed) | {"@search.action"}
    doc_keys = set().union(*[set(d.keys()) for d in docs]) if docs else set()
    unknown = sorted(doc_keys - allowed_with_action)
    if unknown:
        print(f"WARN: unknown fields not in schema: {unknown}", file=sys.stderr)
    
    url = f"{endpoint}/indexes/{index}/docs/index?api-version=2024-07-01"
    headers = {
        "Content-Type": "application/json",
        "api-key": key
    }

    total = 0
    for batch in chunked(docs, args.batch):
        payload = {"value": [{"@search.action": "mergeOrUpload", **d} for d in batch]}
        _ = post_json(url, payload, headers)
        total += len(batch)
        time.sleep(0.05)  # gentle

    print(f"Indexed {total} docs to {index}")
    return 0

if __name__ == "__main__":
    sys.exit(main())
[index_packs.py: end]

File: tools/lint_packs.py
[lint_packs.py: start]
#!/usr/bin/env python3
"""
lint_packs.py
Validates prompt packs. This patch adds backward-compat normalization so
legacy packs using `pack_id` and `templates:` continue to work:
  - id â† pack_id
  - sections â† derived from template file paths or keys
  - version inferred from directory name if missing (e.g., EDG.v1 â†’ "1")
  - status defaults to "draft" if missing

Downstream validation/heuristics remain unchanged.

- Validates pack manifests and basic template structure.
- Enforces PAS/SCQA structure tokens.
- Fails (exit != 0) on any error.

Usage:
  python tools/lint_packs.py [--vault app/vault]
"""
from __future__ import annotations
import argparse, sys, re, json, os
from pathlib import Path
from typing import Dict, Any

PAS_TOKENS = ["Problem", "Agitate", "Solve"]
SCQA_TOKENS = ["Situation", "Complication", "Question", "Answer"]

REQUIRED_PACK_FIELDS = ["id", "version", "status", "sections"]

# Example path patterns we expect: app/vault/EDG.v1/pack.yml
# Capture "ver" from ".v1" or ".1.2.3"
VERSION_FROM_DIR = re.compile(r"[\\/](?P<name>[A-Za-z0-9._-]+)\.(?P<ver>v?\d[\w.-]*)[\\/]")

def infer_version_from_path(path: str) -> str:
    """Extract version from directory name like EDG.v1 â†’ '1' or EDG.v1.2.3 â†’ '1.2.3'"""
    m = VERSION_FROM_DIR.search(str(path))
    if m and m.group("ver"):
        # normalize "v1" or "1.2.3" -> "1" or "1.2.3"
        ver = m.group("ver").lstrip("vV")
        # If just a single number, default to SemVer format
        if re.match(r'^\d+$', ver):
            return f"{ver}.0.0"
        return ver
    return "1.0.0"

def normalize_pack(pack: Dict[str, Any], file_path: str) -> Dict[str, Any]:
    """
    Back-compat shim:
      - Prefer `id`, but fall back to `pack_id`.
      - Prefer explicit `sections`, else derive from `templates` file paths or keys.
      - Fill `version` from folder if missing.
      - Default `status` to "draft" if missing.
    Does NOT change downstream validation logic; it only ensures required keys exist.
    """
    norm = dict(pack) if pack else {}

    # id â† pack_id (back-compat)
    if "id" not in norm and "pack_id" in norm:
        norm["id"] = norm["pack_id"]
        print(f"WARNING: [PACK] {file_path}: using legacy 'pack_id' â†’ 'id' (deprecated)", file=sys.stderr)

    # sections â† templates keys (back-compat)
    # If deriving from templates, prefer extracting from file field or use keys
    if "sections" not in norm:
        templates = norm.get("templates")
        if isinstance(templates, dict) and templates:
            # Try to extract section names from file field, fallback to keys
            sections = []
            for key, tmpl_data in templates.items():
                if isinstance(tmpl_data, dict) and "file" in tmpl_data:
                    # Extract basename from file path, e.g., "templates/about_company.md" -> "about_company"
                    file_path_str = tmpl_data["file"]
                    if file_path_str:  # Only proceed if file path is not empty
                        section_name = Path(file_path_str).stem  # removes .md extension
                        sections.append(section_name)
                    else:
                        # Empty file path: fallback to key name
                        sections.append(key)
                else:
                    # Fallback to key name
                    sections.append(key)
            norm["sections"] = sections if sections else list(templates.keys())
        else:
            norm.setdefault("sections", [])
        if norm.get("sections"):
            print(f"WARNING: [PACK] {file_path}: deriving 'sections' from 'templates' (deprecated)", file=sys.stderr)

    # version
    if "version" not in norm or not norm["version"]:
        norm["version"] = infer_version_from_path(file_path)

    # status
    if "status" not in norm or not norm["status"]:
        norm["status"] = "draft"

    return norm

def read_yaml(path: Path) -> dict:
    import yaml
    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def find_packs(vault_dir: Path):
    for p in vault_dir.glob("*.*"):
        pack_yml = p / "pack.yml"
        if pack_yml.exists():
            yield p, pack_yml

def check_tokens(md_text: str, required_tokens: list[str], file: Path, errs: list[str]):
    for tok in required_tokens:
        # token must appear as a header or strong marker at least once
        pattern = rf"(^|\n)\s*(#+\s*{re.escape(tok)}\b|(\*\*|__){re.escape(tok)}(\*\*|__))"
        if not re.search(pattern, md_text, flags=re.IGNORECASE):
            errs.append(f"[TOKENS] {file}: missing token '{tok}'")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--vault", default="app/vault", help="Root of packs")
    args = ap.parse_args()

    vault = Path(args.vault)
    if not vault.exists():
        print(f"ERR: vault path not found: {vault}", file=sys.stderr)
        return 2

    errors: list[str] = []
    warnings: list[str] = []

    for pack_dir, pack_yml_path in find_packs(vault):
        pack = read_yaml(pack_yml_path)
        # Normalize BEFORE schema/field checks
        pack = normalize_pack(pack, str(pack_yml_path))

        # Basic pack.yml shape
        for field in REQUIRED_PACK_FIELDS:
            if field not in pack:
                errors.append(f"[PACK] {pack_yml_path}: missing '{field}'")

        pack_id = pack.get("id", pack_dir.name.split(".")[0].upper())
        version = str(pack.get("version", ""))
        status = pack.get("status", "").lower() if isinstance(pack.get("status", ""), str) else pack.get("status")
        if status not in {"draft", "candidate", "approved"}:
            errors.append(f"[PACK] {pack_yml_path}: status must be 'draft', 'candidate', or 'approved'")
        
        # Validate version format (basic SemVer check)
        if not version:
            errors.append(f"[PACK] {pack_yml_path}: version is required")
        elif not re.match(r'^\d+\.\d+\.\d+$', version):
            warnings.append(f"[PACK] {pack_yml_path}: version '{version}' doesn't follow SemVer format (x.y.z)")

        sections = pack.get("sections", [])
        if not isinstance(sections, list) or not sections:
            errors.append(f"[PACK] {pack_yml_path}: 'sections' must be a non-empty list")

        # Check templates exist & tokens present
        tmpl_dir = pack_dir / "templates"
        if not tmpl_dir.exists():
            errors.append(f"[PACK] {pack_yml_path}: templates/ folder missing")
            continue

        # Enforce sectionâ†’template parity and validate rubrics
        for sec in sections:
            f = tmpl_dir / f"{sec}.md"
            if not f.exists():
                errors.append(f"[PACK] {pack_yml_path}: section '{sec}' missing template {f}")
                continue
            
            # Check tokens against template rubric if available
            text = f.read_text(encoding="utf-8")
            templates = pack.get("templates", {})
            
            # Find matching template by searching all templates for matching file path
            tmpl_data = None
            for key, tmpl in templates.items():
                if isinstance(tmpl, dict) and tmpl.get("file"):
                    # Extract basename from template's file field to match against section name
                    file_path_str = tmpl["file"]
                    if file_path_str:
                        template_stem = Path(file_path_str).stem
                        if template_stem == sec:
                            tmpl_data = tmpl
                            break
            
            if tmpl_data and "rubric" in tmpl_data:
                rubric = tmpl_data["rubric"]
                if isinstance(rubric, dict) and "required_tokens" in rubric:
                    required_tokens = rubric["required_tokens"]
                    check_tokens(text, required_tokens, f, errors)
                else:
                    # No required_tokens in rubric, use filename heuristic as fallback
                    fname = f.stem.lower()
                    if any(key in fname for key in ["business_case", "impact", "solution", "proposal", "vendor"]):
                        check_tokens(text, PAS_TOKENS, f, errors)
                    else:
                        check_tokens(text, SCQA_TOKENS, f, errors)
            else:
                # No rubric defined for this template, use filename heuristic
                fname = f.stem.lower()
                if any(key in fname for key in ["business_case", "impact", "solution", "proposal", "vendor"]):
                    check_tokens(text, PAS_TOKENS, f, errors)
                else:
                    check_tokens(text, SCQA_TOKENS, f, errors)

        # Optional schema check against repo schema (non-fatal if not present)
        schema_path = Path("smartai-prompts-v2.schema.json")
        if schema_path.exists():
            try:
                import jsonschema
                schema = json.loads(schema_path.read_text(encoding="utf-8"))
                doc = {
                    "id": f"{pack_id}@{version}",
                    "pack": pack_id,
                    "version": version,
                    "status": status,
                    "sections": sections,
                }
                jsonschema.validate(doc, schema)
            except Exception as e:
                warnings.append(f"[SCHEMA] {pack_yml_path}: {e}")

    for w in warnings: print(f"WARNING: {w}")
    if errors:
        for e in errors: print(f"ERROR: {e}", file=sys.stderr)
        return 1
    print("lint_packs: OK")
    return 0

if __name__ == "__main__":
    sys.exit(main())
[lint_packs.py: end]

File: tools/local_pack_casing_smoke.py
[local_pack_casing_smoke.py: start]
#!/usr/bin/env python3
"""
local_pack_casing_smoke.py

Local smoke test for pack_id casing and pack filters.

Checks:
1. tools/build_index_payload.py can build docs for PSG@1.0.1 and EDG@1.0.1.
2. All produced docs:
   - have pack_id in UPPERCASE at the top level, and
   - have metadata_json.pack_id matching and also UPPERCASE.

This is a pure local check â€” no deployment, no FastAPI, no curl.
It reuses the same CLI path as the Promote Pack workflow.
"""

import json
import subprocess
import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
OUT = ROOT / "artifacts" / "local_smoke_index_docs.json"


def run_build_index_payload() -> None:
    """
    Run tools/build_index_payload.py with a fixed pack set:
    PSG@1.0.1 and EDG@1.0.1.

    Uses the same CLI entry point as CI:
      python tools/build_index_payload.py --status approved --packs "PSG@1.0.1,EDG@1.0.1" --out artifacts/...
    """
    OUT.parent.mkdir(parents=True, exist_ok=True)

    cmd = [
        sys.executable,
        "tools/build_index_payload.py",
        "--vault", "app/vault",
        "--status", "approved",
        "--packs", "PSG@1.0.1,EDG@1.0.1",
        "--out", str(OUT),
    ]

    print("Running build_index_payload:", " ".join(cmd))
    result = subprocess.run(cmd, cwd=str(ROOT))

    if result.returncode != 0:
        raise SystemExit(f"[FAIL] build_index_payload.py exited with code {result.returncode}")


def load_docs():
    """
    Load the JSON docs file produced by build_index_payload.
    """
    if not OUT.exists():
        raise SystemExit(f"[FAIL] Expected output file not found: {OUT}")

    with OUT.open("r", encoding="utf-8") as f:
        return json.load(f)


def assert_uppercase_pack_ids(docs) -> None:
    """
    Assert that:
      - At least one doc was produced.
      - Each doc has pack_id in UPPERCASE.
      - metadata_json.pack_id matches doc["pack_id"] and is also UPPERCASE.
    """
    if not docs:
        raise AssertionError(
            "No docs built; expected at least one document for PSG@1.0.1 / EDG@1.0.1. "
            "Check --packs filtering or pack status in pack.yml."
        )

    for d in docs:
        pack_id = d.get("pack_id")
        if not pack_id:
            raise AssertionError(f"Doc missing pack_id: {d}")

        if not pack_id.isupper():
            raise AssertionError(f"pack_id not uppercase: {pack_id!r}")

        meta_raw = d.get("metadata_json") or "{}"
        try:
            meta = json.loads(meta_raw)
        except Exception as e:
            raise AssertionError(f"metadata_json is not valid JSON for doc {d.get('id')}: {e}")

        m_pack = meta.get("pack_id")
        if m_pack != pack_id:
            raise AssertionError(
                f"metadata_json.pack_id mismatch for doc {d.get('id')}: "
                f"{m_pack!r} != {pack_id!r}"
            )

        if not m_pack.isupper():
            raise AssertionError(
                f"metadata_json.pack_id not uppercase for doc {d.get('id')}: {m_pack!r}"
            )

    print(f"[OK] {len(docs)} docs with uppercase pack_id in both top-level and metadata_json.")


def main() -> None:
    print("=== Local pack casing smoke test ===")
    run_build_index_payload()
    docs = load_docs()
    assert_uppercase_pack_ids(docs)
    print("[PASS] local_pack_casing_smoke.py completed successfully.")


if __name__ == "__main__":
    main()

[local_pack_casing_smoke.py: end]

File: tools/offline_eval.py
[offline_eval.py: start]
#!/usr/bin/env python3
"""
offline_eval.py
- Lightweight CI-time evaluation (no model calls).
- Computes proxy metrics over pack templates:
    * groundedness_proxy: share of non-empty lines that contain a citation marker "[source:"
    * avg_chars_per_template and per-section caps (optional)
- Fails PR if thresholds are not met.

Usage:
  python tools/offline_eval.py \
    --vault app/vault \
    --out artifacts/eval_report.json \
    --min_grounded 0.80 \
    --max_chars 12000 \
    [--goldens "app/vault/**/golden/*.jsonl"] \
    [--dry-worker]
"""
from __future__ import annotations
import argparse, json, sys, glob
from pathlib import Path

def read_yaml(path: Path) -> dict:
    import yaml
    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def discover_templates(vault_root: Path):
    """Discover all templates from pack.yml files.
    
    Iterates over pack["templates"] dict, yielding (pack_id, version, section, md_path)
    for each template. Uses section_id from template config if present, otherwise
    uses the template key name as the section.
    """
    for pack_dir in sorted(vault_root.glob("*.*")):
        pack_yml = pack_dir / "pack.yml"
        if not pack_yml.exists():
            continue
        pack = read_yaml(pack_yml)
        pack_id = str(pack.get("id") or pack.get("pack_id") or pack_dir.name.split(".")[0].upper()).upper()
        version = str(pack.get("version"))
        templates = pack.get("templates", {}) or {}
        for tmpl_name, cfg in templates.items():
            file_rel = cfg.get("file")
            if not file_rel:
                continue
            md_path = pack_dir / file_rel
            section = cfg.get("section_id") or tmpl_name
            yield pack_id, version, section, md_path

def groundedness_proxy(md_text: str) -> float:
    lines = [ln.strip() for ln in md_text.splitlines()]
    nonempty = [ln for ln in lines if ln]
    if not nonempty:
        return 0.0
    cited = [ln for ln in nonempty if "[source:" in ln.lower()]
    return len(cited) / max(1, len(nonempty))

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--vault", default="app/vault")
    ap.add_argument("--out", default="artifacts/eval_report.json")
    ap.add_argument("--goldens", default="app/vault/**/golden/*.jsonl")
    ap.add_argument("--min_grounded", type=float, default=0.80)
    ap.add_argument("--max_chars", type=int, default=12000)
    ap.add_argument("--dry-worker", action="store_true", help="placeholder flag for CI parity")
    args = ap.parse_args()

    vault = Path(args.vault)
    out = Path(args.out)
    out.parent.mkdir(parents=True, exist_ok=True)

    results = []
    failures = []

    # Optional: ingest golden hints (caps/overrides)
    # Format (jsonl): {"pack":"PSG","section":"business_case","max_chars":15000,"min_grounded":0}
    golden_overrides = {}
    for path in glob.glob(args.goldens, recursive=True):
        p = Path(path)
        for ln in p.read_text(encoding="utf-8").splitlines():
            if not ln.strip(): continue
            try:
                rec = json.loads(ln)
                pack = rec.get("pack", "").upper()
                sec = rec.get("section")
                if sec:
                    # Key by (pack, section) for per-pack control
                    key = f"{pack}:{sec}" if pack else sec
                    golden_overrides.setdefault(key, {}).update(rec)
            except Exception:
                # ignore malformed lines
                pass

    for pack_id, version, section, md_path in discover_templates(vault):
        if not md_path.exists():
            print(f"ERR: missing template {md_path}", file=sys.stderr)
            failures.append({
                "pack": pack_id, "version": version, "section": section, 
                "reasons": [f"missing template {md_path}"]
            })
            continue
            
        body = md_path.read_text(encoding="utf-8")
        g = groundedness_proxy(body)
        char_len = len(body)

        # Try pack-specific override first, then section-only
        pack_section_key = f"{pack_id}:{section}"
        section_key = section
        sec_caps = golden_overrides.get(pack_section_key, golden_overrides.get(section_key, {}))
        min_g = float(sec_caps.get("min_grounded", args.min_grounded))
        max_c = int(sec_caps.get("max_chars", args.max_chars))

        ok = (g >= min_g) and (char_len <= max_c)
        if not ok:
            reasons = []
            if g < min_g: reasons.append(f"groundedness {g:.2f} < {min_g:.2f}")
            if char_len > max_c: reasons.append(f"chars {char_len} > {max_c}")
            failures.append({
                "pack": pack_id, "version": version, "section": section, "reasons": reasons
            })

        results.append({
            "pack": pack_id,
            "version": version,
            "section": section,
            "groundedness_proxy": round(g, 4),
            "chars": char_len,
            "min_grounded": min_g,
            "max_chars": max_c,
            "path": str(md_path)
        })

    report = {"results": results, "failures": failures}
    out.write_text(json.dumps(report, indent=2), encoding="utf-8")
    print(f"offline_eval: wrote {out} with {len(failures)} failure(s).")
    return 1 if failures else 0

# Quick local sanity checks (run from repo root):
#   python tools/offline_eval.py --vault app/vault --out /tmp/eval_report.json
# Then open /tmp/eval_report.json and confirm:
#   - "results" contains entries for PSG.v1 and EDG.v1 templates.
#   - "failures" is empty on a clean main.
#
# To see a failure, temporarily remove all "[source:" substrings from
# app/vault/PSG.v1/templates/cost_breakdown.md and rerun:
#   python tools/offline_eval.py --vault app/vault --out /tmp/eval_report.json
# You should now see at least one failure where groundedness_proxy < min_grounded.

if __name__ == "__main__":
    sys.exit(main())
[offline_eval.py: end]

File: tools/README.md
[README.md: start]
# Tools

This directory contains Python scripts for pack management and CI/CD operations.

## Scripts

- **`lint_packs.py`** - Validates pack manifests and enforces PAS/SCQA structure tokens
- **`build_index_payload.py`** - Builds JSON payloads for Azure Search indexing  
- **`offline_eval.py`** - Lightweight CI evaluation with groundedness metrics
- **`index_packs.py`** - Upserts docs to Azure AI Search via REST API
- **`wire_check.py`** - Verifies indexed docs are searchable

## Usage

See individual script help: `python tools/<script>.py --help`

[README.md: end]

File: tools/verify_psg_build.py
[verify_psg_build.py: start]
#!/usr/bin/env python3
"""
verify_psg_build.py
- Builds approved docs for PSG only and validates the output shape against expectations.
"""
from __future__ import annotations
import json, re, subprocess, sys, os

REPO_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

def run_build() -> None:
    cmd = [
        sys.executable,
        os.path.join(REPO_ROOT, "tools", "build_index_payload.py"),
        "--status", "approved",
        "--packs", "psg@1.0.1",
        "--out", os.path.join(REPO_ROOT, "artifacts", "index_docs.json"),
    ]
    print("+", " ".join(cmd))
    res = subprocess.run(cmd, capture_output=True, text=True)
    sys.stdout.write(res.stdout)
    sys.stderr.write(res.stderr)
    if res.returncode != 0:
        raise SystemExit(res.returncode)

def inspect_and_validate() -> None:
    path = os.path.join(REPO_ROOT, "artifacts", "index_docs.json")
    docs = json.load(open(path, "r", encoding="utf-8"))
    print("docs:", len(docs))
    assert len(docs) == 5, f"Expected 5 docs for PSG, got {len(docs)}"
    d0 = docs[0]
    print("id:", d0["id"])
    print("keys:", sorted(d0.keys()))
    # 2a. Azure-safe id
    assert re.fullmatch(r"[A-Za-z0-9_=\-]+", d0["id"]), f"Bad id: {d0['id']}"
    # 2b. Top-level fields match index schema
    expected = {"id","pack_id","version","status","section_id","retrieval_tags","template_text","metadata_json"}
    assert expected == set(d0.keys()), f"Unexpected keys: {sorted(set(d0.keys()) - expected)}"
    # 2c. metadata_json has API-expected fields
    meta = json.loads(d0["metadata_json"])
    for k in ["pack_id","version","section_id","template_key","path","labels","updated_at"]:
        assert k in meta, f"metadata_json missing {k}"
    print("metadata_json OK:", list(meta.keys()))

def main() -> int:
    run_build()
    inspect_and_validate()
    return 0

if __name__ == "__main__":
    sys.exit(main())



[verify_psg_build.py: end]

File: tools/wire_check.py
[wire_check.py: start]
#!/usr/bin/env python3
"""
wire_check.py
- Verifies indexed docs are searchable in Azure AI Search.
- Queries by pack@version and reports counts.

Env:
  AZURE_SEARCH_ENDPOINT (e.g., https://<name>.search.windows.net)
  AZURE_SEARCH_INDEX    (e.g., smartai-prompts-v2)
  AZURE_SEARCH_QUERY_KEY

Usage:
  python tools/wire_check.py --packs "psg@1.0.0,edg@1.0.1"
"""
from __future__ import annotations
import argparse, json, os, sys, urllib.request, urllib.parse, urllib.error

def query_search(endpoint: str, index: str, key: str, query: str, debug: bool = False) -> dict:
    # 2024-07-01: use POST /docs/search with JSON body
    url = f"{endpoint}/indexes/{index}/docs/search?api-version=2024-07-01"
    payload = {
        "search": query,
        "count": True,
        "top": 50,
    }
    data = json.dumps(payload).encode("utf-8")
    headers = {
        "api-key": key,
        "Content-Type": "application/json",
    }
    req = urllib.request.Request(url, data=data, headers=headers, method="POST")
    try:
        with urllib.request.urlopen(req) as resp:
            return json.loads(resp.read().decode("utf-8"))
    except urllib.error.HTTPError as e:
        if debug:
            body = e.read().decode("utf-8", errors="ignore")
            print(f"\n=== wire_check HTTPError {e.code} ===", file=sys.stderr)
            print(f"URL: {req.full_url}", file=sys.stderr)
            print("Body:", body[:200], file=sys.stderr)
        raise

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--packs", required=True, help='Comma-separated like "psg@1.0.0,edg@1.0.1"')
    ap.add_argument("--debug", action="store_true", help="Print debug info on HTTP errors")
    args = ap.parse_args()

    endpoint = os.environ.get("AZURE_SEARCH_ENDPOINT", "").rstrip("/")
    index = os.environ.get("AZURE_SEARCH_INDEX", "")
    key = os.environ.get("AZURE_SEARCH_QUERY_KEY", "")

    if not (endpoint and index and key):
        print("ERR: set AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX, AZURE_SEARCH_QUERY_KEY", file=sys.stderr)
        return 2

    packs = [p.strip() for p in args.packs.split(",") if p.strip()]
    all_good = True

    for pack_spec in packs:
        try:
            pack_id, version = pack_spec.split("@", 1)
            # Search for docs with this pack and version
            query = f"pack_id:{pack_id.upper()} AND version:{version}"
            result = query_search(endpoint, index, key, query, debug=args.debug)
            count = result.get("@odata.count", 0)
            print(f"{pack_spec}: {count} docs")
            if count == 0:
                all_good = False
        except Exception as e:
            print(f"ERR: {pack_spec}: {e}", file=sys.stderr)
            all_good = False

    return 0 if all_good else 1

if __name__ == "__main__":
    sys.exit(main())
[wire_check.py: end]

